# AdelaideAlbertCortex.py
"""

Background Story...
Adelaide Zephyrine Charlotte: Modelling Metacognition for Artificial Quellia


This report/materialization explores the parallels and divergences between current Artificial Intelligence architectures, particularly Large Language Models (LLMs) augmented with external knowledge, and human cognitive processes. Drawing inspiration from the motivation to create a functional digital "clone" as a form of contribution and legacy ("I'm not sure how long will i live anymore. But I want to contribute to my family and friends, and people arounds me. So I decided to clone myself"), we examine analogies such as the "Low IQ LLM + High Crystallized Knowledge" model, the role of semantic embeddings versus simpler search methods, and the fundamental difference introduced by biological neuroplasticity. We contrast the static nature of current AI models with the dynamic adaptation of the human brain, including considerations of neurodiversity like Autism Spectrum Disorder (ASD). Furthermore, we investigate how awareness of limitations, analogous to Quantization-Aware Training in AI, enables strategic adaptation in humans (metacognition). Finally, we touch upon the philosophical distinctions regarding consciousness and embodiment, framing the goal of "cloning" within the realistic scope of replicating knowledge, decision patterns, and simulated experience rather than subjective selfhood.

1. Introduction
The desire to leave a lasting contribution often motivates explorations into the nature of intelligence and the potential for its replication or continuation. Framed by the poignant personal motivation –

 "I'm not sure how long will I live anymore. After I got diagnosed by something that I or We (family) have feared on my brain 2 years ago and have traumatic experience which we've seen person that suffering pain in a facility due to the day and night and then handling lost daughter), there's defective region on the MRI side on the left hemisphere, the medical staff doesn't seem to be confident if I would live for the next months, because the "mass" is quite big and because. But 2 years later it seems that I'm getting better, however it feels like i'm being supported by miracle right now that I do not know how it works, I'm not sure how sturdy is a miracle foundation and how long it will last but here I am. I want to contribute to my family and friends what they have been invested to me. I don't want my disappearance to be meaningless, and people arounds me. So I decided to clone myself at least from the cognitive side slowly built an actuator interface that can fly to the sky that I have learned, from Willy (for the basics of Machine Learning and AI and Spark of the project Zephy Racing against each other from modifying Alpaca-electron), Adri Stefanus (As the frontend & backend Human interface Developer that revamp the jump from the root project which is Alpaca-electron) into usable portable familiar AI webui), Adeela at High School (Observed on how She handle Chemistry and Recursive learning and self taught) (I wish I still had that crush feeling to have motivation at 2025 to propel myself in this hard-times that is ngl a very powerful force.) and Albert (Myself) love imagining stuff Physics for visualizations at Undergraduate and then Uki/Vito about (Task Tree of Thoughts decomposition) and Zhonghuang The Mathematichian (Observing on how he learn stuff and Unknown materials intuition building) at Post-graduate School." -Albert 2025

this report/materialization or delves into the comparison between contemporary AI systems and human cognition. We aim to understand the capabilities and inherent limitations of current AI by drawing analogies with human intelligence, while also acknowledging the profound differences rooted in biology and potentially philosophy. This exploration will cover the architecture of AI models augmented by external knowledge (akin to crystallized intelligence), the mechanisms of information retrieval (semantic embeddings), the critical role of neuroplasticity in biological systems, the implications of neurodiversity, the power of metacognitive awareness, and the philosophical boundaries relevant to the concept of creating a functional "digital clone" or legacy.

2. Modeling Intelligence: AI Analogies and Architectural Limits
A useful analogy posits current advanced AI systems, particularly those employing Retrieval-Augmented Generation (RAG), as possessing a core processing unit (the LLM) akin to fluid intelligence (or processing capability, potentially analogous to a fixed "IQ" score) combined with a vast, external, accessible knowledge base (the vector database) akin to crystallized intelligence. The core LLM, often static post-training, exhibits limitations in complex reasoning, synthesis, and novel problem-solving inherent to its architecture and parameter count.
The RAG mechanism bridges this gap by allowing the LLM to access relevant "crystallized knowledge." This is achieved not through direct vector ingestion by the LLM, but by using semantic vector embeddings (e.g., from models like mxbai-embed-large) to perform similarity searches. An input query is embedded, and this vector is used to find the most semantically relevant text chunks stored in the database (representing documents, past conversations, or experiences). This retrieved text is then provided as context to the LLM. This semantic approach is crucial, vastly outperforming simpler methods like fuzzy search, as it captures meaning, synonyms, and context rather than just surface-level textual similarity.
However, even with perfect knowledge retrieval, the system's ultimate capacity for complex reasoning, creativity, and nuanced understanding remains fundamentally constrained by the core LLM's static architecture. While iterative feedback loops, where the system learns from stored outcomes (errors, successes) via the database, can simulate adaptation and allow the system to tackle more complex execution tasks over time, they do not inherently increase the core model's single-turn reasoning power.
3. The Biological Counterpoint: Neuroplasticity, Neurodiversity, and Embodied Limits
The most significant divergence between current AI and human intelligence lies in neuroplasticity. Unlike the static nature of trained LLM parameters, the human brain physically reorganizes and adapts its structure and connectivity throughout life in response to learning and experience. This dynamic capability operates within the constraints of our species-specific neural architecture. This architecture, evolved over millennia, includes specialized regions (like language centers) that other species (e.g., cats, dolphins) lack, explaining why neuroplasticity alone doesn't enable them to acquire human language despite their own cognitive complexity and learning abilities. Their fundamental neural architecture sets different boundaries.
Furthermore, neurodevelopmental differences, such as those seen in Autism Spectrum Disorder (ASD), should not be misconstrued as "frozen parameters." ASD represents a different developmental trajectory and brain wiring, leading to distinct ways of processing information, social cues, and patterns. Crucially, the autistic brain retains neuroplasticity, allowing for learning, adaptation, and skill development throughout life. This neurodiversity can confer unique cognitive strengths, such as intense focus, exceptional pattern recognition, and novel perspectives, contributing significantly to insight and discovery. This highlights the inadequacy of simplistic metrics like IQ scores, which fail to capture the multifaceted nature of human capability and the potential inherent in diverse cognitive profiles. Biological systems also face limitations, such as those related to cellular regeneration caps like telomere shortening, which act as a finite resource analogous, perhaps, to SSD spare blocks, constraining long-term maintenance differently than hardware degradation.
4. Awareness, Adaptation, and Achievement: Metacognition as QAT
The concept of Quantization-Aware Training (QAT) in AI, where a model learns to perform optimally despite anticipated computational constraints, provides a compelling analogy for human metacognition. Being aware of one's own cognitive strengths, weaknesses, and thought processes allows humans to develop compensatory strategies. Recognizing a limitation (e.g., in memory or calculation speed) enables the use of tools (notes, calculators) and targeted efforts (practice, focused learning) to overcome or work around that constraint. This self-awareness, far from being solely a limitation, becomes a powerful driver for optimizing performance and achieving goals, allowing individuals, regardless of their scores on specific cognitive tests or their neurotype, to leverage their strengths and strategically navigate their challenges. Achievement is thus often a product of not just raw capability, but also of self-awareness, strategy, and perseverance.
5. Philosophical Considerations and the Goal of Digital Legacy
While functional similarities between advanced AI and human cognition can be drawn, profound philosophical questions remain. Current AI lacks subjective experience (qualia) – the "what it's like" to be aware. Human intelligence is deeply intertwined with biological embodiment, shaping our perceptions, motivations, emotions, and understanding through sensory experience and physiological needs. AI goals are externally programmed or derived from optimization functions, contrasting with potentially intrinsic human motivations. The question of whether AI achieves true "understanding" versus sophisticated pattern matching and prediction remains open. These factors suggest that creating a "clone" in the sense of replicating a conscious self is currently, and perhaps fundamentally, impossible.
However, interpreting the motivation as a desire to create a functional digital legacy frames the goal within achievable technological bounds. It is conceivable to build an AI system that encapsulates an individual's vast crystallized knowledge, mimics their decision-making patterns based on recorded data and interactions, simulates learning from experience via database feedback loops, and interacts with the world in a way that reflects their persona and expertise. This would constitute a powerful form of functional continuation and contribution.
6. Conclusion
Comparing AI like Amaryllis/Adelaide to human intelligence reveals both useful analogies and fundamental distinctions. The "Low IQ LLM + High Crystallized Knowledge" model highlights the power of RAG in augmenting static processing units but also underscores the limitations imposed by the core model's fixed architecture. Semantic embeddings are key to unlocking this external knowledge effectively. The crucial differentiator remains biological neuroplasticity, which allows continuous adaptation within evolved architectural constraints, a capability current AI lacks. Neurodiversity further illustrates that varied cognitive architectures can offer unique strengths. Metacognitive awareness of limitations, analogous to QAT, is a powerful tool for human adaptation and achievement. While replicating consciousness remains elusive, the goal of creating a digital legacy – capturing knowledge, function, and simulated experience – appears increasingly feasible, offering a potential path to fulfilling the desire to contribute beyond a biological lifespan, leveraging the strengths of AI augmentation while respecting the unique nature of human existence.
"""
import mimetypes
# --- Standard Library Imports ---
import os
import sys
import io
import time
import json
import re
import signal
import pandas as pd
import asyncio
import threading # Used by asyncio.to_thread internally
import subprocess # Used in AgentTools (agent.py)
import base64 # Used for image handling
from io import BytesIO # Used for image handling
from typing import Any, Dict, List, Optional, Tuple, Union # Added Union
from operator import itemgetter # Used in runnables
import shlex # For safe command splitting in agent tools
import shutil # For copying directory trees in setup_assistant_proxy
import tempfile # For creating temporary files in setup_assistant_proxy
import uuid # For generating request/response IDs
import random
import traceback
#from quart import Quart, Response, request, g, jsonify, current_app # Use Quart imports
# --- Third-Party Library Imports ---
import requests # Used for URL fetching
from bs4 import BeautifulSoup # Used for URL parsing
from loguru import logger # Logging library
from PIL import Image # Used for image handling (optional validation/info)
# numpy import moved inside _find_existing_tot_result to make it optional
import threading
import datetime
import queue

import atexit # To signal shutdown
import datetime
import hashlib
import tempfile
from werkzeug.utils import secure_filename # For safely handling uploaded filenames
import difflib
import contextlib # For ensuring driver quit
from urllib.parse import urlparse, parse_qs, quote_plus, urljoin
import langcodes
import math
import cmath
import easyocr

try:
    ocr_reader = easyocr.Reader(['en'], gpu=True) # Use GPU if available
    logger.success("✅ EasyOCR Reader initialized successfully (using GPU).")
except Exception:
    try:
        ocr_reader = easyocr.Reader(['en'], gpu=False)
        logger.success("✅ EasyOCR Reader initialized successfully (using CPU).")
    except Exception as e:
        ocr_reader = None
        logger.error(f"❌ Failed to initialize EasyOCR Reader. OCR functionality will be disabled. Error: {e}")

# --- Selenium Imports (add if not present) ---
try:
    from selenium import webdriver
    from selenium.webdriver.remote.webdriver import WebDriver # For type hints
    from selenium.webdriver.remote.webelement import WebElement # For type hints
    from selenium.webdriver.common.by import By
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC
    from selenium.common.exceptions import (
        NoSuchElementException, TimeoutException, WebDriverException, InvalidSelectorException
    )
    # Using Chrome specific service/manager
    from selenium.webdriver.chrome.service import Service as ChromeService
    from webdriver_manager.chrome import ChromeDriverManager
    SELENIUM_AVAILABLE = True
    logger.info("✅ Selenium and WebDriver Manager imported.")
except ImportError as e:
    SELENIUM_AVAILABLE = False
    WebDriver = None # Define as None if import fails
    WebDriverException = Exception # Define base exception
    NoSuchElementException = Exception
    TimeoutException = TimeoutError
    logger.error(f"❌ Failed to import Selenium/WebDriverManager: {e}. Web scraping/download tools disabled.")
    logger.error("   Install dependencies: pip install selenium webdriver-manager requests beautifulsoup4")
from network_internet_knowledge_fetcher import search_and_scrape_web_async




# --- SQLAlchemy Imports ---
from sqlalchemy.orm import Session, sessionmaker, attributes # Import sessionmaker
from sqlalchemy import update, inspect as sql_inspect, desc
from sqlalchemy.sql import func

# --- Flask Imports ---
from flask import Flask, request, Response, g, jsonify, redirect # Use Flask imports
from flask import stream_with_context
from flask_cors import CORS

try:
    from shared_state import TaskInterruptedException, server_is_busy_event
except ImportError:
    logger.critical("Failed to import shared_state. Server busy signaling disabled.")
    # Create a dummy event if import fails to avoid crashing later code
    server_is_busy_event = threading.Event()

# --- Langchain Core Imports ---
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_core.output_parsers import StrOutputParser, JsonOutputParser
from langchain_core.runnables import RunnableLambda, RunnablePassthrough, RunnableParallel
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.exceptions import OutputParserException
from langchain_core.documents import Document
from langchain_core.vectorstores import VectorStoreRetriever, VectorStore

# --- Langchain Community Imports ---
#from langchain_community.vectorstores import Chroma # Use Chroma for in-memory history/URL RAG
from langchain_chroma import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter

# --- END PROVIDER IMPORTS ---


# --- Fuzzy Search Imports ---
try:
    from thefuzz import process as fuzz_process, fuzz
    FUZZY_AVAILABLE = True
    logger.info("✅ thefuzz imported.")
except ImportError:
    FUZZY_AVAILABLE = False
    logger.warning("⚠️ thefuzz not installed. Fuzzy search RAG fallback disabled. Run 'pip install thefuzz python-Levenshtein'.")
    fuzz_process = None # Placeholder
    fuzz = None # Placeholder
FUZZY_SEARCH_THRESHOLD_APP = getattr(globals(), 'FUZZY_SEARCH_THRESHOLD', 30) # Default to 80 if not from

try:
    from CortexConfiguration import ENABLE_STELLA_ICARUS_HOOKS # Ensure this is imported
    from stella_icarus_utils import StellaIcarusHookManager, StellaIcarusAdaDaemonManager # <<< NEW IMPORT
except ImportError as e:
    StellaIcarusHookManager = None # Define as None if import fails
    ENABLE_STELLA_ICARUS_HOOKS = False # Default to false if config itself fails
    # ...
# --- NEW: Initialize the Ada Daemon Manager ---
stella_icarus_daemon_manager = StellaIcarusAdaDaemonManager()
logger.success("✅ StellaIcarus Ada Daemon Manager Initialized (not yet started).")

# === Global Semaphores and Concurrency Control ===
# Default to a small number, can be overridden by environment variable if desired
_default_max_bg_tasks = 10 #parallel???
try:
    MAX_CONCURRENT_BACKGROUND_GENERATE_TASKS = int(os.getenv("MAX_CONCURRENT_BACKGROUND_GENERATE_TASKS", _default_max_bg_tasks))
    if MAX_CONCURRENT_BACKGROUND_GENERATE_TASKS <= 0:
        logger.warning(f"MAX_CONCURRENT_BACKGROUND_GENERATE_TASKS was <= 0, defaulting to {_default_max_bg_tasks}")
        MAX_CONCURRENT_BACKGROUND_GENERATE_TASKS = _default_max_bg_tasks
except ValueError:
    logger.warning(f"Invalid value for MAX_CONCURRENT_BACKGROUND_GENERATE_TASKS env var, defaulting to {_default_max_bg_tasks}")
    MAX_CONCURRENT_BACKGROUND_GENERATE_TASKS = _default_max_bg_tasks

logger.info(f"🚦 Max concurrent background generate tasks: {MAX_CONCURRENT_BACKGROUND_GENERATE_TASKS}")
background_generate_task_semaphore = threading.Semaphore(MAX_CONCURRENT_BACKGROUND_GENERATE_TASKS)
# === End Global Semaphores ===

# --- Local Imports with Error Handling ---
try:
    from cortex_backbone_provider import CortexEngine
    # Import database components needed in AdelaideAlbertCortex
    
    from database import (
        init_db, queue_interaction_for_batch_logging, add_interaction, get_recent_interactions, # <<< REMOVED get_db
        get_past_tot_interactions, Interaction, SessionLocal, AppleScriptAttempt, # Added AppleScriptAttempt if needed here
        get_global_recent_interactions, get_pending_tot_result, mark_tot_delivered,
        get_past_applescript_attempts, FileIndex, search_file_index, UploadedFileRecord, add_interaction_no_commit, AppleScriptAttempt # Added new DB function
    )
    # Import all config variables (prompts, settings, etc.)
    from CortexConfiguration import * # Ensure this includes the SQLite DATABASE_URL and all prompts/models
    # Import Agent components
    # Make sure AmaryllisAgent and _start_agent_task are correctly defined/imported if used elsewhere
    from file_indexer import (
        FileIndexer,
        initialize_global_file_index_vectorstore as init_file_vs_from_indexer,  # <<< ADD THIS IMPORT and ALIAS
        get_global_file_index_vectorstore  # You already had this for CortexThoughts
    )
    from agent import AmaryllisAgent, AgentTools, _start_agent_task # Keep Agent imports
except ImportError as e:
    print(f"Error importing local modules (database, config, agent, cortex_backbone_provider): {e}")
    logger.exception("Import Error Traceback:") # Log traceback for import errors
    FileIndexer = None # Define as None if import fails
    FileIndex = None
    search_file_index = None
    sys.exit(1)

from reflection_indexer import (
    initialize_global_reflection_vectorstore,
    index_single_reflection, # If you want CortexThoughts to trigger indexing
    get_global_reflection_vectorstore
)

from interaction_indexer import (
    InteractionIndexer,
    initialize_global_interaction_vectorstore,
    get_global_interaction_vectorstore
)

# --- NEW: Import the custom lock ---

try:
    from priority_lock import PriorityQuotaLock, ELP0, ELP1
    logger.info("✅ Successfully imported PriorityQuotaLock, ELP0, ELP1.")
except ImportError as e:
    logger.error(f"❌ Failed to import from priority_lock.py: {e}")
    logger.warning("    Falling back to standard threading.Lock for priority lock (NO PRIORITY/QUOTA).")
    # Define fallbacks so the rest of the code doesn't crash immediately
    import threading
    PriorityQuotaLock = threading.Lock # type: ignore
    ELP0 = 0
    ELP1 = 1
    # You might want to sys.exit(1) here if priority locking is critical
    sys.exit(1)
interruption_error_marker = "Worker task interrupted by higher priority request" # Define consistently

#background_generate semaphore
background_generate_task_semaphore = threading.Semaphore(MAX_CONCURRENT_BACKGROUND_GENERATE_TASKS)


# --- End Local Imports ---

# Add the inspection code again *after* these imports
logger.debug("--- Inspecting Interaction Model Columns AFTER explicit import ---")
logger.debug(f"Columns found by SQLAlchemy: {[c.name for c in Interaction.__table__.columns]}")
if 'tot_delivered' in [c.name for c in Interaction.__table__.columns]: logger.debug("✅ 'tot_delivered' column IS present.")
else: logger.error("❌ 'tot_delivered' column IS STILL MISSING!")
logger.debug("-------------------------------------------------------------")


try:
    import tiktoken
    # Attempt to load the encoder once globally for CortexThoughts if not already done by worker logic
    # Or load it on demand in the helper function.
    # For simplicity here, assume it's available or loaded in a helper.
    TIKTOKEN_AVAILABLE_APP = True
    # Try to get a common encoder
    try:
        cl100k_base_encoder_app = tiktoken.get_encoding("cl100k_base")
    except Exception:
        cl100k_base_encoder_app = tiktoken.encoding_for_model("gpt-4") # Fallback
except ImportError:
    logger.warning("tiktoken not available in AdelaideAlbertCortex. Context truncation will be less accurate (char-based).")
    TIKTOKEN_AVAILABLE_APP = False
    cl100k_base_encoder_app = None


# --- Constants for Streaming ---
LOG_QUEUE_TIMEOUT = 0.05 # How long generator waits for a log message (seconds)
PLACEHOLDER_MESSAGE = "(LoadingStreamingBase)(DoNotPanic)(WellBeRightBack)"
LOG_SINK_LEVEL = "DEBUG" # Minimum log level to forward to client
LOG_SINK_FORMAT = "<green>{time:HH:mm:ss.SSS}</green> | <level>{level: <8}</level> | <level>{message}</level>" # Example format

# --- Assistant Proxy App Constants ---
ASSISTANT_PROXY_APP_NAME = "AdelaideHijackAppleBridge.app" # Your chosen app name
ASSISTANT_PROXY_DEST_PATH = f"/Applications/{ASSISTANT_PROXY_APP_NAME}"
# --- DEFINE SCRIPT_DIR HERE ---
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__)) # Get the directory containing AdelaideAlbertCortex
# --- Use SCRIPT_DIR to define the source path ---
ASSISTANT_PROXY_SOURCE_PATH = os.path.join(SCRIPT_DIR, "AssistantProxy.applescript") # Assumes script is next to AdelaideAlbertCortex
# --- NEW: Define Search Download Directory ---
SEARCH_DOWNLOAD_DIR = os.path.join(SCRIPT_DIR, "LiteratureReviewPool")
# --- END NEW ---


# --- Logger Configuration ---
logger.remove()
logger.add(sys.stderr, format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - <level>{message}</level>", level="DEBUG")
logger.info("📝 Logger configured")
logger.info(f"🐍 Running in directory: {os.getcwd()}")
logger.info(f"🔧 Script directory: {os.path.dirname(os.path.abspath(__file__))}")


# --- ADD DEBUGGING STEP ---
try:
    from database import Interaction # Make sure Interaction is imported
    logger.debug("--- Inspecting Interaction Model Columns BEFORE init_db ---")
    logger.debug(f"Columns found by SQLAlchemy: {[c.name for c in Interaction.__table__.columns]}")
    if 'tot_delivered' in [c.name for c in Interaction.__table__.columns]:
        logger.debug("✅ 'tot_delivered' column IS present in mapped model.")
    else:
        logger.error("❌ 'tot_delivered' column IS MISSING from mapped model BEFORE init_db!")
    logger.debug("---------------------------------------------------------")
except Exception as inspect_err:
    logger.error(f"Failed to inspect Interaction model: {inspect_err}")
# --- END DEBUGGING STEP ---

# --- Initialize Database ---


# --- Determine Python Executable ---
# This will be the Python interpreter that is currently running AdelaideAlbertCortex
# When launched via launcher.py, this will be the venv Python.
APP_PYTHON_EXECUTABLE = sys.executable
logger.info(f"🐍 AdelaideAlbertCortex is running with Python: {APP_PYTHON_EXECUTABLE}")
PYTHON_EXECUTABLE = APP_PYTHON_EXECUTABLE
# ---

# === Global Indexer Thread Management ===

# === Global AI Instances ===



_indexer_thread: Optional[threading.Thread] = None
_indexer_stop_event = threading.Event()

def start_file_indexer():
    """Starts the background file indexer thread."""
    global _indexer_thread, cortex_backbone_provider # <<< Need cortex_backbone_provider here
    if not FileIndexer:
        logger.error("Cannot start file indexer: FileIndexer class not available (import failed?).")
        return
    if not cortex_backbone_provider: # <<< Check if CortexEngine initialized successfully
        logger.error("Cannot start file indexer: CortexEngine (and embedding model) not available.")
        return

    # --- Get embedding model ---
    embedding_model = cortex_backbone_provider.embeddings
    if not embedding_model:
        logger.error("Cannot start file indexer: Embedding model not found within CortexEngine.")
        return
    # --- End get embedding model ---

    if _indexer_thread is None or not _indexer_thread.is_alive():
        logger.info("🚀 Starting background file indexer service...")
        try:
            # --- Pass embedding_model to FileIndexer ---
            indexer_instance = FileIndexer(
                stop_event=_indexer_stop_event,
                provider=cortex_backbone_provider,
                server_busy_event=server_is_busy_event # <<< Pass the busy event
            )
            # --- End pass embedding_model ---
            _indexer_thread = threading.Thread(
                target=indexer_instance.run,
                name="FileIndexerThread",
                daemon=True
            )
            _indexer_thread.start()
            logger.success("✅ File indexer thread started successfully.")
        except Exception as e:
            logger.critical(f"🔥🔥 Failed to instantiate or start FileIndexer thread: {e}")
            logger.exception("Indexer Startup Traceback:")
    else:
        logger.warning("🤔 File indexer thread already running.")

# === NEW: Global Self-Reflection Thread Management ===
_reflector_thread: Optional[threading.Thread] = None
_reflector_stop_event = threading.Event()
_reflector_lock = threading.Lock() # Lock to prevent concurrent reflection cycles if one runs long


def run_self_reflection_loop():
    """
    Main loop for self-reflection. Periodically processes eligible interactions.
    If no new interactions, may proactively re-queue an old one for re-reflection.
    """
    global cortex_backbone_provider, cortex_text_interaction  # Assuming these are global instances
    thread_name = threading.current_thread().name
    logger.info(f"✅ {thread_name} started (Continuous Reflection Logic).")

    if not cortex_backbone_provider or not cortex_text_interaction:
        logger.error(f"🛑 {thread_name}: CortexEngine or CortexThoughts not initialized. Cannot run reflection.")
        return

    logger.info(
        f"{thread_name}: Config - BatchSize={REFLECTION_BATCH_SIZE}, IdleWait={IDLE_WAIT_SECONDS}s, ActivePause={ACTIVE_CYCLE_PAUSE_SECONDS}s")
    logger.info(
        f"{thread_name}: Config - ProactiveReReflect={ENABLE_PROACTIVE_RE_REFLECTION}, Chance={PROACTIVE_RE_REFLECTION_CHANCE}, MinAgeDays={MIN_AGE_FOR_RE_REFLECTION_DAYS}")
    logger.info(f"{thread_name}: Eligible Input Types for new reflection: {REFLECTION_ELIGIBLE_INPUT_TYPES}")

    while not _reflector_stop_event.is_set():
        cycle_start_time = time.monotonic()
        total_processed_this_active_cycle = 0
        work_found_in_this_cycle = False

        logger.info(f"🤔 {thread_name}: Starting ACTIVE reflection cycle...")

        if not _reflector_lock.acquire(blocking=False):
            logger.warning(f"{thread_name}: Previous reflection cycle lock still held? Skipping this cycle attempt.")
            _reflector_stop_event.wait(timeout=IDLE_WAIT_SECONDS)  # Wait before trying again
            continue

        db: Optional[Session] = None
        try:
            # Wait if server is busy (main API requests are active)
            was_busy_waiting = False
            while server_is_busy_event.is_set():
                if not was_busy_waiting:
                    logger.info(f"🚦 {thread_name}: Server busy, pausing reflection start...")
                    was_busy_waiting = True
                if _reflector_stop_event.is_set(): break
                if _reflector_stop_event.wait(timeout=0.5): break
            if was_busy_waiting: logger.info(f"🟢 {thread_name}: Server free, resuming reflection cycle.")
            if _reflector_stop_event.is_set(): break

            db = SessionLocal()  # type: ignore
            if not db: raise RuntimeError("Failed to create DB session for reflection cycle.")
            logger.trace(f"{thread_name}: DB Session created for active cycle.")

            # --- Inner Loop: Process NEW interactions needing reflection ---
            while not _reflector_stop_event.is_set():
                batch_processed_count_this_inner_loop = 0
                interactions_to_reflect: List[Interaction] = []
                try:
                    query = db.query(Interaction).filter(
                        Interaction.reflection_completed == False,
                        Interaction.mode == 'chat',
                        Interaction.input_type.in_(REFLECTION_ELIGIBLE_INPUT_TYPES)
                    ).order_by(Interaction.timestamp.asc()).limit(REFLECTION_BATCH_SIZE)
                    interactions_to_reflect = query.all()
                except Exception as query_err:
                    logger.error(f"{thread_name}: Error querying new interactions for reflection: {query_err}")
                    _reflector_stop_event.wait(timeout=5)  # Wait before breaking inner loop
                    break

                if not interactions_to_reflect:
                    logger.debug(f"{thread_name}: No new eligible interactions found in this batch/query.")
                    break  # Exit the inner batch-processing loop

                work_found_in_this_cycle = True
                logger.info(
                    f"{thread_name}: Found {len(interactions_to_reflect)} new interaction(s) for reflection. Processing batch...")

                for interaction_obj in interactions_to_reflect:  # Renamed to avoid conflict
                    if _reflector_stop_event.is_set(): logger.info(f"{thread_name}: Stop signal during batch."); break
                    if server_is_busy_event.is_set(): logger.info(
                        f"{thread_name}: Server became busy, pausing batch."); break

                    original_input_text = interaction_obj.user_input or "[Original input missing]"
                    original_interaction_id = interaction_obj.id
                    original_input_type_text = interaction_obj.input_type

                    logger.info(
                        f"{thread_name}: --> Triggering reflection for Interaction ID {original_interaction_id} (Type: {original_input_type_text}) - Input: '{original_input_text[:60]}...'")

                    reflection_session_id_for_bg = f"reflection_{original_interaction_id}_{str(uuid.uuid4())[:4]}"
                    task_launched_successfully = False
                    try:
                        # background_generate is async, run it and wait for it to complete
                        # The db session is passed to it.
                        asyncio.run(  # This blocks the current thread until the async function completes
                            cortex_text_interaction.background_generate(  # type: ignore
                                db=db,
                                user_input=original_input_text,  # This is the content to reflect upon
                                session_id=reflection_session_id_for_bg,
                                classification="chat_complex",  # Force complex for reflection
                                image_b64=None,
                                update_interaction_id=original_interaction_id  # Key for reflection
                            )
                        )
                        # If asyncio.run completes without exception, assume the core logic of background_generate
                        # (including its own error handling and DB updates for the original interaction) finished.
                        logger.info(
                            f"{thread_name}: --> Background reflection task for ID {original_interaction_id} completed its execution path.")
                        task_launched_successfully = True  # Indicates background_generate ran
                        batch_processed_count_this_inner_loop += 1
                    except Exception as trigger_err:
                        logger.error(
                            f"{thread_name}: Failed to run/complete background_generate for reflection on ID {original_interaction_id}: {trigger_err}")
                        logger.exception(f"{thread_name} Background Generate Trigger/Run Traceback:")
                        # If background_generate itself fails, the original interaction's reflection_completed
                        # status should ideally remain False due to error handling within background_generate.
                        # We just log here and continue to the next item in batch.

                    # No explicit marking of original_interaction.reflection_completed = True here.
                    # That is now handled *inside* background_generate's finally block.

                    if not _reflector_stop_event.is_set() and ACTIVE_CYCLE_PAUSE_SECONDS > 0:
                        _reflector_stop_event.wait(timeout=ACTIVE_CYCLE_PAUSE_SECONDS)

                total_processed_this_active_cycle += batch_processed_count_this_inner_loop
                logger.info(
                    f"{thread_name}: Finished processing batch ({batch_processed_count_this_inner_loop} items). Total this cycle: {total_processed_this_active_cycle}.")
                if _reflector_stop_event.is_set() or server_is_busy_event.is_set(): break
                # --- End of Inner Loop for NEW interactions ---

            # --- "Gabut State" - Proactive Re-Reflection Logic ---
            if not work_found_in_this_cycle and ENABLE_PROACTIVE_RE_REFLECTION and random.random() < PROACTIVE_RE_REFLECTION_CHANCE:
                logger.info(f"{thread_name}: Gabut State - No new work. Attempting proactive re-reflection.")
                re_reflected_interaction_id: Optional[int] = None
                try:
                    time_threshold = datetime.datetime.now(datetime.timezone.utc) - datetime.timedelta(
                        days=MIN_AGE_FOR_RE_REFLECTION_DAYS)

                    # For SQLite ORDER BY RANDOM()
                    order_by_clause = func.random() if db.bind and db.bind.dialect.name == 'sqlite' else Interaction.timestamp
                    # For other DBs, random might be different or less efficient. Fallback to oldest.
                    if db.bind and db.bind.dialect.name != 'sqlite':
                        logger.warning(
                            f"{thread_name}: RANDOM() for re-reflection query might be inefficient for {db.bind.dialect.name}. Consider dialect-specific random or oldest.")

                    candidate_for_re_reflection = db.query(Interaction).filter(
                        Interaction.reflection_completed == True,
                        Interaction.mode == 'chat',
                        Interaction.input_type.in_(REFLECTION_ELIGIBLE_INPUT_TYPES),
                        Interaction.timestamp < time_threshold
                    ).order_by(order_by_clause).limit(1).first()

                    if candidate_for_re_reflection:
                        logger.info(
                            f"{thread_name}: Selected Interaction ID {candidate_for_re_reflection.id} (Type: {candidate_for_re_reflection.input_type}) for proactive re-reflection.")

                        candidate_for_re_reflection.reflection_completed = False
                        note = f"\n\n[System Re-queued for Reflection ({datetime.datetime.now(datetime.timezone.utc).isoformat()}) due to Gabut State]"
                        current_resp = candidate_for_re_reflection.llm_response or ""
                        candidate_for_re_reflection.llm_response = (current_resp + note)[
                                                                   :getattr(Interaction.llm_response.type, 'length',
                                                                            4000)]
                        # last_modified_db will be updated by SQLAlchemy's onupdate if configured, or manually:
                        # candidate_for_re_reflection.last_modified_db = datetime.datetime.now(datetime.timezone.utc)

                        db.commit()
                        re_reflected_interaction_id = candidate_for_re_reflection.id
                        work_found_in_this_cycle = True  # Signal that work was "found" for next cycle logic
                        logger.info(
                            f"{thread_name}: Interaction ID {re_reflected_interaction_id} re-queued for reflection.")
                    else:
                        logger.info(f"{thread_name}: No suitable old interactions found for proactive re-reflection.")
                except Exception as e_re_reflect:
                    logger.error(f"{thread_name}: Error during proactive re-reflection: {e_re_reflect}")
                    if db: db.rollback()
            # --- END "Gabut State" Logic ---

        except RuntimeError as rt_err:  # Catch DB session creation failure
            logger.error(f"💥 {thread_name}: Runtime error in reflection cycle (likely DB session): {rt_err}")
        except Exception as cycle_err:
            logger.error(f"💥 {thread_name}: Unhandled error during active reflection cycle: {cycle_err}")
            logger.exception(f"{thread_name} Cycle Traceback:")
            if db: db.rollback()
        finally:
            if db:
                try:
                    db.close(); logger.debug(f"{thread_name}: DB session closed for cycle.")
                except Exception as close_err:
                    logger.error(f"{thread_name}: Error closing DB session: {close_err}")

            try:
                _reflector_lock.release(); logger.trace(f"{thread_name}: Released cycle lock.")
            except (threading.ThreadError, RuntimeError) as lk_err:
                logger.warning(f"{thread_name}: Lock release issue: {lk_err}")

            cycle_duration = time.monotonic() - cycle_start_time
            logger.info(
                f"{thread_name}: ACTIVE reflection cycle finished in {cycle_duration:.2f}s. Processed: {total_processed_this_active_cycle} new interaction(s).")

        # --- Wait logic before next full cycle check ---
        wait_time_seconds = IDLE_WAIT_SECONDS if not work_found_in_this_cycle else ACTIVE_CYCLE_PAUSE_SECONDS
        if wait_time_seconds > 0 and not _reflector_stop_event.is_set():
            logger.debug(f"{thread_name}: Waiting {wait_time_seconds:.2f} seconds before next cycle check...")
            stopped_during_wait = _reflector_stop_event.wait(timeout=wait_time_seconds)
            if stopped_during_wait: logger.info(f"{thread_name}: Stop signal received during idle wait."); break
        elif _reflector_stop_event.is_set():  # If stop set and no wait needed
            logger.info(f"{thread_name}: Stop signal detected after active cycle.")
            break

    logger.info(f"🛑 {thread_name}: Exiting self-reflection loop.")


def start_self_reflector():
    """Starts the background self-reflection thread."""
    global _reflector_thread
    if not ENABLE_SELF_REFLECTION:
        logger.info("🤔 Self-reflection thread disabled via config.")
        return

    if _reflector_thread is None or not _reflector_thread.is_alive():
        logger.info("🚀 Starting background self-reflection service...")
        try:
            _reflector_stop_event.clear() # Ensure stop event is not set
            _reflector_thread = threading.Thread(
                target=run_self_reflection_loop,
                name="SelfReflectorThread",
                daemon=True
            )
            _reflector_thread.start()
            logger.success("✅ Self-reflection thread started successfully.")
        except Exception as e:
            logger.critical(f"🔥🔥 Failed to start SelfReflector thread: {e}")
            logger.exception("Reflector Startup Traceback:")
    else:
        logger.warning("🤔 Self-reflection thread already running.")

def stop_self_reflector():
    """Signals the self-reflection thread to stop."""
    global _reflector_thread
    if not ENABLE_SELF_REFLECTION: return # Don't try to stop if disabled

    if _reflector_thread and _reflector_thread.is_alive():
        logger.info("Signaling self-reflection thread to stop...")
        _reflector_stop_event.set()
        # Optional: Wait for thread to finish (might take time if in sleep)
        # _reflector_thread.join(timeout=10)
        # if _reflector_thread.is_alive(): logger.warning("Self-reflection thread did not stop within timeout.")
        logger.info("Stop signal sent to self-reflection thread.")
    else:
        logger.info("Self-reflection thread not running or already stopped.")

def stop_file_indexer():
    """Signals the file indexer thread to stop."""
    global _indexer_thread
    if _indexer_thread and _indexer_thread.is_alive():
        logger.info("Signaling file indexer thread to stop...")
        _indexer_stop_event.set()
        # Optional: Wait for thread to finish with a timeout
        # _indexer_thread.join(timeout=30) # Wait up to 30 seconds
        # if _indexer_thread.is_alive():
        #     logger.warning("File indexer thread did not stop within timeout.")
        logger.info("Stop signal sent to file indexer thread.")
    else:
        logger.info("File indexer thread not running or already stopped.")

# Register the stop function to run when the application exits
atexit.register(stop_file_indexer)
# Register the stop function for application exit
atexit.register(stop_self_reflector)
def shutdown_app_services():
    """A single shutdown hook to orchestrate a graceful shutdown of all background services."""
    logger.info("--- Orchestrating graceful shutdown of all background services... ---")

    # 1. Stop the provider's internal threads first (like the relaxation thread)
    if 'cortex_backbone_provider' in globals() and cortex_backbone_provider and hasattr(cortex_backbone_provider, '_priority_quota_lock'):
        if hasattr(cortex_backbone_provider._priority_quota_lock, 'shutdown_relaxation_thread'):
            logger.info("Shutting down AgenticRelaxationThread...")
            cortex_backbone_provider._priority_quota_lock.shutdown_relaxation_thread()


    # 2. Stop the StellaIcarus Ada Daemons
    if 'stella_icarus_daemon_manager' in globals() and stella_icarus_daemon_manager:
        logger.info("Shutting down StellaIcarus Ada Daemons...")
        stella_icarus_daemon_manager.stop_all()

    # 3. Stop the application's main background threads
    logger.info("Shutting down Self Reflector and File Indexer...")
    stop_self_reflector()
    stop_file_indexer()


    # The database log writer and compression hooks registered with atexit elsewhere will run after this.
    logger.info("--- Graceful shutdown sequence initiated. ---")
# Register the single, organized shutdown function
atexit.register(shutdown_app_services)

# json Fixer
def _extract_json_candidate_string(raw_llm_text: str, log_prefix: str = "JSONExtract") -> Optional[str]:
    """
    Robustly extracts a potential JSON string from raw LLM text.
    Handles <think> tags, markdown code blocks, and finding outermost braces.
    """
    if not raw_llm_text or not isinstance(raw_llm_text, str):
        return None

    logger.trace(f"{log_prefix}: Starting JSON candidate extraction from raw text (len {len(raw_llm_text)}).")

    # 1. Remove <think> tags and ChatML tokens first
    # Remove <think> tags
    text_cleaned = re.sub(r'<think>.*?</think>', '', raw_llm_text, flags=re.DOTALL | re.IGNORECASE)
    # Remove ChatML tokens
    text_cleaned = re.sub(r'<\|im_start\|>.*?<\|im_end\|>', '', text_cleaned, flags=re.DOTALL | re.IGNORECASE)
    text_after_think_removal = text_cleaned.strip()
    if not text_after_think_removal:
        logger.trace(f"{log_prefix}: Text empty after <think> and ChatML token removal.")
        return None

    # 2. Remove common LLM preambles/postambles around the JSON content
    cleaned_text = text_after_think_removal
    # Remove common assistant start patterns, case insensitive
    cleaned_text = re.sub(r"^\s*(assistant\s*\n?)?(<\|im_start\|>\s*(system|assistant)\s*\n?)?", "", cleaned_text,
                          flags=re.IGNORECASE).lstrip()
    # Remove trailing ChatML end token
    if CHATML_END_TOKEN and cleaned_text.endswith(CHATML_END_TOKEN):  # CHATML_END_TOKEN from CortexConfiguration
        cleaned_text = cleaned_text[:-len(CHATML_END_TOKEN)].strip()

    # 3. Look for JSON within markdown code blocks (```json ... ```)
    json_markdown_match = re.search(r"```json\s*(\{[\s\S]*?\})\s*```", cleaned_text, re.DOTALL)
    if json_markdown_match:
        extracted_str = json_markdown_match.group(1).strip()
        logger.trace(f"{log_prefix}: Extracted JSON from markdown block: '{extracted_str[:100]}...'")
        return extracted_str

    # 4. If not in markdown, find the first '{' and last '}' that likely enclose the main JSON object
    # This helps strip extraneous text before the first '{' or after the last '}'.
    first_brace = cleaned_text.find('{')
    last_brace = cleaned_text.rfind('}')
    if first_brace != -1 and last_brace != -1 and last_brace > first_brace:
        extracted_str = cleaned_text[first_brace: last_brace + 1].strip()
        logger.trace(f"{log_prefix}: Extracted JSON using outermost braces: '{extracted_str[:100]}...'")
        return extracted_str

    # 5. If still no clear structure, but text might be just the JSON (e.g. after LLM reformat)
    if cleaned_text.startswith("{") and cleaned_text.endswith("}"):
        logger.trace(f"{log_prefix}: Assuming cleaned text itself is the JSON candidate: '{cleaned_text[:100]}...'")
        return cleaned_text

    logger.warning(
        f"{log_prefix}: No clear JSON structure found after extraction attempts. Raw (after think): '{text_after_think_removal[:100]}...'")
    return None  # Return None if no candidate found


def _programmatic_json_parse_and_fix(
        json_candidate_str: str,
        max_fix_attempts: int = 3,  # How many times to try fixing based on errors
        log_prefix: str = "JSONFixParse"
) -> Optional[Union[Dict, List]]:
    """
    Attempts to parse a JSON string, applying a limited set of programmatic fixes
    if initial parsing fails. Iterates up to max_fix_attempts.
    Returns the parsed Python object (dict/list) or None.
    """
    if not json_candidate_str or not isinstance(json_candidate_str, str):
        return None

    current_text_to_parse = json_candidate_str
    json_parser = JsonOutputParser()  # Or just use json.loads directly

    for attempt in range(max_fix_attempts):
        logger.debug(
            f"{log_prefix}: Parse/fix attempt {attempt + 1}/{max_fix_attempts}. Current text snippet: '{current_text_to_parse[:100]}...'")
        try:
            # Standard parse attempt
            parsed_object = json_parser.parse(
                current_text_to_parse)  # Langchain's parser can sometimes fix minor issues
            # Or use: parsed_object = json.loads(current_text_to_parse)
            logger.debug(f"{log_prefix}: Successfully parsed JSON on attempt {attempt + 1}.")
            return parsed_object
        except (json.JSONDecodeError, OutputParserException) as e:
            logger.warning(f"{log_prefix}: Attempt {attempt + 1} parse failed: {type(e).__name__} - {str(e)[:100]}")

            if attempt == max_fix_attempts - 1:  # Last attempt failed, don't try to fix further
                logger.error(f"{log_prefix}: Max fix attempts reached. Could not parse after error: {e}")
                break

            # --- Apply limited programmatic fixes based on common LLM issues ---
            text_before_fixes = current_text_to_parse

            # Fix 1: Remove trailing commas (before closing brackets/braces)
            # This is a common issue. Python's json.loads tolerates ONE, but stricter parsers/standards don't.
            current_text_to_parse = re.sub(r",\s*([\}\]])", r"\1", current_text_to_parse)
            if current_text_to_parse != text_before_fixes:
                logger.debug(f"{log_prefix}: Applied trailing comma fix.")

            # Fix 2: Attempt to add quotes to unquoted keys (very heuristic and simple cases)
            # Looks for { or , followed by whitespace, then word chars (key), then whitespace, then :
            # This is risky and might not work for all cases or could corrupt complex keys.
            # Example: { key : "value"} -> { "key" : "value"}
            # Example: [{"key" : "value", next_key : "next_value"}] -> [{"key" : "value", "next_key" : "next_value"}]
            # Only apply if the error message suggests unquoted keys, if possible (hard to check generically here)
            if "Expecting property name enclosed in double quotes" in str(e):
                text_after_key_quote_fix = re.sub(r"([{,\s])(\w+)(\s*:)", r'\1"\2"\3', current_text_to_parse)
                if text_after_key_quote_fix != current_text_to_parse:
                    logger.debug(f"{log_prefix}: Applied heuristic key quoting fix.")
                    current_text_to_parse = text_after_key_quote_fix

            # Fix 3: Remove JavaScript-style comments
            text_after_comment_removal = re.sub(r"//.*?\n", "\n", current_text_to_parse, flags=re.MULTILINE)
            text_after_comment_removal = re.sub(r"/\*.*?\*/", "", text_after_comment_removal, flags=re.DOTALL)
            if text_after_comment_removal != current_text_to_parse:
                logger.debug(f"{log_prefix}: Applied JS comment removal fix.")
                current_text_to_parse = text_after_comment_removal.strip()

            # If no fixes changed the string, and it's not the last attempt,
            # it means our simple fixes aren't working for this error.
            if current_text_to_parse == text_before_fixes and attempt < max_fix_attempts - 1:
                logger.warning(
                    f"{log_prefix}: Programmatic fixes did not alter the string for error '{str(e)[:50]}...'. Further fixes might be needed or error is complex.")
                # For more complex errors, one might analyze e.pos, e.msg here, but it's very hard.
                # We are relying on the LLM re-request to do most heavy lifting.
                break  # Break from fix attempts if no change, let outer loop handle if it's the LLM re-request loop

    logger.error(
        f"{log_prefix}: Failed to parse JSON after all programmatic fix attempts. Original candidate: '{json_candidate_str[:200]}...'")
    return None


# --- Flask App Setup ---
APP_START_TIME = time.monotonic()
app = Flask(__name__) # Use Flask app

#Allowing recieving big dataset
#app.config['MAX_CONTENT_LENGTH'] = 2 ** 63
app.config['MAX_CONTENT_LENGTH'] = None
#app.config['MAX_CONTENT_LENGTH'] = 3 * 1024 * 1024 * 1024 + 500 * 1024 * 1024


#This is important for Zephy GUI to work
#CORS(app, resources={r"/*": {"origins": "http://localhost:5173"}})
CORS(app) #Allow all origin

# --- Request Context Functions for DB ---
@app.before_request
def setup_and_log_request():
    """Opens DB session and logs incoming request details."""
    # 0. Signal Busy Start
    if not server_is_busy_event.is_set(): # Avoid unnecessary locking if already set
        logger.trace("--> Request IN - Setting server_is_busy_event")
        server_is_busy_event.set()
    # --- If using a counter approach ---
    # g.request_count = getattr(g, 'request_count', 0) + 1
    # if g.request_count == 1:
    #    server_is_busy_event.set()
    # ---
    # 1. Open DB session
    try:
        g.db = SessionLocal()
        logger.trace("DB session opened for request.")
    except Exception as db_err:
        logger.error(f"!!! FAILED TO OPEN DB SESSION in before_request: {db_err}")
        g.db = None # Ensure g.db is None if opening failed

    # 2. Log Incoming Request
    try:
        headers = dict(request.headers)
        content_type = headers.get("Content-Type", "N/A")
        content_length = headers.get("Content-Length", "N/A")
        remote_addr = request.remote_addr or "Unknown"
        query_string = request.query_string.decode() if request.query_string else ""

        log_message = (
            f"--> REQ IN : {request.method} {request.path} "
            f"QS='{query_string}' "
            f"From={remote_addr} "
            f"Type={content_type} Len={content_length}"
        )
        logger.info(log_message)
        logger.debug(f"    REQ Headers: {json.dumps(headers, indent=2)}")
        # Body logging snippet (optional, use with caution) - Same as before
        # if request.content_length and request.content_length < 5000: # Only log small bodies
        #    try:
        #        body_snippet = request.get_data(as_text=True)[:500] # Read snippet
        #        logger.debug(f"    REQ Body Snippet: {body_snippet}...")
        #    except Exception as body_err: logger.warning(f"    REQ Body: Error reading snippet: {body_err}")
        # elif request.content_length: logger.debug(f"    REQ Body: Exists but not logging snippet (Length: {request.content_length}).")
        # else: logger.debug("    REQ Body: No body or zero length.")

    except Exception as log_err:
        logger.error(f"!!! Error during incoming request logging: {log_err}")
        # Continue processing the request anyway

@app.after_request
def add_cors_headers(response):
    # Only add headers for the specified origin
    # You can add more complex logic here if needed (e.g., checking request.origin)
    response.headers['Access-Control-Allow-Origin'] = '*' #everywhere
    response.headers['Access-Control-Allow-Headers'] = 'Content-Type,Authorization' # Add any other headers your frontend might send
    response.headers['Access-Control-Allow-Methods'] = 'GET,PUT,POST,DELETE,OPTIONS' # Add all methods your frontend uses
    response.headers['Access-Control-Allow-Credentials'] = 'true' # If you send cookies/credentials
    return response

@app.after_request
def log_and_clear_busy(response: Response) -> Response:
    """Logs details of the outgoing response AFTER the route handler."""
    # (Keep the exact same logic as the previous log_outgoing_response function)
    try:
        if request:
            log_message = (
                f"<-- RESP OUT: {request.method} {request.path} "
                f"Status={response.status_code} "
                f"Type={response.content_type} Len={response.content_length}"
            )
            logger.info(log_message)
            if (not response.is_streamed
                and response.mimetype == 'application/json'
                and response.content_length is not None
                and response.content_length < 10000):
                try:
                    data = response.get_data(as_text=True)
                    logger.debug(f"    RESP Body Snippet: {data[:500]}...")
                except Exception: logger.debug("    RESP Body: Could not get/decode JSON data for logging snippet.")
            elif response.is_streamed: logger.debug("    RESP Body: Streamed response (not logging snippet).")
            else: logger.debug(f"    RESP Body: Not logging snippet (Type: {response.mimetype}, Streamed: {response.is_streamed}).")
        else:
            logger.warning("!!! Response logging skipped: Request context not found.")
    except Exception as log_err:
        logger.error(f"!!! Error during outgoing response logging: {log_err}")
    finally:
        if server_is_busy_event.is_set():
            logger.trace("<-- Request OUT - Clearing server_is_busy_event")
            server_is_busy_event.clear()
        return response

@app.teardown_request
def teardown_request_db(exception=None): # Use your original function name if you prefer
    """Close the DB session after each request."""
    db = g.pop('db', None)
    if db is not None:
        db.close()
        logger.trace("DB session closed for request.")
    if exception:
         # Log the exception that might have caused the teardown
         logger.error(f"Exception during request: {exception}")



def setup_assistant_proxy():
    """Reads AssistantProxy.applescript, compiles it, copies to /Applications, and attempts permission priming."""
    logger.info(f"Checking/Creating Assistant Proxy at {ASSISTANT_PROXY_DEST_PATH}...")

    # 1. Check if source AppleScript file exists
    if not os.path.isfile(ASSISTANT_PROXY_SOURCE_PATH):
        logger.critical(f"❌ Source AppleScript file not found at: {ASSISTANT_PROXY_SOURCE_PATH}")
        logger.critical("   Cannot create the Assistant Proxy application.")
        return False

    # Create a temporary directory for compilation
    with tempfile.TemporaryDirectory() as tmpdir:
        compiled_app_path_tmp = os.path.join(tmpdir, ASSISTANT_PROXY_APP_NAME)

        # 2. Compile the AppleScript from file into an Application bundle
        compile_cmd = ["osacompile", "-o", compiled_app_path_tmp, ASSISTANT_PROXY_SOURCE_PATH]
        logger.debug(f"Running osacompile: {' '.join(compile_cmd)}")
        try:
            process = subprocess.run(compile_cmd, capture_output=True, text=True, check=True)
            logger.debug(f"osacompile stdout: {process.stdout}")
            if process.stderr: # Log stderr even on success, might contain warnings
                 logger.warning(f"osacompile stderr: {process.stderr}")
            logger.success(f"✅ Successfully compiled proxy app in temporary location: {compiled_app_path_tmp}")
        except subprocess.CalledProcessError as e:
            logger.error(f"❌ osacompile failed (RC={e.returncode}): {e.stderr or e.stdout}")
            logger.error(f"   Check syntax in source file: {ASSISTANT_PROXY_SOURCE_PATH}")
            return False
        except FileNotFoundError:
            logger.error("❌ osacompile command not found. Is Xcode Command Line Tools installed?")
            return False
        except Exception as e:
            logger.error(f"❌ Error running osacompile: {e}")
            return False

        # 3. Copy the compiled .app to /Applications (requires sudo privileges)
        logger.info(f"Attempting to copy compiled app to {ASSISTANT_PROXY_DEST_PATH}...")
        try:
            if os.path.exists(ASSISTANT_PROXY_DEST_PATH):
                logger.warning(f"'{ASSISTANT_PROXY_DEST_PATH}' already exists. Removing old version (requires sudo).")
                # Use sudo directly since the script is assumed to run with sudo
                subprocess.run(["rm", "-rf", ASSISTANT_PROXY_DEST_PATH], check=True)

            # Use sudo directly for copy
            copy_cmd = ["cp", "-R", compiled_app_path_tmp, "/Applications/"]
            logger.debug(f"Running copy command: sudo {' '.join(copy_cmd)}")
            subprocess.run(copy_cmd, check=True) # Run without explicit sudo here, as parent script has it

            logger.success(f"✅ Successfully copied '{ASSISTANT_PROXY_APP_NAME}' to /Applications.")

            # --- 4. Attempt to Prime Permissions ---
            logger.info("Attempting to trigger initial permission prompts (may require user interaction)...")
            priming_action_details = {
                "actionType": "prime_permissions",
                "actionParamsJSON": "{}" # No specific params needed for priming
            }
            # Prepare osascript command to call the new handler
            params_json_str = priming_action_details["actionParamsJSON"]
            escaped_json_param = json.dumps(params_json_str) # Double encode
            applescript_command = f'''
            tell application "{ASSISTANT_PROXY_DEST_PATH}"
                handleAction given parameters:{{actionType:"prime_permissions", actionParamsJSON:{escaped_json_param}}}
            end tell
            '''
            osa_command = ["osascript", "-e", applescript_command]
            try:
                logger.debug(f"Running permission priming command: {osa_command}")
                # Run with a short timeout, don't check return code as errors are expected if permissions denied
                prime_process = subprocess.run(osa_command, capture_output=True, text=True, timeout=15, check=False)
                logger.info("Permission priming command sent.")
                if prime_process.stdout: logger.debug(f"Priming stdout: {prime_process.stdout.strip()}")
                if prime_process.stderr: logger.warning(f"Priming stderr: {prime_process.stderr.strip()}") # Stderr expected if prompts shown/denied
            except subprocess.TimeoutExpired:
                logger.warning("Permission priming script timed out (might be waiting for user input).")
            except Exception as prime_e:
                logger.warning(f"Failed to run priming script (this might be ok): {prime_e}")
            # --- End Priming ---

            # --- Final User Instructions ---
            print("-" * 60)
            print(f"IMPORTANT: Assistant Proxy Setup Complete!")
            print(f"'{ASSISTANT_PROXY_APP_NAME}' is now in /Applications.")
            print("\n>>> PERMISSION PROMPTS MAY HAVE APPEARED <<<")
            print("If macOS asked for permission to access Calendars, Contacts,")
            print("Reminders, etc., please ensure you clicked 'OK'/'Allow'.")
            print("\n>>> PLEASE MANUALLY CHECK/GRANT PERMISSIONS <<<")
            print("1. Open 'System Settings' > 'Privacy & Security'.")
            print("2. Check these sections for 'AssistantProxy' and enable it:")
            print("    - Full Disk Access (Recommended for file operations)")
            print("    - Automation (Allow control of Finder, System Events, etc.)")
            print("    - Calendars")
            print("    - Contacts")
            print("    - Reminders")
            print("    - Photos (If needed)")
            print("    - Accessibility (If needed)")
            print("\nFor Calendars, Contacts, Reminders: If AssistantProxy is not")
            print("listed yet, it will be added automatically after you allow")
            print("the first permission prompt triggered by an action.")
            print("For Full Disk Access/Automation: You may need to click '+'")
            print("to add '/Applications/AssistantProxy.app'.")
            print("-" * 60)
            # --- END Final User Instructions ---

            return True

        except subprocess.CalledProcessError as e:
            logger.error(f"❌ Failed to copy/remove app in /Applications (RC={e.returncode}): {e.stderr or e.stdout}")
            logger.error("   This script needs to be run with sudo privileges.")
            return False
        except Exception as e:
            logger.error(f"❌ Error copying proxy app to /Applications: {e}")
            return False
    # Temporary directory tmpdir is automatically cleaned up


# === AI Chat Logic (Amaryllis - SQLite RAG with Fuzzy Search) ===
class CortexThoughts:
    """Handles Chat Mode interactions with RAG, ToT, Action Analysis, Multi-LLM routing, and VLM preprocessing."""

    def __init__(self, provider: CortexEngine):
        self.provider = provider # CortexEngine instance with multiple models
        self.vectorstore_url: Optional[Chroma] = None
        self.vectorstore_history: Optional[Chroma] = None # In-memory store for current request
        self.current_session_id: Optional[str] = None
        self.setup_prompts()

        # --- NEW: Initialize StellaIcarusHookManager ---
        # --- MODIFIED: Initialize StellaIcarusHookManager ---
        self.stella_icarus_manager: Optional[StellaIcarusHookManager] = None
        if ENABLE_STELLA_ICARUS_HOOKS and StellaIcarusHookManager is not None:  # Check if class was imported
            try:
                self.stella_icarus_manager = StellaIcarusHookManager()
                if self.stella_icarus_manager.hook_load_errors:
                    logger.warning("CortexThoughts Init: StellaIcarusHookManager loaded with some errors.")
                elif not self.stella_icarus_manager.hooks:
                    logger.info("CortexThoughts Init: StellaIcarusHookManager loaded, but no hooks found/active.")
                else:
                    logger.success("CortexThoughts Init: StellaIcarusHookManager loaded successfully with hooks.")
            except Exception as e_sihm_init:
                logger.error(f"CortexThoughts Init: Failed to initialize StellaIcarusHookManager: {e_sihm_init}")
                self.stella_icarus_manager = None
        elif not StellaIcarusHookManager:
            logger.error("CortexThoughts Init: StellaIcarusHookManager class not available (import failed?). Hooks disabled.")
        else:  # ENABLE_STELLA_ICARUS_HOOKS is False
            logger.info("CortexThoughts Init: StellaIcarusHooks are disabled by configuration.")
        # --- END MODIFIED ---
        # --- END NEW ---

    @staticmethod
    def _construct_raw_chatml_prompt(
            system_content: Optional[str],
            history_turns: List[Dict[str, str]],  # e.g., [{"role": "user", "content": "..."}]
            current_turn_content: Optional[str] = None,  # Content for the current user/instruction turn
            current_turn_role: str = "user",  # Role for the current_turn_content
            prompt_for_assistant_response: bool = True  # Add "<|im_start|>assistant\n" at the end
    ) -> str:
        """
        Constructs a raw ChatML prompt string.
        History turns are processed in order.
        """
        prompt_parts = []

        if system_content and system_content.strip():
            prompt_parts.append(
                f"{CHATML_START_TOKEN}system{CHATML_NL}{system_content.strip()}{CHATML_END_TOKEN}{CHATML_NL}")

        for turn in history_turns:
            role = turn.get("role", "user").lower()
            content = str(turn.get("content", "")).strip()  # Ensure content is string
            if role not in ["user", "assistant", "system"]:  # System in history is rare but possible
                logger.warning(f"ChatML Constructor: Unknown role '{role}' in history. Skipping.")
                continue
            if content:  # Only add turns with actual content
                prompt_parts.append(f"{CHATML_START_TOKEN}{role}{CHATML_NL}{content}{CHATML_END_TOKEN}{CHATML_NL}")

        if current_turn_content and current_turn_content.strip():
            current_turn_role = current_turn_role.lower()
            if current_turn_role not in ["user", "system"]:  # Typically "user" or "system" for instructions
                logger.warning(
                    f"ChatML Constructor: Invalid role '{current_turn_role}' for current turn. Defaulting to 'user'.")
                current_turn_role = "user"
            prompt_parts.append(
                f"{CHATML_START_TOKEN}{current_turn_role}{CHATML_NL}{current_turn_content.strip()}{CHATML_END_TOKEN}{CHATML_NL}")

        if prompt_for_assistant_response:
            prompt_parts.append(f"{CHATML_START_TOKEN}assistant{CHATML_NL}")

        return "".join(prompt_parts)
    #Legacy Backup for generate Method class! change when needed since this is uses ELP1 and eats the time response and not "router" class, cautious!
    async def generate(self, db: Session, user_input: str, session_id: str, classification: str = "chat_simple") -> str:
        """
        Main entry point for generating a response.
        This method acts as a dispatcher to the appropriate generation logic
        based on the provided classification. For requests expecting a direct
        string response, it primarily uses the direct_generate (ELP1) path.
        For more complex, non-blocking tasks, the caller should use background_generate directly.
        """
        req_id = f"gen-dispatcher-{uuid.uuid4()}"
        log_prefix = f"🧠 {req_id}"
        logger.info(f"{log_prefix} Dispatching generate call. Session: {session_id}, Class: {classification}")

        # For a direct, blocking-style call that returns a string,
        # we will always use the ELP1 direct_generate path.
        # The 'classification' can be used for logging or minor adjustments if needed.
        # The more complex logic (like spawning ToT) is handled by background_generate.

        if not cortex_text_interaction:
            logger.error(f"{log_prefix} cortex_text_interaction (CortexThoughts) instance is not available.")
            return "Error: AI chat instance is not initialized."

        try:
            # We use direct_generate for all calls that expect an immediate string response.
            # It's designed to be fast and provide the ELP1 answer.
            # The dual-generate logic in /v1/chat/completions will handle spawning
            # the ELP0 background_generate task separately.
            response_text = await self.direct_generate(
                db=db,
                user_input=user_input,
                session_id=session_id,
                # No VLM description or image for these text-only generate calls
                vlm_description=None,
                image_b64=None
            )
            return response_text

        except Exception as e:
            logger.error(f"{log_prefix} Error in generate dispatcher: {e}", exc_info=True)
            return f"An internal error occurred while generating a response: {e}"

    def _count_tokens(self, text: str) -> int:
        """Counts tokens using tiktoken if available, else estimates by characters."""
        if TIKTOKEN_AVAILABLE_APP and cl100k_base_encoder_app and text:
            try:
                return len(cl100k_base_encoder_app.encode(text))
            except Exception as e:
                logger.warning(f"Tiktoken counting error in CortexThoughts: {e}. Falling back to char count.")
                return len(text) // 4  # Rough char to token estimate
        elif text:
            return len(text) // 4  # Rough char to token estimate
        return 0

    def _truncate_rag_context(self, context_str: str, max_tokens: int) -> str:
        """Truncates RAG context string to not exceed max_tokens."""
        if not context_str or max_tokens <= 0:
            return ""

        current_tokens = self._count_tokens(context_str)
        if current_tokens <= max_tokens:
            return context_str

        # Simple truncation by characters (more sophisticated truncation is possible)
        # Estimate characters per token (very rough, depends on tokenizer)
        avg_chars_per_token = 3.5  # Can be adjusted
        target_chars = int(max_tokens * avg_chars_per_token)

        if len(context_str) > target_chars:
            truncated_context = context_str[:target_chars]
            # Try to truncate at a natural boundary (e.g., end of a "Source Chunk")
            last_source_chunk_end = truncated_context.rfind("\n--- End Relevant Context ---")  # if you add this
            if last_source_chunk_end != -1:
                truncated_context = truncated_context[:last_source_chunk_end + len("\n--- End Relevant Context ---")]
            else:
                # Fallback to word boundary
                last_space = truncated_context.rfind(' ')
                if last_space != -1:
                    truncated_context = truncated_context[:last_space]

            logger.warning(
                f"Truncated RAG context from {current_tokens} tokens to approx. {self._count_tokens(truncated_context)} tokens (target: {max_tokens}).")
            return truncated_context + "\n[...RAG context truncated due to length...]"
        return context_str  # Should not be reached if current_tokens > max_tokens and char truncation applied

    def setup_prompts(self):
        """Initializes Langchain prompt templates."""
        logger.debug("Setting up CortexThoughts prompt templates...")
        self.text_prompt_template = ChatPromptTemplate.from_messages(
            [
                ("system", PROMPT_BACKGROUND_MULTISTAGE_GRINDING), # Expects various context keys
                ("human", "{input}")
            ]
        )
        self.visual_prompt_template = ChatPromptTemplate.from_messages(
            [
                SystemMessage(content=PROMPT_VISUAL_CHAT),
                MessagesPlaceholder(variable_name="history_rag_messages"),
                HumanMessage(content="Image Description:\n{image_description}\n\nEmotion/Context Analysis: {emotion_analysis}\n\nUser Query: {input}"),
            ]
        )
        # Prompt specifically for getting a description from the VLM
        self.vlm_description_prompt = ChatPromptTemplate.from_template(
            "Describe the key elements, objects, people, and activities in the provided image accurately and concisely. Focus on factual observation."
        )
        self.input_classification_prompt = ChatPromptTemplate.from_template(PROMPT_COMPLEXITY_CLASSIFICATION)
        self.tot_prompt = ChatPromptTemplate.from_template(PROMPT_TREE_OF_THOUGHTS)
        self.emotion_analysis_prompt = ChatPromptTemplate.from_template(PROMPT_EMOTION_ANALYSIS)
        self.image_latex_prompt = ChatPromptTemplate.from_template(PROMPT_IMAGE_TO_LATEX)
        logger.debug("CortexThoughts prompt templates setup complete.")

    def _convert_avif_to_png_b64(self, avif_b64: str) -> Optional[str]:
        """Decodes an AVIF base64 string and re-encodes it as a PNG base64 string."""
        log_prefix = f"AVIF_CONVERT|{self.current_session_id}"
        try:
            # Ensure the avif plugin is registered if not already
            from PIL import features
            if not features.check('avif'):
                logger.error(f"{log_prefix}: Pillow AVIF plugin not available or registered!")
                return None

            avif_bytes = base64.b64decode(avif_b64)
            image = Image.open(io.BytesIO(avif_bytes))

            png_buffer = io.BytesIO()
            image.save(png_buffer, format="PNG")
            png_b64 = base64.b64encode(png_buffer.getvalue()).decode('utf-8')
            return png_b64
        except Exception as e:
            logger.error(f"{log_prefix}: Failed to convert AVIF to PNG: {e}", exc_info=True)
            return None

    # Modify the generic _describe_image_async
    async def _should_i_imagine_async(self, db: Session, session_id: str, user_input: str,
                                      context_summary: str) -> bool:
        """
        Uses an LLM to decide if generating an image is a relevant action for the user's request.

        This function performs a focused, binary (Yes/No) analysis. It then logs its own
        decision as a new `Interaction` record in the database, making the cognitive step
        explicit and traceable. This allows subsequent processes to see this decision in the
        RAG context.

        Args:
            db: The active SQLAlchemy database session.
            session_id: The session ID for the current interaction.
            user_input: The user's latest input text being analyzed.
            context_summary: A brief summary of the recent conversation for context.

        Returns:
            bool: True if the LLM decides an image should be generated, False otherwise.
        """
        request_id_suffix = str(uuid.uuid4())[:8]
        log_prefix = f"🤔 ShouldImagine|ELP0|{session_id[:8]}-{request_id_suffix}"
        logger.info(f"{log_prefix} Analyzing input for imagination potential: '{user_input[:50]}...'")

        # 1. Select the appropriate model for this decision task.
        # A 'router' or 'general_fast' model is ideal as this is a simple classification.
        decision_model = self.provider.get_model("router")
        if not decision_model:
            logger.error(
                f"{log_prefix}: Router model not available for 'should_imagine' decision. Defaulting to False.")
            # Log this failure to the database for traceability
            await asyncio.to_thread(
                add_interaction, db, session_id=session_id, mode="chat", input_type="log_error",
                user_input="[ShouldImagine Model Unavailable]",
                llm_response="Router model for imagination decision was not configured. Skipped imagination."
            )
            await asyncio.to_thread(db.commit)
            return False

        # 2. Prepare the prompt and the Langchain chain.
        prompt_input = {
            "user_input": user_input,
            "context_summary": context_summary
        }
        # PROMPT_SHOULD_I_IMAGINE should be defined in CortexConfiguration.py
        chain = (
                ChatPromptTemplate.from_template(PROMPT_SHOULD_I_IMAGINE)
                | decision_model
                | StrOutputParser()
        )
        timing_data = {"session_id": session_id, "mode": "chat"}

        # 3. Execute the LLM call and parse the result.
        # Default to a "no" decision in case of any failure.
        decision_json_str = '{"should_imagine": false, "reasoning": "Analysis failed to produce a valid result."}'
        should_imagine = False

        try:
            # _call_llm_with_timing runs at ELP0 by default and handles interruptions.
            raw_llm_response = await asyncio.to_thread(
                self._call_llm_with_timing, chain, prompt_input, timing_data
            )

            # Use the robust JSON helper functions to extract and parse the response.
            json_candidate = self._extract_json_candidate_string(raw_llm_response, log_prefix)
            if json_candidate:
                parsed_json = self._programmatic_json_parse_and_fix(json_candidate, log_prefix=log_prefix)
                if parsed_json and isinstance(parsed_json, dict) and "should_imagine" in parsed_json:
                    should_imagine = bool(parsed_json.get("should_imagine", False))
                    decision_json_str = json.dumps(parsed_json, indent=2)  # Store the full, well-formatted JSON
                else:
                    # The JSON was invalid or missing the required key.
                    reason = f"Parsed JSON was invalid or missing 'should_imagine' key. Parsed: {str(parsed_json)[:200]}"
                    decision_json_str = json.dumps({'should_imagine': False, 'reasoning': reason})
            else:
                # No JSON structure could be found in the LLM's output.
                reason = f"Failed to extract any JSON structure from LLM raw response: {raw_llm_response[:200]}"
                decision_json_str = json.dumps({'should_imagine': False, 'reasoning': reason})

        except TaskInterruptedException as tie:
            logger.warning(f"🚦 {log_prefix} 'should_imagine' task was interrupted: {tie}")
            decision_json_str = json.dumps(
                {'should_imagine': False, 'reasoning': f'Task interrupted by higher priority request: {tie}'})
            should_imagine = False
            # We will still log this interruption event to the DB below.
        except Exception as e:
            logger.error(f"{log_prefix} Exception during 'should_imagine' analysis: {e}")
            logger.exception(f"{log_prefix} Traceback:")
            decision_json_str = json.dumps(
                {'should_imagine': False, 'reasoning': f'An exception occurred during analysis: {e}'})
            should_imagine = False

        # 4. Log the outcome of this cognitive step to the database.
        # This makes the decision explicit and available for future RAG.
        await asyncio.to_thread(
            add_interaction,
            db,  # Use the session passed into this function
            session_id=session_id,
            mode="chat",
            input_type="log_info_imagine_decision",
            user_input=f"Imagination decision analysis for: {user_input[:500]}",
            llm_response=decision_json_str,  # Log the full JSON result for traceability
            classification="internal_cognitive_step"
        )
        await asyncio.to_thread(db.commit)  # Commit this log entry so it's visible to the next steps in the process

        logger.info(f"{log_prefix} Decision: {should_imagine}. Logged to DB.")

        # 5. Return the simple boolean result.
        return should_imagine

    async def _refine_direct_image_prompt_async(
            self,
            db: Session,
            session_id: str,
            user_image_request: str,  # The prompt from the /v1/images/generations request
            history_rag_str: str,
            recent_direct_history_str: str,
            priority: int = ELP1  # Default to ELP1 for user-facing requests
    ) -> Optional[str]:
        """
        Uses an LLM to refine a user's direct image request into a more detailed image generation prompt,
        considering some conversational context. Runs with the specified priority.
        Strips <think> tags programmatically.
        """
        req_id = f"refineimgprompt-{uuid.uuid4()}"
        log_prefix = f"🖌️ {req_id}|ELP{priority}"  # Include priority in log
        logger.info(
            f"{log_prefix} Refining direct image request for session {session_id}: '{user_image_request[:100]}...'")

        # Use a general-purpose model for this creative task
        refiner_model = self.provider.get_model("general")  # Or "router"
        if not refiner_model:
            logger.error(f"{log_prefix} Model for image prompt refinement ('general') not available.")
            try:
                add_interaction(db, session_id=session_id, mode="image_gen", input_type="log_error",
                                user_input="[ImgPromptRefine Failed - Model Unavailable]",
                                llm_response="Image prompt refinement model not configured.")
            except Exception as db_err:
                logger.error(f"Failed log img prompt refine model error: {db_err}")
            return user_image_request  # Fallback to original request if model unavailable

        prompt_inputs = {
            "original_user_input": user_image_request,
            "history_rag": history_rag_str,
            "recent_direct_history": recent_direct_history_str,
        }

        chain = (
                ChatPromptTemplate.from_template(PROMPT_REFINE_USER_IMAGE_REQUEST)  # Use the new prompt
                | refiner_model
                | StrOutputParser()
        )
        timing_data = {"session_id": session_id, "mode": "image_gen", "execution_time_ms": 0}
        refined_prompt_raw = None

        try:
            refined_prompt_raw = await asyncio.to_thread(
                self._call_llm_with_timing, chain, prompt_inputs, timing_data, priority=priority
            )
            logger.trace(
                f"{log_prefix}: LLM Raw Output for Image Prompt:\n```\n{refined_prompt_raw}\n```")  # Log full raw output

            if not refined_prompt_raw:
                logger.warning(f"{log_prefix}: LLM returned empty image prompt string.")
                return user_image_request

            # Step 1: Remove <think> tags
            prompt_after_think_removal = re.sub(r'<think>.*?</think>', '', refined_prompt_raw,
                                                flags=re.DOTALL | re.IGNORECASE)
            logger.trace(f"{log_prefix}: After <think> removal:\n```\n{prompt_after_think_removal}\n```")

            # Step 2: Remove preambles
            cleaned_prompt_intermediate = prompt_after_think_removal
            preambles = [
                r"^(image generation prompt:|here is the prompt:|sure, here's an image prompt:|okay, based on the context, here's an image prompt:|refined image prompt:)\s*",
                r"^(Okay, I've generated an image prompt based on.*)\n*"
            ]
            for i, preamble_pattern in enumerate(preambles):
                before_preamble_strip = cleaned_prompt_intermediate
                cleaned_prompt_intermediate = re.sub(preamble_pattern, "", cleaned_prompt_intermediate,
                                                     flags=re.IGNORECASE | re.MULTILINE).strip()
                if before_preamble_strip != cleaned_prompt_intermediate:
                    logger.trace(
                        f"{log_prefix}: After preamble strip {i + 1} ('{preamble_pattern}'):\n```\n{cleaned_prompt_intermediate}\n```")

            # Step 3: Remove "Image Generation Prompt:" line
            before_header_strip = cleaned_prompt_intermediate
            cleaned_prompt_intermediate = re.sub(r"^\s*Image Generation Prompt:\s*\n?", "", cleaned_prompt_intermediate,
                                                 flags=re.MULTILINE | re.IGNORECASE).strip()
            if before_header_strip != cleaned_prompt_intermediate:
                logger.trace(
                    f"{log_prefix}: After 'Image Generation Prompt:' header strip:\n```\n{cleaned_prompt_intermediate}\n```")

            # Step 4: Trim whitespace (already done by .strip() in preamble loop, but good for final)
            cleaned_prompt_intermediate = cleaned_prompt_intermediate.strip()
            # logger.trace(f"{log_prefix}: After final strip:\n```\n{cleaned_prompt_intermediate}\n```")

            # Step 5: Remove surrounding quotes
            before_quote_strip = cleaned_prompt_intermediate
            cleaned_prompt_intermediate = re.sub(r'^["\'](.*?)["\']$', r'\1', cleaned_prompt_intermediate)
            if before_quote_strip != cleaned_prompt_intermediate:
                logger.trace(f"{log_prefix}: After surrounding quote strip:\n```\n{cleaned_prompt_intermediate}\n```")

            # Step 6: Remove "Output only this:"
            before_output_only_strip = cleaned_prompt_intermediate
            cleaned_prompt = re.sub(r"\(Output only this\):?", "", cleaned_prompt_intermediate,
                                    flags=re.IGNORECASE).strip()
            if before_output_only_strip != cleaned_prompt:
                logger.trace(f"{log_prefix}: After '(Output only this):' strip:\n```\n{cleaned_prompt}\n```")

            if not cleaned_prompt:
                logger.warning(f"{log_prefix} LLM generated an empty image prompt after all cleaning steps.")
                # Log the raw and intermediate steps if this happens
                logger.debug(
                    f"{log_prefix} DEBUG: Raw='{refined_prompt_raw}', AfterThink='{prompt_after_think_removal}'")
                return user_image_request

            logger.info(f"{log_prefix} Final Refined Image Prompt: '{cleaned_prompt}'")

        except TaskInterruptedException as tie:
            logger.warning(f"🚦 {log_prefix} Image prompt refinement INTERRUPTED: {tie}")
            raise tie  # Propagate for the endpoint to handle
        except Exception as e:
            logger.error(f"❌ {log_prefix} Error refining direct image prompt: {e}")
            logger.exception(f"{log_prefix} ImgPromptRefine Traceback:")
            try:
                add_interaction(db, session_id=session_id, mode="image_gen", input_type="log_error",
                                user_input="[ImgPromptRefine Failed]",
                                llm_response=f"Error: {e}. Raw: {str(refined_prompt_raw)[:200]}")
            except Exception:
                pass
            return user_image_request  # Fallback to original on error

    async def _generate_image_generation_prompt_async(
        self,
        db: Session,
        session_id: str,
        original_user_input: str,
        current_thought_context: str, # Specific idea/ToT output to visualize
        history_rag_str: str,
        file_index_context_str: str,
        recent_direct_history_str: str,
        url_context_str: str,
        log_context_str: str
    ) -> Optional[str]:
        """
        Uses an LLM (e.g., 'general' or 'router') to generate a concise, creative
        image generation prompt based on comprehensive context. Strips <think> tags.
        Called with ELP0 priority.
        """
        req_id = f"imgpromptgen-{uuid.uuid4()}"
        log_prefix = f"🎨 {req_id}|ELP0"
        logger.info(f"{log_prefix} Generating image prompt for session {session_id} with rich context.")

        prompt_gen_model = self.provider.get_model("general") # Or "router"
        if not prompt_gen_model:
            logger.error(f"{log_prefix} Model for image prompt generation ('general') not available.")
            try:
                add_interaction(db, session_id=session_id, mode="chat", input_type="log_error",
                                user_input="[ImgPromptGen Failed - Model Unavailable]",
                                llm_response="Image prompt generation model not configured.")
            except Exception as db_err: logger.error(f"Failed log img prompt gen model error: {db_err}")
            return None

        # Prepare the input dictionary for the prompt template
        prompt_inputs = {
            "original_user_input": original_user_input,
            "current_thought_context": current_thought_context,
            "history_rag": history_rag_str,
            "file_index_context": file_index_context_str,
            "recent_direct_history": recent_direct_history_str,
            "url_context": url_context_str,
            "log_context": log_context_str
        }

        chain = (
            ChatPromptTemplate.from_template(PROMPT_CREATE_IMAGE_PROMPT) # Uses the updated prompt from CortexConfiguration
            | prompt_gen_model
            | StrOutputParser()
        )
        timing_data = {"session_id": session_id, "mode": "chat", "execution_time_ms": 0}
        generated_prompt_raw = None

        try:
            # Call LLM with ELP0 priority
            generated_prompt_raw = await asyncio.to_thread(
                self._call_llm_with_timing, chain, prompt_inputs, timing_data, priority=ELP0
            )

            if not generated_prompt_raw:
                logger.warning(f"{log_prefix} LLM returned empty image generation prompt string.")
                return None

            # --- Programmatic <think> tag removal and cleaning ---
            # 1. Remove <think> tags (case-insensitive, multiline)
            cleaned_prompt = re.sub(r'<think>.*?</think>', '', generated_prompt_raw, flags=re.DOTALL | re.IGNORECASE)
            # 2. Remove common LLM preamble/postamble
            preambles = [
                r"^(image generation prompt:|here is the prompt:|sure, here's an image prompt:|okay, based on the context, here's an image prompt:)\s*",
                r"^(Okay, I've generated an image prompt based on.*)\n*"
            ]
            for preamble_pattern in preambles:
                cleaned_prompt = re.sub(preamble_pattern, "", cleaned_prompt, flags=re.IGNORECASE | re.MULTILINE).strip()
            # 3. Remove any "Image Generation Prompt:" line if it somehow survived or was re-added by the model
            cleaned_prompt = re.sub(r"^\s*Image Generation Prompt:\s*\n?", "", cleaned_prompt, flags=re.MULTILINE | re.IGNORECASE).strip()
            # 4. Trim whitespace
            cleaned_prompt = cleaned_prompt.strip()
            # 5. Remove surrounding quotes if the model added them
            cleaned_prompt = re.sub(r'^["\'](.*?)["\']$', r'\1', cleaned_prompt)
            # 6. Remove any remaining "Output only this:" type instructions if they leak
            cleaned_prompt = re.sub(r"\(Output only this\):?", "", cleaned_prompt, flags=re.IGNORECASE).strip()


            if not cleaned_prompt:
                logger.warning(f"{log_prefix} LLM generated an empty image prompt after cleaning.")
                try: add_interaction(db, session_id=session_id, mode="chat", input_type="log_warning", user_input="[ImgPromptGen Empty]", llm_response=f"Raw: {generated_prompt_raw[:200]}")
                except Exception: pass
                return None

            logger.info(f"{log_prefix} Generated image prompt: '{cleaned_prompt}' (Raw len: {len(generated_prompt_raw)}, Cleaned len: {len(cleaned_prompt)})")
            try: add_interaction(db, session_id=session_id, mode="chat", input_type="log_debug", user_input="[ImgPromptGen Success]", llm_response=f"Prompt: '{cleaned_prompt}'. Raw: {generated_prompt_raw[:200]}")
            except Exception: pass
            return cleaned_prompt

        except TaskInterruptedException as tie:
            logger.warning(f"🚦 {log_prefix} Image prompt generation INTERRUPTED: {tie}")
            raise tie
        except Exception as e:
            logger.error(f"❌ {log_prefix} Error generating image prompt: {e}")
            logger.exception(f"{log_prefix} ImgPromptGen Traceback:")
            try: add_interaction(db, session_id=session_id, mode="chat", input_type="log_error", user_input="[ImgPromptGen Failed]", llm_response=f"Error: {e}. Raw: {str(generated_prompt_raw)[:200]}")
            except Exception: pass
            return None

    # --- NEW HELPER: Describe Image with VLM (ELP0) ---
        # AdelaideAlbertCortex -> CortexThoughts class

    async def _describe_generated_image_async(self, db: Session, session_id: str, image_avif_b64: str) -> Optional[str]:
        """
        Sends a base64 AVIF image to the VLM to get a textual description.
        This now acts as a wrapper for the generic _describe_image_async.
        """
        # We pass the AVIF data directly to the generic helper, telling it to convert.
        description, error = await self._describe_image_async(
            db, session_id, image_avif_b64, "describe_generated_image", ELP0, is_avif=True
        )
        if error:
            logger.error(f"Failed to get VLM description for generated image: {error}")
            return None # Or return an error string if you prefer
        return description

    async def _should_i_create_a_hook_async(self, db: Session, session_id: str, query: str, rag_context: str,
                                            answer: str) -> Tuple[bool, str]:
        """Uses the router model to decide if an interaction should be automated into a hook."""
        log_prefix = f"🤔 ShouldCreateHook|{session_id[:8]}"
        logger.info(f"{log_prefix} Analyzing interaction for hook creation potential.")

        decision_model = self.provider.get_model("router")
        if not decision_model:
            logger.error(f"{log_prefix} Router model unavailable. Cannot decide on hook creation.")
            return False, "Router model unavailable."

        prompt_input = {
            "user_query": query,
            "rag_context": self._truncate_rag_context(rag_context, 500),  # Truncate for decision prompt
            "final_answer": answer
        }
        chain = (ChatPromptTemplate.from_template(PROMPT_SHOULD_I_CREATE_HOOK) | decision_model | StrOutputParser())

        try:
            raw_response = await asyncio.to_thread(chain.invoke, prompt_input)
            json_candidate = self._extract_json_candidate_string(raw_response, log_prefix)
            if json_candidate:
                parsed = self._programmatic_json_parse_and_fix(json_candidate, log_prefix=log_prefix)
                if parsed and isinstance(parsed, dict) and "should_create_hook" in parsed:
                    decision = bool(parsed.get("should_create_hook"))
                    reason = parsed.get("reasoning", "N/A")
                    logger.info(f"{log_prefix} Decision: {decision}. Reason: {reason}")
                    return decision, reason
        except Exception as e:
            logger.error(f"{log_prefix} Error during hook creation decision: {e}", exc_info=True)

        return False, "Analysis failed."

    async def _generate_stella_icarus_hook_async(self, db: Session, session_id: str, query: str, rag_context: str,
                                                 answer: str) -> None:
        """Generates and saves a new Python hook file based on a successful interaction."""
        log_prefix = f"✍️ GenerateHook|{session_id[:8]}"
        logger.info(f"{log_prefix} Starting generation of new StellaIcarus hook.")

        code_model = self.provider.get_model("code")
        if not code_model:
            logger.error(f"{log_prefix} Code model unavailable. Cannot generate hook.")
            return

        try:
            # 1. Read the template file
            template_path = os.path.join(STELLA_ICARUS_HOOK_DIR, "basic_math_hook.py")
            with open(template_path, 'r', encoding='utf-8') as f:
                template_content = f.read()
        except Exception as e:
            logger.error(f"{log_prefix} Could not read hook template file '{template_path}': {e}")
            return

        # 2. Prepare prompt and generate code
        prompt_input = {
            "template_content": template_content,
            "user_query": query,
            "rag_context": self._truncate_rag_context(rag_context, 500),
            "final_answer": answer,
        }
        chain = (ChatPromptTemplate.from_template(PROMPT_GENERATE_STELLA_ICARUS_HOOK) | code_model | StrOutputParser())

        try:
            generated_code = await asyncio.to_thread(chain.invoke, prompt_input)

            if not generated_code or "PATTERN" not in generated_code or "def handler" not in generated_code:
                logger.warning(
                    f"{log_prefix} Generated code appears invalid or empty. Aborting save. Code: {generated_code[:300]}")
                return

            # 3. Save the new hook file
            sanitized_query = re.sub(r'\W+', '_', query.lower())[:50]
            timestamp = datetime.datetime.now().strftime("%Y%m%d%H%M%S")
            new_hook_filename = f"{timestamp}_{sanitized_query}_generatedStellaIcarus.py"
            new_hook_filepath = os.path.join(STELLA_ICARUS_HOOK_DIR, new_hook_filename)

            with open(new_hook_filepath, 'w', encoding='utf-8') as f:
                f.write("# Auto-generated by Adelaide AI\n")
                f.write(f"# Original Query: {query}\n")
                f.write(f"# Timestamp: {datetime.datetime.now().isoformat()}\n\n")
                f.write(generated_code)

            logger.success(f"{log_prefix} ✅ Successfully generated and saved new hook: {new_hook_filename}")

            # Log this event to the DB
            await asyncio.to_thread(
                add_interaction, db, session_id=session_id, mode="self_improvement", input_type="hook_generated",
                user_input=f"Generated hook for query: {query}", llm_response=f"Saved to {new_hook_filename}"
            )
            await asyncio.to_thread(db.commit)

        except Exception as e:
            logger.error(f"{log_prefix} Error during hook generation or saving: {e}", exc_info=True)

    def _get_rag_retriever_thread_wrapper(self, db_session: Session, user_input_str: str, priority_val: int) -> Dict[str, Any]:
        """
        Synchronous wrapper for _get_rag_retriever to be run in asyncio.to_thread.
        Catches exceptions and returns a structured dictionary.
        """
        log_prefix = f"RAGThreadWrap|ELP{priority_val}|{self.current_session_id or 'NoSession'}"
        try:
            logger.debug(f"{log_prefix}: Executing _get_rag_retriever in thread...")
            # Call the actual synchronous _get_rag_retriever method
            result_tuple = self._get_rag_retriever(db_session, user_input_str, priority_val) # Passes it correctly
            logger.debug(
                f"{log_prefix}: _get_rag_retriever completed. Result tuple length: {len(result_tuple) if isinstance(result_tuple, tuple) else 'N/A'}")
            return {"status": "success", "data": result_tuple}
        except TaskInterruptedException as tie_wrapper:
            logger.warning(
                f"🚦 {log_prefix}: TaskInterruptedException caught: {tie_wrapper}. Returning interruption status.")
            return {"status": "interrupted", "error_message": str(tie_wrapper)}
        except Exception as e_wrapper:
            logger.error(f"❌ {log_prefix}: Exception caught: {e_wrapper}")
            logger.exception(f"{log_prefix} _get_rag_retriever_thread_wrapper Exception Details:")
            return {"status": "error", "error_message": str(e_wrapper)}

    class _CustomVectorSearchRetriever(VectorStoreRetriever):
        vectorstore: VectorStore
        search_type: str = "similarity"
        search_kwargs: Dict[str, Any]
        vector_to_search: List[float]

        def _get_relevant_documents(self, query: str, *, run_manager: Any) -> List[Document]:
            if not self.vectorstore: raise ValueError("Vectorstore not set on _CustomVectorSearchRetriever")
            k_val = self.search_kwargs.get("k", 4)
            return self.vectorstore.similarity_search_by_vector(embedding=self.vector_to_search, k=k_val)

        async def _aget_relevant_documents(self, query: str, *, run_manager: Any) -> List[Document]:
            if not self.vectorstore: raise ValueError("Vectorstore not set on _CustomVectorSearchRetriever")
            k_val = self.search_kwargs.get("k", 4)
            # Assuming similarity_search_by_vector is synchronous for Chroma
            return await asyncio.to_thread(
                self.vectorstore.similarity_search_by_vector,
                embedding=self.vector_to_search,
                k=k_val
            )



    def _build_on_the_fly_retriever(self, interactions: List[Interaction], query: str, query_vector: List[float], priority: int) -> List[Document]:
        """Helper to perform vector and fuzzy search on a list of interactions."""
        log_prefix = f"OnTheFlyRAG|ELP{priority}|{self.current_session_id or 'NoSession'}"
        retrieved_docs: List[Document] = []

        # Vector search part
        texts_to_embed = []
        metadata_map = []
        interaction_map = {} # Map text content back to interaction object
        for interaction in interactions:
            content = f"User: {interaction.user_input or ''}\nAI: {interaction.llm_response or ''}"
            if content.strip():
                texts_to_embed.append(content)
                metadata_map.append({"source": "on_the_fly_session", "interaction_id": interaction.id})
                interaction_map[content] = interaction

        if texts_to_embed:
            # Manually embed with priority, then create the Chroma store
            embeddings = self.provider.embeddings.embed_documents(texts_to_embed, priority=priority)
            if embeddings and len(embeddings) == len(texts_to_embed):
                # Create Document objects, which is the expected format for from_documents
                documents_to_add = [
                    Document(page_content=text, metadata=meta) for text, meta in zip(texts_to_embed, metadata_map)
                ]
                # Use the correct from_documents method
                temp_vs = Chroma.from_documents(
                    documents=documents_to_add,
                    embedding=self.provider.embeddings  # Pass the embedding function
                )
                # Now we can search it
                vector_results = temp_vs.similarity_search_by_vector(query_vector, k=RAG_HISTORY_COUNT // 2)
                retrieved_docs.extend(vector_results)
                logger.info(f"{log_prefix} On-the-fly vector search found {len(vector_results)} docs.")
            else:
                logger.error(f"{log_prefix} On-the-fly embedding failed or returned mismatched vectors.")

        # Fuzzy search part
        if FUZZY_AVAILABLE and len(retrieved_docs) < RAG_HISTORY_COUNT // 2:
            processed_ids = {doc.metadata.get("interaction_id") for doc in retrieved_docs if doc.metadata}
            fuzzy_matches: List[Tuple[Interaction, int]] = []
            
            for interaction in interactions:
                if interaction.id in processed_ids:
                    continue
                
                text_to_match = f"{interaction.user_input or ''} {interaction.llm_response or ''}"
                if text_to_match.strip():
                    score = fuzz.partial_ratio(query.lower(), text_to_match.lower())
                    if score >= FUZZY_SEARCH_THRESHOLD_APP:
                        fuzzy_matches.append((interaction, score))
            
            if fuzzy_matches:
                fuzzy_matches.sort(key=lambda x: x[1], reverse=True)
                needed_count = max(0, (RAG_HISTORY_COUNT // 2) - len(retrieved_docs))
                
                for interaction, score in fuzzy_matches[:needed_count]:
                    content = f"User: {interaction.user_input or ''}\nAI: {interaction.llm_response or ''}"
                    doc = Document(
                        page_content=content,
                        metadata={
                            "source": "on_the_fly_fuzzy", 
                            "interaction_id": interaction.id, 
                            "score": score
                        }
                    )
                    retrieved_docs.append(doc)
                logger.info(f"{log_prefix} On-the-fly fuzzy search added {len(fuzzy_matches[:needed_count])} docs.")
        
        return retrieved_docs

    async def _get_direct_rag_context_elp1(self, db: Session, user_input: str, session_id: str) -> str:
        """
        A lightweight, fast RAG retriever specifically for the ELP1 direct_generate path.
        CORRECTED: Now explicitly embeds the query with ELP1 priority before searching
        to ensure the entire RAG process is high-priority.
        """
        log_prefix = f"⚡️ DirectRAG-Prioritized|ELP1|{session_id}"
        logger.info(f"{log_prefix} Performing lightweight, ELP1-prioritized RAG for direct response.")

        interaction_vs = get_global_interaction_vectorstore()
        if not interaction_vs:
            logger.warning(f"{log_prefix} Global interaction vector store not available for direct RAG.")
            return "No historical context is available."

        try:
            # --- STEP 1: Explicitly embed the query with ELP1 priority ---
            logger.debug(f"{log_prefix} Embedding query with ELP1 priority...")

            # We must run the synchronous `embed_query` in a thread to be non-blocking.
            # Your `embed_query` method in the provider should accept a `priority` kwarg.
            query_vector = await asyncio.to_thread(
                self.provider.embeddings.embed_query,
                user_input,
                priority=ELP1  # Pass the priority directly
            )

            if not query_vector:
                logger.error(f"{log_prefix} Query embedding with ELP1 failed. Aborting RAG.")
                return "[An error occurred while preparing historical context.]"

            # --- STEP 2: Search the vector store using the pre-computed vector ---
            logger.debug(f"{log_prefix} Searching vector store using the ELP1-generated vector...")
            search_kwargs = {
                "k": 3,
                "filter": {
                    "input_type": {"$in": ["text", "llm_response"]}
                }
            }

            # Use `similarity_search_by_vector`, which does NOT call the embedder again.
            # This is also a synchronous call and needs to be run in a thread.
            search_results_docs = await asyncio.to_thread(
                interaction_vs.similarity_search_by_vector,
                embedding=query_vector,
                **search_kwargs
            )

            if not search_results_docs:
                logger.info(f"{log_prefix} No relevant conversational documents found in persistent store.")
                return "No specific historical context was found for this query."

            logger.info(f"{log_prefix} Found {len(search_results_docs)} purified documents for direct context.")
            return self._format_docs(search_results_docs, "History RAG")

        except TaskInterruptedException as tie:
            logger.warning(f"🚦 {log_prefix} Prioritized direct RAG was interrupted: {tie}")
            return "[Context retrieval interrupted by a higher priority task]"
        except Exception as e:
            logger.error(f"❌ {log_prefix} Error during prioritized direct RAG retrieval: {e}", exc_info=True)
            return "[An error occurred while retrieving historical context]"

    #for ELP1 rag retriever since it's uses small amount of resources for calculation vector and retrieve it's better to put it on ELP1 by default
    def _get_rag_retriever(self, db: Session, user_input_for_rag_query: str, priority: int = ELP1) -> Tuple[
    Optional[VectorStoreRetriever], Optional[VectorStoreRetriever], Optional[VectorStoreRetriever], str
    ]:
        """
        Hybrid RAG Retriever with verbose logging and fuzzy search fallback for history.
        """
        log_prefix = f"RAGRetriever|ELP{priority}|{self.current_session_id or 'NoSession'}"
        logger.info(f"Hybrid RAG Retriever: Combining vector search and fuzzy search for query: '{user_input_for_rag_query[:30]}...'")

        # Initialize all return values
        url_retriever: Optional[VectorStoreRetriever] = None
        session_history_retriever: Optional[VectorStoreRetriever] = None
        reflection_chunks_retriever: Optional[VectorStoreRetriever] = None
        session_history_ids_str: str = ""
        
        rag_query_vector: Optional[List[float]] = None
        
        try:
            # --- Step 0: Pre-embed the RAG query ---
            if user_input_for_rag_query and self.provider and self.provider.embeddings:
                logger.debug(f"{log_prefix} Pre-embedding main RAG query with priority ELP{priority}...")
                rag_query_vector = self.provider.embeddings.embed_query(user_input_for_rag_query, priority=priority)
                if not rag_query_vector:
                    logger.error(f"{log_prefix} Main RAG query embedding resulted in None.")

            # --- Step 1: URL Retriever (In-Memory for current session) ---
            if hasattr(self, 'vectorstore_url') and self.vectorstore_url and rag_query_vector:
                url_retriever = self._CustomVectorSearchRetriever(
                    vectorstore=self.vectorstore_url,
                    search_kwargs={"k": RAG_URL_COUNT},
                    vector_to_search=rag_query_vector
                )
            
            # --- Step 2: Hybrid Interaction History Retriever (Vector + Fuzzy) ---
            all_history_docs: List[Document] = []
            
            # 2a. On-the-fly Vector Search (Recent, Unindexed Interactions)
            recent_unindexed_interactions = get_recent_interactions(
                db, RAG_HISTORY_COUNT * 4, self.current_session_id, "chat", False
            )
            recent_unindexed_interactions = [inter for inter in recent_unindexed_interactions if not inter.is_indexed_for_rag]
            recent_unindexed_interactions.reverse()
            logger.info(f"[RAG VERBOSITY] Found {len(recent_unindexed_interactions)} recent unindexed interactions to search in-memory.")
            
            if recent_unindexed_interactions and rag_query_vector:
                recent_texts_to_embed = [f"User: {i.user_input or ''}\nAI: {i.llm_response or ''}" for i in recent_unindexed_interactions]
                if recent_texts_to_embed:
                    on_the_fly_embeddings = self.provider.embeddings.embed_documents(recent_texts_to_embed, priority=priority)
                    temp_vs = Chroma(embedding_function=self.provider.embeddings)
                    if on_the_fly_embeddings:
                        temp_vs._collection.add(
                            embeddings=on_the_fly_embeddings,
                            documents=recent_texts_to_embed,
                            ids=[f"onthefly_{interaction.id}" for i, interaction in enumerate(recent_unindexed_interactions)]
                        )
                    on_the_fly_docs = temp_vs.similarity_search(user_input_for_rag_query, k=RAG_HISTORY_COUNT // 2)
                    logger.info(f"[RAG VERBOSITY] In-memory vector search found {len(on_the_fly_docs)} documents.")
                    all_history_docs.extend(on_the_fly_docs)

            # 2b. Persistent Vector Search (Indexed Interactions)
            interaction_vs = get_global_interaction_vectorstore()
            if interaction_vs and rag_query_vector:
                persistent_retriever = self._CustomVectorSearchRetriever(
                    vectorstore=interaction_vs,
                    search_kwargs={"k": RAG_HISTORY_COUNT},
                    vector_to_search=rag_query_vector
                )
                persistent_docs = persistent_retriever.invoke(user_input_for_rag_query)
                logger.info(f"[RAG VERBOSITY] Persistent vector store search found {len(persistent_docs or [])} documents.")
                all_history_docs.extend(persistent_docs or [])

            # --- NEW FUZZY LOGIC ---
            # Augment with fuzzy search if vector results are sparse
            if FUZZY_AVAILABLE and len(all_history_docs) < RAG_HISTORY_COUNT:
                logger.info(f"[RAG VERBOSITY] Augmenting with Fuzzy Search (Threshold: {FUZZY_SEARCH_THRESHOLD})...")
                
                # Fetch a pool of recent interactions to perform fuzzy search on
                fuzzy_candidate_pool = get_recent_interactions(db, limit=RAG_HISTORY_COUNT * 5, session_id=self.current_session_id, mode="chat", include_logs=False)
                vector_found_interaction_ids = {doc.metadata.get("interaction_id") for doc in all_history_docs if doc.metadata and "interaction_id" in doc.metadata}
                
                fuzzy_matches: List[Tuple[Interaction, int]] = []
                for interaction in fuzzy_candidate_pool:
                    if interaction.id in vector_found_interaction_ids:
                        continue
                    
                    text_to_match = f"{interaction.user_input or ''} {interaction.llm_response or ''}"
                    if text_to_match.strip():
                        score = fuzz.partial_ratio(user_input_for_rag_query.lower(), text_to_match.lower())
                        if score >= FUZZY_SEARCH_THRESHOLD:
                            fuzzy_matches.append((interaction, score))
                
                if fuzzy_matches:
                    fuzzy_matches.sort(key=lambda x: x[1], reverse=True)
                    needed_count = RAG_HISTORY_COUNT - len(all_history_docs)
                    top_fuzzy_matches = fuzzy_matches[:needed_count]
                    logger.info(f"[RAG VERBOSITY] Fuzzy search found {len(top_fuzzy_matches)} new documents above threshold.")
                    for interaction, score in top_fuzzy_matches:
                        content = f"User: {interaction.user_input or ''}\nAI: {interaction.llm_response or ''}"
                        doc = Document(page_content=content, metadata={"source": "history_fuzzy", "interaction_id": interaction.id, "score": score})
                        all_history_docs.append(doc)
            # --- END NEW FUZZY LOGIC ---

            logger.info(f"[RAG VERBOSITY] Total combined documents for history_rag: {len(all_history_docs)}")
            
            # 2c. Combine all history results into a single retriever
            if all_history_docs:
                # First, de-duplicate the documents based on their content
                unique_docs_dict = {doc.page_content: doc for doc in all_history_docs}
                unique_docs = list(unique_docs_dict.values())

                combined_texts = []
                combined_metadatas = []

                # <<< FIX: Iterate and validate/fix metadata for each document >>>
                for doc in unique_docs:
                    # Ensure page_content is a string
                    page_content_str = str(doc.page_content or "")
                    combined_texts.append(page_content_str)

                    # Ensure metadata is a valid, non-empty dictionary
                    metadata = doc.metadata or {}  # Start with an empty dict if metadata is None
                    if not isinstance(metadata, dict):
                        metadata = {"source": "unknown", "original_metadata_type": str(type(metadata))}

                    # If the dictionary is empty, add a placeholder key
                    if not metadata:
                        metadata["source"] = "history_combined_placeholder"

                    combined_metadatas.append(metadata)

                ids_for_combined_vs = [f"combined_{i}" for i in range(len(unique_docs))]

                # Ensure we don't proceed with empty lists if something went wrong
                if not combined_texts or not combined_metadatas:
                    logger.warning(
                        f"{log_prefix} No valid documents left after cleaning for combined history. Skipping.")
                else:
                    temp_combined_vs = Chroma(embedding_function=self.provider.embeddings)

                    # We need to re-embed the combined list to create the final temporary store
                    combined_embeddings = self.provider.embeddings.embed_documents(combined_texts, priority=priority)
                    if combined_embeddings:
                        # This call is now safe because `combined_metadatas` is guaranteed to contain non-empty dicts
                        temp_combined_vs._collection.add(
                            embeddings=combined_embeddings,
                            documents=combined_texts,
                            metadatas=combined_metadatas,
                            ids=ids_for_combined_vs
                        )
                    session_history_retriever = temp_combined_vs.as_retriever(search_kwargs={"k": RAG_HISTORY_COUNT})

            # --- Step 3: Persistent Reflection Retriever ---
            reflection_vs = get_global_reflection_vectorstore()
            if reflection_vs and rag_query_vector:
                reflection_chunks_retriever = self._CustomVectorSearchRetriever(
                    vectorstore=reflection_vs,
                    search_kwargs={"k": RAG_HISTORY_COUNT},
                    vector_to_search=rag_query_vector
                )
                reflection_docs = reflection_chunks_retriever.invoke(user_input_for_rag_query)
                logger.info(f"[RAG VERBOSITY] Reflection vector store search found {len(reflection_docs or [])} documents.")

            return (url_retriever, session_history_retriever, reflection_chunks_retriever, "")

        except Exception as e:
            logger.error(f"❌ UNHANDLED EXCEPTION in Hybrid RAG retriever: {e}")
            logger.exception("Hybrid RAG Retriever Traceback:")
            return None, None, None, ""
        


    async def _generate_file_search_query_async(self, db: Session, user_input_for_analysis: str, recent_direct_history_str: str, history_rag_str: str, session_id: str) -> str:
        """
        Uses the default LLM to generate a concise search query for the file index.
        MODIFIED: Now accepts and uses history_rag_str.
        """

        prompt_input = {
            "input": user_input_for_analysis,
            "recent_direct_history": recent_direct_history_str,
            "history_rag": history_rag_str
        }
        

        query_gen_id = f"fqgen-{uuid.uuid4()}"

        logger.debug(f"{query_gen_id}: Debugging file search query ctx input {user_input_for_analysis} dirHist {recent_direct_history_str} histRAGVec{history_rag_str}")

        logger.info(f"{query_gen_id}: Generating dedicated file search query...")

        file_search_query_gen_model = self.provider.get_model("general_fast")
        if not file_search_query_gen_model:
            logger.error(f"{query_gen_id}: Router model not available for file query generation. Falling back to user input.")
            return user_input_for_analysis

        # FIX: Added history_rag to the prompt input dictionary
        prompt_input = {
            "input": user_input_for_analysis,
            "recent_direct_history": recent_direct_history_str,
            "history_rag": history_rag_str
        }

        chain = (
            ChatPromptTemplate.from_template(PROMPT_GENERATE_FILE_SEARCH_QUERY)
            # --- MODIFICATION START ---
            | file_search_query_gen_model.bind(max_tokens=FILE_SEARCH_QUERY_GEN_MAX_OUTPUT_TOKENS) # Use the selected model
            # --- MODIFICATION END ---
            | StrOutputParser()
        )

        logger.debug(f"{query_gen_id}: Prompt Query _generate_file_search_query_async LLM to be processed {chain}")
        logger.debug(f"{query_gen_id}: Prompt Query _generate_file_search_query_async LLM to be processed prompt_input {prompt_input}")


        query_gen_timing_data = {"session_id": session_id, "mode": "chat", "execution_time_ms": 0}
        try:
            generated_query_raw = await asyncio.to_thread(
                self._call_llm_with_timing, chain, prompt_input, query_gen_timing_data
            )
            logger.debug(f"{query_gen_id}: generatedRaw Query _generate_file_search_query_async {generated_query_raw}")

            # ... (rest of the cleanup logic is the same) ...
            cleaned_query = re.sub(r'<think>.*?</think>', '', generated_query_raw, flags=re.DOTALL | re.IGNORECASE)
            cleaned_query = cleaned_query.strip()
            cleaned_query = re.sub(r'^["\']|["\']$', '', cleaned_query)

            if not cleaned_query:
                 logger.warning(f"{query_gen_id}: LLM generated an empty search query. Falling back to user input.")
                 return user_input_for_analysis

            logger.info(f"{query_gen_id}: Generated file search query: '{cleaned_query}'")
            return cleaned_query

        except Exception as e:
            logger.error(f"❌ {query_gen_id}: Error generating file search query: {e}")
            logger.exception(f"{query_gen_id}: Query Generation Traceback")
            return user_input_for_analysis
    # --- END NEW HELPER ---

    def _format_file_index_results(self, results: List[FileIndex]) -> str:
        """Formats FileIndex search results for the LLM prompt."""
        if not results:
            return "No relevant files found in the index."
        if not isinstance(results, list):
            logger.error(f"_format_file_index_results received non-list: {type(results)}")
            return "Invalid file index results provided."
        if not results: # Check again
            return "No relevant files found in the index."

        context_str = ""
        max_snippet_len = 300 # Max characters per snippet
        max_total_len = 2000 # Max total context length
        current_len = 0

        for i, record in enumerate(results):
            snippet = ""
            if record.index_status == 'indexed_text' and record.indexed_content:
                snippet = record.indexed_content[:max_snippet_len]
                if len(record.indexed_content) > max_snippet_len:
                    snippet += "..."
            elif record.processing_error:
                snippet = f"[Error accessing file: {record.processing_error}]"
            elif record.index_status == 'error_permission':
                 snippet = "[Error: Permission Denied]"
            elif record.index_status == 'skipped_size':
                 snippet = "[Content not indexed: File too large]"
            else:
                 snippet = "[Metadata indexed, no text content extracted]"

            entry = (f"--- File Result {i+1} ---\n"
                     f"Path: {record.file_path}\n"
                     f"Modified: {record.last_modified_os.strftime('%Y-%m-%d %H:%M') if record.last_modified_os else 'Unknown'}\n"
                     f"Status: {record.index_status}\n"
                     f"Content Snippet: {snippet}\n"
                     f"---\n")

            if current_len + len(entry) > max_total_len:
                context_str += "[File index context truncated due to length limit]...\n"
                break

            context_str += entry
            current_len += len(entry)

        return context_str if context_str else "No relevant files found in the index."

    def _cleanup_llm_output(self, text: str) -> str:
        """Removes potential log lines, extra processing messages, think tags, and leaked analysis from LLM output."""
        if not isinstance(text, str):
            logger.trace(f"Cleanup received non-str type: {type(text)}, returning as is.")
            return text

        # Pattern to match typical log lines: [HH:MM:SS.ms LEVEL] Message
        log_prefix_pattern = r"^\s*\[\d{2}:\d{2}:\d{2}(\.\d{3,6})?\s+\w*\]\s+.*\n?"
        cleaned_text = re.sub(log_prefix_pattern, '', text, flags=re.MULTILINE)

        # Pattern to remove standalone "Processing complete." or "Log stream complete." lines
        processing_complete_pattern = r"^\s*(Processing complete|Log stream complete)\.?\s*\n?"
        cleaned_text = re.sub(processing_complete_pattern, '', cleaned_text, flags=re.IGNORECASE | re.MULTILINE)

        # --- ADDED: Pattern to remove leaked Emotion/User Analysis Preamble ---
        # Looks for lines starting with common analysis phrases up to where the actual response should start
        # This might need refinement based on variations in the LLM's preamble output
        analysis_preamble_pattern = r"^(?:The user(?:'s input|\s+expressed|\s+is asking)|Analysis:|Emotional Tone:|Intent:|Context:).*\n+"
        # Use re.DOTALL? No, process line by line likely safer with MULTILINE
        # Keep removing matches until none are found at the beginning of the string
        original_len = -1
        while len(cleaned_text) != original_len: # Loop until no more changes
            original_len = len(cleaned_text)
            cleaned_text = re.sub(analysis_preamble_pattern, '', cleaned_text.lstrip(), count=1, flags=re.IGNORECASE | re.MULTILINE)
            cleaned_text = cleaned_text.lstrip() # Remove leading space after removal

        # Optional: Remove "Draft Response:" lines if they leak
        draft_response_pattern = r"^\s*(?:Draft Response|Your Final, Refined Response).*?:?\s*\n?"
        cleaned_text = re.sub(draft_response_pattern, '', cleaned_text, flags=re.IGNORECASE | re.MULTILINE)
        # --- END ADDED ---


        # Remove think tags and ChatML tokens just in case
        cleaned_text = re.sub(r'<think>.*?</think>', '', cleaned_text, flags=re.DOTALL | re.IGNORECASE)
        # Remove any remaining ChatML tokens like <|im_start|>assistant or <|im_end|>
        cleaned_text = re.sub(r'<\|im_start\|>.*?<\|im_end\|>', '', cleaned_text, flags=re.DOTALL | re.IGNORECASE)
        cleaned_text = re.sub(r'<\|im_start\|>assistant', '', cleaned_text, flags=re.DOTALL | re.IGNORECASE)
        cleaned_text = re.sub(r'<\|im_end\|>', '', cleaned_text, flags=re.DOTALL | re.IGNORECASE)

        # Remove leading/trailing whitespace that might be left
        cleaned_text = cleaned_text.strip()

        if cleaned_text != text:
            logger.warning(f"Applied cleanup to LLM output. Original len: {len(text)}, Cleaned len: {len(cleaned_text)}")
            logger.debug(f"Original Text Starts: '{text[:150]}...'")
            logger.debug(f"Cleaned Text Starts: '{cleaned_text[:150]}...'")


        return cleaned_text
    async def _trigger_web_search(self, db: Session, session_id: str, query: str) -> str:
        """
        Launches the new playwright-based web search as a background task.
        """
        req_id = f"websearch-{uuid.uuid4()}"
        logger.info(f"🔍 {req_id} Triggering background web search task for query: '{query}'")

        # --- Define which engines to use for this search ---
        # This can be made more dynamic later if needed
        engines_to_use = [
            'ddg', 'google', 'searx', 'semantic_scholar', 'google_scholar', 
            'base', 'core', 'sciencegov', 'baidu_scholar', 'refseek', 
            'scidirect', 'mdpi', 'tandf', 'ieee', 'springer'
        ]
        
        # Log the initiation of the search action
        add_interaction(
            db, session_id=session_id, mode="chat", input_type="log_info",
            user_input=f"Web Search Action Triggered: {query}",
            assistant_action_type="search_web",
            assistant_action_params=json.dumps({"query": query, "engines": engines_to_use}),
            assistant_action_executed=True
        )
        db.commit()

        async def run_search_and_log_result():
            """The actual task that runs in the background."""
            search_results = await search_and_scrape_web_async(query=query, engines=engines_to_use)
            
            bg_db = SessionLocal()
            try:
                if search_results:
                    # Format the results into a single text block for logging
                    formatted_results = "\n\n".join([
                        f"--- Result from {res.get('source_engine', 'N/A')} ---\n"
                        f"Title: {res.get('title', 'No Title')}\n"
                        f"URL: {res.get('url')}\n"
                        f"Snippet: {res.get('snippet', 'N/A')}"
                        for res in search_results
                    ])
                    
                    logger.success(f"{req_id} Web search successful. Logging {len(search_results)} results to DB.")
                    add_interaction(
                        bg_db, session_id=session_id, mode="chat",
                        input_type="web_search_result",
                        user_input=f"[Results for Web Search: {query}]",
                        llm_response=formatted_results
                    )
                else:
                    logger.error(f"{req_id} Web search failed or returned no content.")
                    add_interaction(
                        bg_db, session_id=session_id, mode="chat", input_type="log_error",
                        user_input=f"[Web Search Failed: {query}]",
                        llm_response="The web search process failed to retrieve any content."
                    )
                bg_db.commit()
            finally:
                bg_db.close()

        # Schedule the background task
        asyncio.create_task(run_search_and_log_result())
        
        return f"Okay, I've started a web search for '{query}' in the background. I will analyze the results from {', '.join(engines_to_use)} shortly."
    
    async def _correct_response(self, db: Session, session_id: str, original_input: str, context: Dict, draft_response: str) -> str:
        """
        Uses the corrector LLM (ELP0) to refine a draft response.
        Handles TaskInterruptedException by re-raising it.
        Cleans the output if successful, otherwise returns the cleaned draft on other errors.
        """
        # Unique ID for this specific correction attempt for tracing
        correction_id = f"corr-{uuid.uuid4()}"
        log_prefix = f"✍️ {correction_id}|ELP0" # Add ELP0 marker to log prefix
        logger.info(f"{log_prefix} Refining draft response for session {session_id}...")

        # Get the model configured for correction (using "router" key)
        corrector_model = self.provider.get_model("router")

        # Handle case where the corrector model itself isn't available
        if not corrector_model:
            logger.error(f"{log_prefix} Corrector model (key 'router') not available! Returning cleaned draft.")
            # Clean the original draft using the instance method before returning
            cleaned_draft = self._cleanup_llm_output(draft_response)
            # Log this fallback action
            try: # Use try-except for DB logging
                add_interaction(db, session_id=session_id, mode="chat", input_type="log_warning",
                                user_input="Corrector Fallback",
                                llm_response="Corrector model unavailable, returned cleaned draft.")
            except Exception as db_err:
                logger.error(f"{log_prefix} Failed log corrector model unavailable: {db_err}")
            return cleaned_draft

        # Prepare the input dictionary for the corrector prompt template
        prompt_input = {
            "input": original_input,
            "context": context.get("url_context", "None."),
            "history_rag": context.get("history_rag", "None."),
            "file_index_context": context.get("file_index_context", "None."),
            "log_context": context.get("log_context", "None."),
            "recent_direct_history": context.get("recent_direct_history", "None."),
            "emotion_analysis": context.get("emotion_analysis", "N/A."),
            "draft_response": draft_response
        }

        # Define the Langchain chain for correction
        try:
            chain = (
                ChatPromptTemplate.from_template(PROMPT_CORRECTOR)
                | corrector_model
                | StrOutputParser()
            )
        except Exception as chain_setup_err:
             logger.error(f"{log_prefix} Failed to set up corrector chain: {chain_setup_err}")
             try: # Use try-except for DB logging
                add_interaction(db, session_id=session_id, mode="chat", input_type="log_error",
                                user_input="Corrector Chain Setup Error",
                                llm_response=f"Failed: {chain_setup_err}")
             except Exception as db_err:
                logger.error(f"{log_prefix} Failed log corrector chain setup error: {db_err}")
             # Fallback to cleaned draft if chain setup fails
             cleaned_draft = self._cleanup_llm_output(draft_response)
             return cleaned_draft

        # Prepare timing data dictionary for the LLM call
        corrector_timing_data = {"session_id": session_id, "mode": "chat", "execution_time_ms": 0}

        try:
            # Execute the corrector chain (sync Langchain call) in a separate thread with ELP0
            logger.debug(f"{log_prefix} Calling corrector LLM...")
            refined_response_raw = await asyncio.to_thread(
                self._call_llm_with_timing, # Use the modified helper
                chain,
                prompt_input,
                corrector_timing_data,
                priority=ELP0 # Explicitly set ELP0 priority
            )
            logger.info(f"{log_prefix} Refinement LLM call complete. Raw length: {len(refined_response_raw)}")

            # Apply robust cleanup using the instance method
            final_response_cleaned = self._cleanup_llm_output(refined_response_raw)
            logger.info(f"{log_prefix} Cleaned response length: {len(final_response_cleaned)}")

            # Add detailed log comparing inputs/outputs (Consider sampling if responses are huge)
            log_message = (
                f"Refined draft.\n"
                f"Original Input Snippet: '{original_input[:100]}...'\n"
                f"Draft Response Snippet: '{draft_response[:100]}...'\n"
                f"Raw Corrector Output Snippet: '{refined_response_raw[:100]}...'\n"
                f"Cleaned Final Snippet: '{final_response_cleaned[:100]}...'"
            )
            try: # Use try-except for DB logging
                add_interaction(db, session_id=session_id, mode="chat", input_type="log_debug",
                                user_input="Corrector Step Details",
                                llm_response=log_message[:4000]) # Limit log length
            except Exception as db_err:
                 logger.error(f"{log_prefix} Failed log corrector details: {db_err}")

            # Return the cleaned final response (successful path)
            return final_response_cleaned

        except TaskInterruptedException as tie:
            # Specific handling for interruption
            logger.warning(f"🚦 {log_prefix} Corrector step INTERRUPTED: {tie}")
            # Re-raise the exception to be handled by the calling function (e.g., background_generate)
            raise tie

        except Exception as e:
            # Handle other, non-interruption errors during the LLM call itself
            logger.error(f"❌ {log_prefix} Error during correction LLM call: {e}")
            logger.exception(f"{log_prefix} Corrector Execution Traceback:") # Log full traceback

            # Log the failure to the database
            try: # Use try-except for DB logging
                add_interaction(db, session_id=session_id, mode="chat", input_type="log_error",
                                user_input="Corrector Step Failed",
                                llm_response=f"Correction failed: {e}")
            except Exception as db_err:
                 logger.error(f"{log_prefix} Failed log corrector step failure: {db_err}")

            # Fallback: Clean the original draft response if correction fails
            logger.warning(f"{log_prefix} Falling back to cleaned original draft due to corrector error.")
            cleaned_draft = self._cleanup_llm_output(draft_response)
            return cleaned_draft

        except Exception as e:
            # Handle errors during the LLM call itself
            logger.error(f"❌ {correction_id} Error during correction LLM call: {e}")
            logger.exception("Corrector Execution Traceback:") # Log full traceback
            # Log the failure to the database
            add_interaction(db, session_id=session_id, mode="chat", input_type="log_error",
                            user_input="Corrector Step Failed",
                            llm_response=f"Correction failed: {e}")

            # Fallback: Clean the original draft response if correction fails
            logger.warning(f"{correction_id} Falling back to cleaned original draft due to corrector error.")
            cleaned_draft = self._cleanup_llm_output(draft_response)
            return cleaned_draft


    async def _execute_assistant_action(self, db: Session, session_id: str, action_details: Dict[str, Any], triggering_interaction: Interaction) -> str:
        """
        Executes the specified action by detecting the OS, generating an appropriate script via the agent,
        and running it. Includes a RAG-based refinement loop for script generation.
        V4: Multi-platform script generation (AppleScript, Bash, PowerShell).
        """
        action_type = action_details.get("action_type", "unknown")
        parameters = action_details.get("parameters", {})
        req_id = f"act-{uuid.uuid4()}"
        platform = sys.platform  # 'darwin', 'win32', or 'linux'
        script_lang = 'applescript' if platform == 'darwin' else 'powershell' if platform == 'win32' else 'bash'

        logger.info(f"🚀 {req_id} Handling assistant action: '{action_type}' on platform '{platform}' (Trigger ID: {triggering_interaction.id})")

        # --- Define Fallback Messages ---
        # These are used if the entire process fails after all retries.
        exec_fallback = f"Okay, I tried to perform the '{action_type}' action, but couldn't get it to work after {AGENT_MAX_SCRIPT_RETRIES} attempts. The script kept having errors. You might need to do it manually or check system permissions."
        generation_fallback = f"Sorry, I had trouble figuring out the exact steps to perform the '{action_type}' action. My script generation attempts failed."
        search_exec_fallback = f"Sorry, I encountered an error while trying to start the web search for '{parameters.get('query', 'that topic')}'. Please try again later."
        
        # Use a separate DB session for this self-contained action to avoid conflicts.
        exec_db = SessionLocal()
        try:
            # --- Handle Web Search (Non-Script Action) ---
            if action_type == "search" and parameters.get("query"):
                logger.info(f"{req_id} Handling 'search' action type. Triggering background web search.")
                try:
                    # Log the trigger of the search action
                    add_interaction(exec_db, session_id=session_id, mode="chat", input_type="log_info",
                                    user_input=f"Triggering Web Search: {parameters['query']}",
                                    assistant_action_type=action_type, assistant_action_params=json.dumps(parameters),
                                    assistant_action_executed=True, assistant_action_result="[Search process launched]")
                    exec_db.commit()
                    # Await the trigger function and get the confirmation message
                    confirmation_message = await self._trigger_web_search(exec_db, session_id, parameters["query"])
                    if triggering_interaction:
                        triggering_interaction.assistant_action_executed = True
                        triggering_interaction.assistant_action_result = confirmation_message
                        exec_db.merge(triggering_interaction)
                        exec_db.commit()
                    return confirmation_message
                except Exception as search_err:
                    logger.error(f"{req_id} Error during web search trigger: {search_err}")
                    exec_db.rollback()
                    return search_exec_fallback
            elif action_type == "keyword_based_response_fallback":
                logger.info(f"{req_id} Handling 'keyword_based_response_fallback' action type. Returning explanation directly.")
                # This action type is a fallback and should not attempt script execution.
                # It simply returns the explanation provided in its parameters.
                fallback_message = parameters.get("explanation", "An internal fallback action was triggered.")
                if triggering_interaction:
                    triggering_interaction.assistant_action_executed = True
                    triggering_interaction.assistant_action_result = fallback_message
                    exec_db.merge(triggering_interaction)
                    exec_db.commit()
                return fallback_message

            # --- Platform-Aware Script Generation and Execution Loop ---
            params_json = json.dumps(parameters, sort_keys=True)
            script_to_execute: Optional[str] = None
            last_error_summary: str = "No previous errors."
            last_stderr: str = ""
            last_stdout: str = ""
            last_rc: int = 0

            # This loop handles retries for script GENERATION and EXECUTION.
            for attempt in range(1, AGENT_MAX_SCRIPT_RETRIES + 1):
                logger.info(f"{req_id} {script_lang.capitalize()} Script Attempt {attempt}/{AGENT_MAX_SCRIPT_RETRIES} for '{action_type}'")

                # --- 1. LLM: Generate or Refine Script ---
                # This logic now resides in the AmaryllisAgent.
                script_llm = self.provider.get_model("code")
                if not script_llm:
                    return generation_fallback + " (Code model unavailable)"
                
                # Determine prompt templates based on platform
                if platform == 'darwin':
                    gen_prompt = PROMPT_GENERATE_APPLESCRIPT; refine_prompt = PROMPT_REFINE_APPLESCRIPT
                elif platform == 'win32':
                    gen_prompt = PROMPT_GENERATE_POWERSHELL_SCRIPT; refine_prompt = PROMPT_REFINE_POWERSHELL_SCRIPT
                else: # Linux/other
                    gen_prompt = PROMPT_GENERATE_BASH_SCRIPT; refine_prompt = PROMPT_REFINE_BASH_SCRIPT

                llm_prompt_template = gen_prompt if attempt == 1 else refine_prompt
                
                # RAG: Get past attempts for context
                past_attempts = await asyncio.to_thread(get_past_applescript_attempts, exec_db, action_type, params_json, limit=5)
                past_attempts_context = self._format_script_rag_context(past_attempts)

                llm_input = {
                    "action_type": action_type,
                    "parameters_json": params_json,
                    "past_attempts_context": past_attempts_context
                }
                if attempt > 1:
                    llm_input.update({
                        "failed_script": script_to_execute or "[Script Missing]",
                        "return_code": last_rc,
                        "stderr": last_stderr,
                        "stdout": last_stdout,
                        "error_summary": last_error_summary,
                    })

                script_chain = ChatPromptTemplate.from_template(llm_prompt_template) | script_llm | StrOutputParser()
                
                try:
                    generated_script_raw = await asyncio.to_thread(script_chain.invoke, llm_input)
                    script_to_execute = re.sub(rf"^```(?:{script_lang})?\s*|```\s*$", "", generated_script_raw, flags=re.MULTILINE).strip()
                    if not script_to_execute:
                        logger.warning(f"{req_id} LLM returned empty script on attempt {attempt}.")
                        last_error_summary = "LLM generated an empty script."
                        continue # Go to next attempt
                    logger.info(f"{req_id} LLM {'generated' if attempt == 1 else 'refined'} a {script_lang} script.")
                except Exception as gen_err:
                    logger.error(f"{req_id} Error calling LLM for script attempt {attempt}: {gen_err}")
                    last_error_summary = f"LLM call failed: {gen_err}"
                    if attempt == AGENT_MAX_SCRIPT_RETRIES: return generation_fallback
                    continue

                # --- 2. Execute Script ---
                exec_command: List[str] = []
                if platform == 'darwin':
                    exec_command = ["osascript", "-e", script_to_execute]
                elif platform == 'win32':
                    exec_command = ["powershell.exe", "-ExecutionPolicy", "Bypass", "-Command", script_to_execute]
                else: # Linux and other Unix-like systems
                    exec_command = ["bash", "-c", script_to_execute]

                logger.debug(f"{req_id} Running command: {' '.join(exec_command)}")
                exec_start_time = time.monotonic()
                process = await asyncio.to_thread(subprocess.run, exec_command, capture_output=True, text=True, timeout=120, check=False)
                exec_duration_ms = (time.monotonic() - exec_start_time) * 1000
                
                stdout = process.stdout.strip()
                stderr = process.stderr.strip()
                rc = process.returncode
                logger.info(f"{req_id} Script finished in {exec_duration_ms:.0f}ms. RC={rc}.")
                if stdout: logger.debug(f"  STDOUT: {stdout}")
                if stderr: logger.error(f"  STDERR: {stderr}")

                # --- 3. Store Attempt Result (Crucial for RAG) ---
                success = (rc == 0)
                error_summary = f"RC={rc}. Stderr: {stderr}" if not success else None
                
                attempt_record = AppleScriptAttempt(
                    session_id=session_id,
                    triggering_interaction_id=triggering_interaction.id,
                    action_type=action_type, parameters_json=params_json, attempt_number=attempt,
                    generated_script=script_to_execute, execution_success=success,
                    execution_return_code=rc, execution_stdout=stdout, execution_stderr=stderr,
                    execution_duration_ms=exec_duration_ms,
                    error_summary=error_summary[:1000] if error_summary else None
                )
                exec_db.add(attempt_record)
                exec_db.commit()

                # --- 4. Check Outcome ---
                if success:
                    logger.success(f"✅ {req_id} Script execution successful on attempt {attempt}.")
                    final_result = stdout or f"Action '{action_type}' completed."
                    if triggering_interaction:
                        triggering_interaction.assistant_action_executed = True
                        triggering_interaction.assistant_action_result = final_result
                        exec_db.merge(triggering_interaction)
                        exec_db.commit()
                    return final_result
                else: # Failure
                    logger.error(f"❌ {req_id} {script_lang.capitalize()} attempt {attempt} FAILED. Error: {error_summary}")
                    last_error_summary, last_stderr, last_stdout, last_rc = error_summary, stderr, stdout, rc
                    # Loop continues to the next attempt for refinement...

            # --- End of Loop ---
            logger.error(f"❌ {req_id} Script execution failed after all {AGENT_MAX_SCRIPT_RETRIES} attempts.")
            if triggering_interaction:
                triggering_interaction.assistant_action_executed = True # It was attempted to exhaustion
                triggering_interaction.assistant_action_result = f"Failed after {AGENT_MAX_SCRIPT_RETRIES} attempts. Last Error: {last_error_summary}"
                triggering_interaction.input_type = "log_error"
                exec_db.merge(triggering_interaction)
                exec_db.commit()
            return exec_fallback

        except Exception as e:
            err_msg = f"Unexpected error during action execution for '{action_type}': {e}"
            logger.error(f"{req_id} {err_msg}", exc_info=True)
            if exec_db: exec_db.rollback()
            try:
                if triggering_interaction:
                    triggering_interaction.assistant_action_executed = True # Attempted
                    triggering_interaction.assistant_action_result = err_msg[:1000]
                    triggering_interaction.input_type = "log_error"
                    exec_db.merge(triggering_interaction)
                    exec_db.commit()
            except Exception as log_err:
                logger.error(f"Failed to log final action execution error: {log_err}")
                if exec_db: exec_db.rollback()
            return f"Sorry, I encountered an unexpected internal issue while trying the '{action_type}' action."
        finally:
            if exec_db: exec_db.close()

    def _format_script_rag_context(self, attempts: List[Any]) -> str:
        """Helper to format past script attempts for RAG context. (Kept from original)"""
        if not attempts:
            return "None available."
        context_str = ""
        for i, attempt in enumerate(attempts):
            context_str += f"--- Past Attempt {i+1} ({attempt.timestamp.isoformat()}) ---\n"
            context_str += f"Script:\n```\n{attempt.generated_script or '[Script Missing]'}\n```\n"
            context_str += f"Success: {attempt.execution_success}\n"
            if not attempt.execution_success:
                context_str += f"  RC: {attempt.execution_return_code}\n"
                context_str += f"  Error Summary: {attempt.error_summary}\n"
            context_str += "---\n"
            if len(context_str) > 2000:
                context_str += "[Context truncated]...\n"
                break
        return context_str

    def _format_docs(self, docs: List[Any], source_type: str = "Context") -> str:
        """Helper to format retrieved Langchain Documents into a single string."""
        if not docs:
            logger.trace(f"_format_docs received empty list for {source_type}")
            return f"No relevant {source_type.lower()} found."
        if not isinstance(docs, list):
             logger.warning(f"_format_docs received non-list: {type(docs)}")
             return f"Invalid document list provided for {source_type}."
        if not docs: # Check again after type check
             return f"No relevant {source_type.lower()} found."

        if hasattr(docs[0], 'page_content'):
             return "\n\n".join(f"Source Chunk ({source_type}):\n{doc.page_content}" for doc in docs)
        else:
            logger.warning(f"Unrecognized doc type in _format_docs: {type(docs[0])}. Assuming Interaction list.")
            return self._format_interaction_list_to_string(docs)


    def _format_interaction_list_to_string(self, interactions: List[Interaction], include_type=False) -> str:
        """Formats a list of Interaction objects into a string for RAG/log context."""
        if not interactions:
            return "None found."
        if not isinstance(interactions, list):
             logger.error(f"_format_interaction_list_to_string received non-list: {type(interactions)}")
             return "Invalid interaction list provided."
        if not interactions:
             return "None found."

        str_parts = []
        sorted_interactions = sorted(interactions, key=lambda i: i.timestamp) # Oldest first

        for interaction in sorted_interactions:
            prefix, text = None, None
            if interaction.input_type == 'text' and interaction.user_input:
                prefix = "User:"
                text = interaction.user_input
            elif interaction.llm_response and interaction.input_type not in ['system', 'error', 'log_error', 'log_warning', 'log_info', 'log_debug']:
                prefix = "AI:"
                text = interaction.llm_response
            elif interaction.input_type.startswith('log_'):
                prefix = f"LOG ({interaction.input_type.split('_')[1].upper()}):"
                text = interaction.llm_response
            elif interaction.input_type == 'error':
                prefix = "LOG (ERROR):"
                text = interaction.llm_response
            elif interaction.input_type == 'system':
                prefix = "System:"
                text = interaction.user_input

            if prefix and text:
                entry = f"{prefix} {text}"
                if include_type:
                     entry = f"[{interaction.timestamp.strftime('%H:%M:%S')} {interaction.input_type}] {entry}"
                text_snippet = (entry[:250] + '...') if len(entry) > 250 else entry
                str_parts.append(text_snippet)

        return "\n---\n".join(str_parts) if str_parts else "None found."

    def _format_direct_history(self, interactions: List[Interaction]) -> str:
        """Formats a list of Interaction objects into a chronological string for the prompt."""
        if not interactions:
            return "No recent global conversation history available."
        if not isinstance(interactions, list):
             logger.error(f"_format_direct_history received non-list: {type(interactions)}")
             return "Invalid direct history list provided."
        if not interactions:
             return "No recent global conversation history available."

        history_str_parts = []
        for interaction in interactions: # Assumes sorted oldest first
            prefix, text = None, None
            if interaction.input_type == 'text' and interaction.user_input:
                prefix = "User:"
                text = interaction.user_input
            elif interaction.llm_response and interaction.input_type == 'llm_response':
                 prefix = "AI:"
                 text = interaction.llm_response

            if prefix and text:
                text_snippet = (text[:150] + '...') if len(text) > 150 else text
                history_str_parts.append(f"{prefix} {text_snippet}")

        if not history_str_parts:
             return "No textual conversation history available."

        return "\n".join(history_str_parts)

    def _format_log_history(self, interactions: List[Interaction]) -> str:
        """Formats a list of Interaction log objects into a string for the prompt."""
        if not interactions:
            return "No recent relevant logs found."
        if not isinstance(interactions, list):
             logger.error(f"_format_log_history received non-list: {type(interactions)}")
             return "Invalid log history list provided."
        if not interactions:
             return "No recent relevant logs found."

        log_str_parts = []
        sorted_interactions = sorted(interactions, key=lambda i: i.timestamp, reverse=True)

        for interaction in sorted_interactions:
            log_level = interaction.input_type.split('_')[-1].upper() if '_' in interaction.input_type else interaction.input_type.upper()
            log_message = interaction.llm_response or interaction.user_input or "[Log content missing]"
            timestamp_str = interaction.timestamp.strftime('%H:%M:%S')
            log_snippet = (log_message[:200] + '...') if len(log_message) > 200 else log_message
            log_str_parts.append(f"[{timestamp_str} {log_level}] {log_snippet}")

        return "\n".join(log_str_parts) if log_str_parts else "No recent relevant logs found."

    def _call_llm_with_timing(self, chain: Any, inputs: Any, interaction_data: Dict[str, Any], priority: int = ELP0):
        """
        Wrapper to call LLM chain/model, measure time, log, and handle priority/interruptions.
        Implements retries for ELP0 tasks if they encounter TaskInterruptedException.
        """
        request_start_time = time.monotonic()  # Time for the whole operation including retries
        response_from_llm = None

        # Determine retry parameters based on priority
        # Only ELP0 tasks will attempt retries on interruption
        max_retries = LLM_CALL_ELP0_INTERRUPT_MAX_RETRIES if priority == ELP0 else 0
        retry_delay_seconds = LLM_CALL_ELP0_INTERRUPT_RETRY_DELAY

        attempt_count = 0
        while attempt_count <= max_retries:
            attempt_count += 1
            call_start_time = time.monotonic()  # Time for this specific attempt
            log_prefix_call = f"LLMCall|ELP{priority}|Attempt-{attempt_count}"

            try:
                logger.trace(f"{log_prefix_call}: Invoking chain/model {type(chain)}...")

                llm_call_config = {'priority': priority}  # For LlamaCppChatWrapper

                # The actual call to the LLM (via chain or model)
                if hasattr(chain, 'invoke') and callable(chain.invoke):  # Langchain runnable
                    response_from_llm = chain.invoke(inputs, config=llm_call_config)
                elif callable(chain):  # Direct model call (e.g., for raw ChatML in direct_generate)
                    # Assuming 'chain' is the model and 'inputs' is the raw prompt string.
                    # The LlamaCppChatWrapper._call method handles 'priority' from CortexConfiguration.
                    response_from_llm = chain(messages=inputs, stop=[CHATML_END_TOKEN], **llm_call_config)
                else:
                    raise TypeError(f"Unsupported chain/model type for _call_llm_with_timing: {type(chain)}")

                call_duration_ms = (time.monotonic() - call_start_time) * 1000
                # Log duration for this specific attempt
                logger.info(f"⏱️ {log_prefix_call}: Succeeded in {call_duration_ms:.2f} ms")

                # Update total execution time in interaction_data with this attempt's duration
                interaction_data['execution_time_ms'] = interaction_data.get('execution_time_ms', 0) + call_duration_ms

                # Check if the response string itself indicates an interruption (from worker)
                if isinstance(response_from_llm, str) and interruption_error_marker in response_from_llm:
                    logger.warning(f"🚦 {log_prefix_call}: Task Interrupted (marker found in LLM response string).")
                    raise TaskInterruptedException(response_from_llm)  # Trigger retry logic

                return response_from_llm  # Successful call, exit retry loop

            except TaskInterruptedException as tie:
                call_duration_ms_on_interrupt = (time.monotonic() - call_start_time) * 1000
                interaction_data['execution_time_ms'] = interaction_data.get('execution_time_ms',
                                                                             0) + call_duration_ms_on_interrupt
                logger.warning(
                    f"🚦 {log_prefix_call}: Caught TaskInterruptedException after {call_duration_ms_on_interrupt:.2f}ms: {tie}")

                if priority == ELP0 and attempt_count <= max_retries:
                    logger.info(
                        f"    Retrying ELP0 task (attempt {attempt_count}/{max_retries + 1}) after {retry_delay_seconds}s due to interruption...")
                    # For asyncio.to_thread compatibility, use synchronous time.sleep
                    time.sleep(retry_delay_seconds)
                    # Loop continues for the next attempt
                else:
                    # Max retries reached for ELP0, or it's not an ELP0 task, or error from non-LLM part
                    if priority == ELP0:
                        logger.error(f"    ELP0 task giving up after {attempt_count} interruption attempts.")
                    raise  # Re-raise TaskInterruptedException to be handled by the caller (e.g., background_generate)

            except Exception as e:  # Handles other exceptions not related to TaskInterruptedException
                call_duration_ms_on_error = (time.monotonic() - call_start_time) * 1000
                interaction_data['execution_time_ms'] = interaction_data.get('execution_time_ms',
                                                                             0) + call_duration_ms_on_error
                log_err_msg = f"LLM Chain/Model Error (ELP{priority}, Attempt {attempt_count}): {e}"
                logger.error(f"❌ {log_err_msg}")
                # Log full traceback for these non-interruption errors
                logger.exception(f"Traceback for LLM Chain/Model error ({log_prefix_call}):")

                # Log this error to DB (simplified for brevity, actual DB logging in background_generate)
                session_id_for_log = interaction_data.get("session_id", "unknown_session")
                # add_interaction(db, session_id=session_id_for_log, ..., llm_response=log_err_msg)

                raise  # Re-raise the original non-interruption error; these are not retried by this loop

        # This part of the function should ideally not be reached if the loop logic is correct,
        # as success returns directly, and exceptions (including TaskInterruptedException after max retries) are re-raised.
        # This is a fallback.
        total_duration_ms = (time.monotonic() - request_start_time) * 1000
        interaction_data['execution_time_ms'] = total_duration_ms  # Ensure total time is updated
        logger.error(
            f"{log_prefix_call}: LLM call failed after all retries or was not retriable. Returning error indication.")
        return f"[LLM_CALL_UNEXPECTED_EXIT_ELP{priority}]"

    def _extract_json_candidate_string(self, raw_llm_text: str, log_prefix: str = "JSONExtract") -> Optional[str]:
        """
        Robustly extracts a potential JSON string from raw LLM text.
        Handles <think> tags, markdown code blocks, and finding outermost braces.
        """
        if not raw_llm_text or not isinstance(raw_llm_text, str):
            return None

        logger.trace(f"{log_prefix}: Starting JSON candidate extraction from raw text (len {len(raw_llm_text)}).")

        text_after_think_removal = re.sub(r'<think>.*?</think>', '', raw_llm_text,
                                          flags=re.DOTALL | re.IGNORECASE).strip()
        if not text_after_think_removal:
            logger.trace(f"{log_prefix}: Text empty after <think> removal.")
            return None

        cleaned_text = text_after_think_removal
        cleaned_text = re.sub(r"^\s*(assistant\s*\n?)?(<\|im_start\|>\s*(system|assistant)\s*\n?)?", "", cleaned_text,
                              flags=re.IGNORECASE).lstrip()
        if CHATML_END_TOKEN and cleaned_text.endswith(CHATML_END_TOKEN):
            cleaned_text = cleaned_text[:-len(CHATML_END_TOKEN)].strip()

        # Remove common LLM specific tokens that might interfere with JSON parsing
        cleaned_text = re.sub(r"<\|im_start\|>.*?<\|im_end\|>", "", cleaned_text, flags=re.DOTALL)
        cleaned_text = re.sub(r"<\|im_start\|>", "", cleaned_text)
        cleaned_text = re.sub(r"<\|im_end\|>", "", cleaned_text)
        json_markdown_match = re.search(r"```json\s*(\{[\s\S]*?\})\s*```", cleaned_text, re.DOTALL)
        if json_markdown_match:
            extracted_str = json_markdown_match.group(1).strip()
            logger.trace(f"{log_prefix}: Extracted JSON from markdown block: '{extracted_str[:100]}...'")
            return extracted_str

        first_brace = cleaned_text.find('{')
        last_brace = cleaned_text.rfind('}')
        if first_brace != -1 and last_brace != -1 and last_brace > first_brace:
            extracted_str = cleaned_text[first_brace: last_brace + 1].strip()
            logger.trace(f"{log_prefix}: Extracted JSON using outermost braces: '{extracted_str[:100]}...'")
            return extracted_str

        if cleaned_text.startswith("{") and cleaned_text.endswith("}"):
            logger.trace(f"{log_prefix}: Assuming cleaned text itself is the JSON candidate: '{cleaned_text[:100]}...'")
            return cleaned_text

        logger.warning(
            f"{log_prefix}: No clear JSON structure found after extraction attempts. Raw (after think): '{text_after_think_removal[:100]}...'")
        return None

    def _programmatic_json_parse_and_fix(self, json_candidate_str: str,
                                         max_fix_attempts: int = 3,
                                         log_prefix: str = "JSONFixParse") -> Optional[Union[Dict, List]]:
        """
        Attempts to parse a JSON string, applying a limited set of programmatic fixes
        if initial parsing fails.
        """
        if not json_candidate_str or not isinstance(json_candidate_str, str):
            return None

        current_text_to_parse = json_candidate_str
        json_parser = JsonOutputParser()

        for attempt in range(max_fix_attempts):
            logger.debug(f"{log_prefix}: Parse/fix attempt {attempt + 1}/{max_fix_attempts}.")
            try:
                parsed_object = json_parser.parse(current_text_to_parse)
                logger.debug(f"{log_prefix}: Successfully parsed JSON on attempt {attempt + 1}.")
                return parsed_object
            except (json.JSONDecodeError, OutputParserException) as e:
                logger.warning(f"{log_prefix}: Attempt {attempt + 1} parse failed: {type(e).__name__} - {str(e)[:100]}")
                if attempt == max_fix_attempts - 1:
                    logger.error(f"{log_prefix}: Max fix attempts reached. Could not parse after error: {e}")
                    break

                text_before_fixes = current_text_to_parse
                current_text_to_parse = re.sub(r",\s*([\}\]])", r"\1", current_text_to_parse)  # Fix trailing commas
                text_after_comment_removal = re.sub(r"//.*?\n", "\n", current_text_to_parse, flags=re.MULTILINE)
                current_text_to_parse = re.sub(r"/\*.*?\*/", "", text_after_comment_removal, flags=re.DOTALL).strip()

                if current_text_to_parse == text_before_fixes:
                    logger.warning(f"{log_prefix}: Programmatic fixes did not alter the string. Error may be complex.")
                    break  # Break if no progress is made

        logger.error(f"{log_prefix}: Failed to parse JSON after all programmatic fix attempts.")
        return None


    async def _classify_input_complexity(self, db: Session, user_input: str,
                                         interaction_data_for_metrics: dict) -> str:
        """
        Classifies input as 'chat_simple', 'chat_complex', or 'agent_task'.
        Uses router model, expects JSON, and now uses robust extraction and parsing/fixing.
        """
        request_id_suffix = str(uuid.uuid4())[:8]
        # Use session_id from interaction_data if available, else a default
        current_session_id = interaction_data_for_metrics.get("session_id", "unknown_classify_session")
        log_prefix = f"🤔 Classify|ELP0|{current_session_id[:8]}-{request_id_suffix}"
        logger.info(f"{log_prefix} Classifying input complexity for: '{user_input[:50]}...'")

        # Get history summary synchronously (it's a DB call)
        history_summary = await asyncio.to_thread(self._get_history_summary, db, MEMORY_SIZE)  # MEMORY_SIZE from CortexConfiguration

        classification_model_instance = self.provider.get_model("router")
        if not classification_model_instance:
            logger.warning(f"{log_prefix} Router model unavailable for classification. Falling back to default model.")
            classification_model_instance = self.provider.get_model("default")

        if not classification_model_instance:
            error_msg = "Classification model (router/default) not available."
            logger.error(f"{log_prefix} ❌ {error_msg}")
            interaction_data_for_metrics['classification'] = "chat_simple"  # Fallback
            interaction_data_for_metrics['classification_reason'] = error_msg
            await asyncio.to_thread(add_interaction, db, session_id=current_session_id, mode="chat",
                                    input_type="log_error", user_input="[Classify Model Unavailable]",
                                    llm_response=error_msg)
            await asyncio.to_thread(db.commit)
            return "chat_simple"

        classification_model_for_call = classification_model_instance
        if hasattr(classification_model_instance, 'bind') and callable(getattr(classification_model_instance, 'bind')):
            try:
                classification_model_for_call = classification_model_instance.bind(temperature=0.1)
            except Exception as bind_err:
                logger.warning(f"{log_prefix} Could not bind temperature: {bind_err}.")

        prompt_inputs_for_classification = {"input": user_input, "history_summary": history_summary}
        classification_chain_raw_output = (
                self.input_classification_prompt  # PROMPT_COMPLEXITY_CLASSIFICATION from CortexConfiguration
                | classification_model_for_call
                | StrOutputParser()
        )

        last_error_for_log: Optional[Exception] = None
        raw_llm_response_for_final_log: str = "Classification LLM call did not produce parsable output."
        parsed_json_output: Optional[Dict[str, Any]] = None

        # Initial LLM calls and parsing attempts
        for attempt in range(DEEP_THOUGHT_RETRY_ATTEMPTS):  # DEEP_THOUGHT_RETRY_ATTEMPTS from CortexConfiguration
            current_attempt_num = attempt + 1
            logger.debug(
                f"{log_prefix} Classification LLM call attempt {current_attempt_num}/{DEEP_THOUGHT_RETRY_ATTEMPTS}")

            raw_llm_text_this_attempt = ""
            try:
                raw_llm_text_this_attempt = await asyncio.to_thread(
                    self._call_llm_with_timing,
                    classification_chain_raw_output,
                    prompt_inputs_for_classification,
                    interaction_data_for_metrics,  # For timing
                    priority=ELP0
                )
                raw_llm_response_for_final_log = raw_llm_text_this_attempt  # Save last raw output
                logger.trace(
                    f"{log_prefix} Raw LLM for classification (Attempt {current_attempt_num}): '{raw_llm_text_this_attempt[:200]}...'")

                json_candidate_str = self._extract_json_candidate_string(raw_llm_text_this_attempt,
                                                                         log_prefix + "-Extract")
                if json_candidate_str:
                    # Try parsing with limited fixes (e.g., 1 attempt for this initial loop)
                    parsed_json_output = self._programmatic_json_parse_and_fix(
                        json_candidate_str,
                        1,  # Only 1 fix attempt per initial LLM call
                        log_prefix + f"-InitialFixAttempt{current_attempt_num}"
                    )
                    if parsed_json_output and isinstance(parsed_json_output, dict) and \
                            "classification" in parsed_json_output and "reason" in parsed_json_output:
                        classification_val = str(parsed_json_output.get("classification", "chat_simple")).lower()
                        reason_val = str(parsed_json_output.get("reason", "N/A"))
                        if classification_val not in ["chat_simple", "chat_complex", "agent_task"]:
                            logger.warning(f"{log_prefix} Invalid category '{classification_val}'. Defaulting.")
                            classification_val = "chat_simple"
                        interaction_data_for_metrics['classification'] = classification_val
                        interaction_data_for_metrics['classification_reason'] = reason_val
                        logger.info(f"✅ {log_prefix} Input classified as: '{classification_val}'. Reason: {reason_val}")
                        return classification_val  # Success
                else:  # No candidate extracted
                    last_error_for_log = ValueError(
                        f"No JSON candidate extracted from LLM output: {raw_llm_text_this_attempt[:100]}")

            except TaskInterruptedException as tie:
                raise tie  # Propagate immediately
            except Exception as e:
                last_error_for_log = e

            logger.warning(
                f"⚠️ {log_prefix} Classification attempt {current_attempt_num} failed. Error: {last_error_for_log}")

        # --- If all initial attempts failed, try LLM Re-request to fix format ---
        if not parsed_json_output:  # Check if we still don't have valid JSON
            logger.warning(
                f"{log_prefix} Initial classification attempts failed. Trying LLM re-request to fix format. Last raw output: '{raw_llm_response_for_final_log[:200]}...'")
            action_analysis_model = self.provider.get_model("router")
            reformat_prompt_input = {"faulty_llm_output_for_reformat": raw_llm_response_for_final_log}
            reformat_chain = ChatPromptTemplate.from_template(
                PROMPT_REFORMAT_TO_ACTION_JSON) | action_analysis_model | StrOutputParser()  # PROMPT_REFORMAT_TO_ACTION_JSON from CortexConfiguration

            reformatted_llm_output_text = await asyncio.to_thread(
                self._call_llm_with_timing, reformat_chain, reformat_prompt_input,
                interaction_data_for_metrics, priority=ELP0
            )

            if reformatted_llm_output_text and not (
                    isinstance(reformatted_llm_output_text, str) and "ERROR" in reformatted_llm_output_text.upper()):
                logger.info(
                    f"{log_prefix} Received reformatted output from LLM for classification. Attempting to parse/fix...")
                json_candidate_from_reformat = self._extract_json_candidate_string(reformatted_llm_output_text,
                                                                                   log_prefix + "-ReformatExtract")
                if json_candidate_from_reformat:
                    parsed_json_output = self._programmatic_json_parse_and_fix(
                        json_candidate_from_reformat,
                        JSON_FIX_RETRY_ATTEMPTS_AFTER_REFORMAT,  # from CortexConfiguration (e.g., 2-3 attempts)
                        log_prefix + "-ReformatFix"
                    )
                    if parsed_json_output and isinstance(parsed_json_output, dict) and \
                            "classification" in parsed_json_output and "reason" in parsed_json_output:
                        classification_val = str(parsed_json_output.get("classification", "chat_simple")).lower()
                        reason_val = str(parsed_json_output.get("reason", "N/A (reformatted)"))
                        if classification_val not in ["chat_simple", "chat_complex", "agent_task"]:
                            logger.warning(
                                f"{log_prefix} Invalid category '{classification_val}' from reformat. Defaulting.")
                            classification_val = "chat_simple"
                        interaction_data_for_metrics['classification'] = classification_val
                        interaction_data_for_metrics['classification_reason'] = reason_val
                        logger.info(
                            f"✅ {log_prefix} Reformat & Fix successful: Classified as '{classification_val}'. Reason: {reason_val}")
                        return classification_val  # Success after reformat and fix
                else:
                    logger.error(
                        f"{log_prefix} Failed to extract any JSON from LLM's reformat attempt. Output: {reformatted_llm_output_text[:200]}")
            else:
                logger.error(
                    f"{log_prefix} LLM re-request for JSON formatting failed or returned error: {reformatted_llm_output_text}")

        # --- Fallback if all methods failed ---
        final_fallback_classification = "chat_simple"
        final_fallback_reason = f"Classification failed after all attempts. Last error: {last_error_for_log}. Last LLM raw: {raw_llm_response_for_final_log[:200]}..."
        logger.error(
            f"{log_prefix} ❌ All classification methods failed. Defaulting to '{final_fallback_classification}'. Reason: {final_fallback_reason}")

        interaction_data_for_metrics['classification'] = final_fallback_classification
        interaction_data_for_metrics['classification_reason'] = final_fallback_reason
        await asyncio.to_thread(add_interaction, db, session_id=current_session_id, mode="chat", input_type="log_error",
                                user_input=f"[Classify Max Retries for: {user_input[:100]}]",
                                llm_response=final_fallback_reason[:4000])
        await asyncio.to_thread(db.commit)
        return final_fallback_classification


    def _run_tree_of_thought(self, db: Session, input: str, rag_context_docs: List[Any], history_rag_interactions: List[Interaction], log_context_str: str, recent_direct_history_str: str, file_index_context_str: str, interaction_data: Dict[str, Any], triggering_interaction_id: int) -> str:
        """Runs Tree of Thoughts simulation (synchronous), includes direct history and logs."""
        user_input = input
        logger.warning(f"🌳 Running ToT for input: '{user_input[:50]}...' (Trigger ID: {triggering_interaction_id})")
        interaction_data['tot_analysis_requested'] = True
        rag_context_str = self._format_docs(rag_context_docs, source_type="URL")
        history_rag_str = self._format_interaction_list_to_string(history_rag_interactions) # Format Interaction list

        tot_model = self.provider.get_model("router")
        if not tot_model:
            logger.error("ToT model ('router') not available. Cannot run Tree of Thoughts.")
            return "Error: Deep analysis model is not configured."

        chain = (self.tot_prompt | tot_model | StrOutputParser())
        tot_result = "Error during ToT analysis."
        try:
            llm_result = self._call_llm_with_timing(
                chain,
                {
                    "input": user_input,
                    "context": rag_context_str,
                    "history_rag": history_rag_str,
                    "file_index_context": file_index_context_str, # Added here
                    "log_context": log_context_str,
                    "recent_direct_history": recent_direct_history_str
                },
                interaction_data
            )
            tot_result = llm_result
            logger.info(f"🌳 ToT analysis LLM call complete for Trigger ID: {triggering_interaction_id}.")

            if triggering_interaction_id:
                logger.debug(f"Attempting to save ToT result to original interaction ID: {triggering_interaction_id}")
                trigger_interaction = db.query(Interaction).filter(Interaction.id == triggering_interaction_id).first()
                if trigger_interaction:
                    trigger_interaction.tot_result = tot_result
                    trigger_interaction.tot_analysis_requested = True
                    trigger_interaction.tot_delivered = False
                    db.commit()
                    logger.success(f"✅ Saved ToT result to Interaction ID {triggering_interaction_id} (undelivered).")
                else:
                    logger.error(f"❌ Could not find original interaction {triggering_interaction_id} to save ToT result.")
                    add_interaction(db, session_id=interaction_data.get("session_id"), mode="chat", input_type="log_warning", llm_response=f"Orphaned ToT Result for input '{user_input[:50]}...': {tot_result[:200]}...")
            else:
                 logger.warning("No triggering interaction ID provided to save ToT result.")

            return tot_result
        except Exception as e:
            err_msg = f"Error during ToT generation (Trigger ID: {triggering_interaction_id}): {e}"
            logger.error(f"❌ {err_msg}")
            add_interaction(db, session_id=interaction_data.get("session_id"), mode="chat", input_type="log_error", llm_response=err_msg)
            if triggering_interaction_id:
                 trigger_interaction = db.query(Interaction).filter(Interaction.id == triggering_interaction_id).first()
                 if trigger_interaction:
                     trigger_interaction.tot_result = err_msg
                     trigger_interaction.tot_delivered = False
                     db.commit()
            return "Error during deep analysis."

    def _run_emotion_analysis(self, db: Session, user_input: str, interaction_data: dict) -> str:
        """
        Analyzes emotion/context (synchronous).
        Updates interaction_data with the analysis result or error.
        Returns the analysis string or an error message.
        """
        request_id_suffix = str(uuid.uuid4())[:8]
        log_prefix = f"😊 EmotionAnalyze|{interaction_data.get('session_id', 'unknown')[:8]}-{request_id_suffix}"
        logger.info(f"{log_prefix} Analyzing input emotion/context for: '{user_input[:50]}...'")

        history_summary = self._get_history_summary(db, MEMORY_SIZE)  # MEMORY_SIZE from CortexConfiguration

        # Determine which model role to use for emotion analysis
        emotion_model_role = "router"  # Configurable: could be "router" or a dedicated role
        emotion_model = self.provider.get_model(emotion_model_role)

        analysis_result_for_return = "Analysis unavailable."  # Default return

        if not emotion_model:
            error_msg = f"Emotion analysis model ('{emotion_model_role}') not available."
            logger.error(f"{log_prefix} {error_msg}")
            interaction_data['emotion_context_analysis'] = error_msg  # Update main interaction data
            analysis_result_for_return = f"Could not analyze emotion (model '{emotion_model_role}' unavailable)."
            try:
                add_interaction(db, session_id=interaction_data.get("session_id"), mode="chat",
                                input_type="log_error",
                                user_input="[Emotion Analysis Init Failed]",
                                llm_response=error_msg)
            except Exception as db_err:
                logger.error(f"{log_prefix} Failed to log emotion model unavailable error: {db_err}")
            return analysis_result_for_return

        try:
            # Construct the chain with the fetched model
            chain = (self.emotion_analysis_prompt | emotion_model | StrOutputParser())

            # _call_llm_with_timing mutates interaction_data for 'execution_time_ms'
            # It uses ELP0 by default unless overridden
            analysis = self._call_llm_with_timing(
                chain,
                {"input": user_input, "history_summary": history_summary},
                interaction_data  # Pass the main interaction_data for timing updates
            )

            # Clean up the analysis string
            cleaned_analysis = self._cleanup_llm_output(analysis)  # Use existing cleanup

            logger.info(f"{log_prefix} Emotion/Context Analysis Result: {cleaned_analysis[:200]}...")
            interaction_data['emotion_context_analysis'] = cleaned_analysis  # Update main interaction data
            analysis_result_for_return = cleaned_analysis

            # Log the successful analysis (optional, as it's stored in interaction_data)
            # try:
            #     add_interaction(db, session_id=interaction_data.get("session_id"), mode="chat",
            #                     input_type="log_debug", user_input="[Emotion Analysis Success]",
            #                     llm_response=cleaned_analysis[:500])
            # except Exception: pass

        except TaskInterruptedException as tie:
            error_msg = f"[Emotion Analysis Interrupted by higher priority task: {tie}]"
            logger.warning(f"🚦 {log_prefix} Emotion analysis INTERRUPTED: {tie}")
            interaction_data['emotion_context_analysis'] = error_msg
            analysis_result_for_return = error_msg
            try:
                add_interaction(db, session_id=interaction_data.get("session_id"), mode="chat",
                                input_type="log_warning",
                                user_input="[Emotion Analysis Interrupted]",
                                llm_response=str(tie)[:4000])
            except Exception as db_err:
                logger.error(f"{log_prefix} Failed log emotion analysis interruption: {db_err}")
            # For emotion analysis, we typically don't re-raise interruption to stop the whole background_generate,
            # just record that it was interrupted.
        except Exception as e:
            error_msg = f"Error during emotion analysis: {e}"
            logger.error(f"❌ {log_prefix} {error_msg}")
            logger.exception(f"{log_prefix} Emotion Analysis Traceback:")
            interaction_data['emotion_context_analysis'] = error_msg
            analysis_result_for_return = f"Could not analyze emotion (processing error: {type(e).__name__})."
            try:
                add_interaction(db, session_id=interaction_data.get("session_id"), mode="chat",
                                input_type="log_error",
                                user_input="[Emotion Analysis Failed]",
                                llm_response=error_msg[:4000])
            except Exception as db_err:
                logger.error(f"{log_prefix} Failed to log emotion analysis error: {db_err}")

        return analysis_result_for_return


    def _get_history_summary(self, db: Session, limit: int) -> str:
        """Gets a simple string summary of recent chat interactions (synchronous)."""
        interactions = get_recent_interactions(db, limit=limit * 2, session_id=self.current_session_id, mode="chat", include_logs=False)
        if not interactions:
            return "No recent conversation history."
        summary = []
        interactions.reverse()
        processed_count = 0
        for interaction in interactions:
             prefix, text = None, None
             if interaction.llm_response and interaction.input_type != 'system': prefix, text = "AI:", interaction.llm_response
             elif interaction.user_input and interaction.input_type != 'system': prefix, text = "User:", interaction.user_input

             if text:
                text = (text[:150] + '...') if len(text) > 150 else text
                summary.append(f"{prefix} {text}")
                processed_count += 1
                if processed_count >= limit:
                    break
        return "\n".join(summary)

    async def _run_tot_in_background_wrapper_v2(self, db_session_factory: Any,
                                                original_input_for_tot: str,  # Renamed for clarity
                                                rag_context_docs: List[Any],
                                                history_rag_interactions: List[Any],
                                                log_context_str: str,
                                                recent_direct_history_str: str,
                                                file_index_context_str: str,
                                                triggering_interaction_id: int,
                                                # ID of interaction that triggered this ToT
                                                imagined_image_context_str: str):

        # This wrapper runs in a separate thread created by asyncio.create_task(self._run_tot_in_background_wrapper_v2(...))
        # So, it needs its own DB session.
        db_for_tot_thread: Optional[Session] = None
        thread_log_prefix = f"BG_ToT_Wrap|TrigID:{triggering_interaction_id}"
        logger.info(f"{thread_log_prefix}: Background ToT task thread started.")

        try:
            db_for_tot_thread = db_session_factory()  # Create a new session for this thread
            if not db_for_tot_thread:
                logger.error(f"{thread_log_prefix}: Failed to create DB session. Aborting ToT.")
                return

            # Prepare interaction_data for the _call_llm_with_timing within _run_tree_of_thought_v2
            # This is for metrics of the ToT LLM call itself.
            interaction_data_for_llm_call = {
                'session_id': self.current_session_id,  # Use session_id from the CortexThoughts instance
                'mode': 'chat',  # Or 'internal_tot_llm_call'
                'execution_time_ms': 0
            }

            # Run the synchronous _run_tree_of_thought_v2 using asyncio.to_thread
            # because _run_tree_of_thought_v2 itself makes blocking calls (_call_llm_with_timing)
            await asyncio.to_thread(
                self._run_tree_of_thought_v2,
                db=db_for_tot_thread,
                input=original_input_for_tot,  # This is passed to ToT prompt as {input}
                rag_context_docs=rag_context_docs,
                history_rag_interactions=history_rag_interactions,
                log_context_str=log_context_str,
                recent_direct_history_str=recent_direct_history_str,
                file_index_context_str=file_index_context_str,
                imagined_image_context_str=imagined_image_context_str,
                interaction_data_for_tot_llm_call=interaction_data_for_llm_call,  # For the LLM call timing
                original_user_input_for_log=original_input_for_tot,  # For logging within ToT result record
                triggering_interaction_id_for_log=triggering_interaction_id  # For logging
            )
            logger.info(f"{thread_log_prefix}: _run_tree_of_thought_v2 completed execution.")

            # Mark the original interaction as "ToT analysis spawned"
            # This is better than directly putting the ToT result on it.
            trigger_interaction = db_for_tot_thread.query(Interaction).filter(
                Interaction.id == triggering_interaction_id).first()
            if trigger_interaction:
                if hasattr(trigger_interaction, 'tot_analysis_spawned'):
                    trigger_interaction.tot_analysis_spawned = True  # type: ignore
                    trigger_interaction.last_modified_db = time.strftime("%Y-%m-%d %H:%M:%S")  # type: ignore
                    db_for_tot_thread.commit()
                    logger.info(
                        f"{thread_log_prefix}: Marked original Interaction ID {triggering_interaction_id} as tot_analysis_spawned=True.")
                else:
                    logger.warning(
                        f"{thread_log_prefix}: Original Interaction ID {triggering_interaction_id} missing 'tot_analysis_spawned' field.")
            else:
                logger.error(
                    f"{thread_log_prefix}: Could not find original Interaction ID {triggering_interaction_id} to mark as ToT spawned.")

        except TaskInterruptedException:
            logger.warning(f"🚦 {thread_log_prefix}: ToT task was interrupted.")
            # The interruption should have been logged by _run_tree_of_thought_v2 already.
        except Exception as e:
            logger.error(f"{thread_log_prefix}: Error running ToT in background wrapper: {e}")
            logger.exception(f"{thread_log_prefix} ToT Wrapper Traceback:")
            # The error should have been logged by _run_tree_of_thought_v2 as a new interaction.
        finally:
            if db_for_tot_thread:
                try:
                    db_for_tot_thread.close()
                except Exception as e_close:
                    logger.error(f"{thread_log_prefix}: Error closing ToT DB session: {e_close}")
            logger.info(f"{thread_log_prefix}: Background ToT task thread finished.")

    async def _run_tot_in_background_wrapper(self, db_session_factory: Any, input: str, rag_context_docs: List[Any], history_rag_interactions: List[Interaction], log_context_str: str, recent_direct_history_str: str, file_index_context_str: str, triggering_interaction_id: int):
        """Async wrapper to run synchronous ToT logic with its own DB session."""
        logger.info(f"BG ToT Wrapper: Starting for trigger ID {triggering_interaction_id}")
        db = db_session_factory()
        bg_interaction_data = {'id': triggering_interaction_id, 'execution_time_ms': 0, 'session_id': self.current_session_id, 'mode': 'chat'}
        try:
            await asyncio.to_thread(
                self._run_tree_of_thought,
                db=db,
                input=input,
                rag_context_docs=rag_context_docs,
                history_rag_interactions=history_rag_interactions,
                log_context_str=log_context_str,
                recent_direct_history_str=recent_direct_history_str,
                file_index_context_str=file_index_context_str, # Passed here
                interaction_data=bg_interaction_data,
                triggering_interaction_id=triggering_interaction_id
            )
            logger.info(f"BG ToT Wrapper: Finished successfully for trigger ID {triggering_interaction_id}")
        except Exception as e:
            logger.error(f"BG ToT Wrapper: Error running ToT for trigger ID {triggering_interaction_id}: {e}")
        finally:
            if db:
                 db.close()

    async def _analyze_assistant_action(self, db: Session, user_input: str, session_id: str,
                                        context: Dict[str, str]) -> Optional[Dict[str, Any]]:
        request_id_suffix = str(uuid.uuid4())[:8]
        log_prefix = f"🤔 ActionAnalyze|ELP0|{session_id[:8]}-{request_id_suffix}"
        logger.info(f"{log_prefix} Analyzing input for action: '{user_input[:50]}...'")

        prompt_input_initial = {
            "input": user_input, "history_summary": context.get("history_summary", "N/A"),
            "log_context": context.get("log_context", "N/A"),
            "recent_direct_history": context.get("recent_direct_history", "N/A")
        }

        action_analysis_model = self.provider.get_model("router") or self.provider.get_model("default")
        if not action_analysis_model:
            logger.error(f"{log_prefix} ❌ Action analysis model (router/default) not available!")
            return None

        analysis_chain_raw = ChatPromptTemplate.from_template(
            PROMPT_ASSISTANT_ACTION_ANALYSIS) | action_analysis_model | StrOutputParser()
        action_timing_data = {"session_id": session_id, "mode": "chat", "execution_time_ms": 0}

        raw_llm_output_from_initial_loop: str = "Initial LLM action analysis did not yield parsable output."
        parsed_action_json: Optional[Dict[str, Any]] = None

        for attempt in range(DEEP_THOUGHT_RETRY_ATTEMPTS):
            current_attempt_num = attempt + 1
            logger.debug(
                f"{log_prefix} Initial Action analysis LLM call attempt {current_attempt_num}/{DEEP_THOUGHT_RETRY_ATTEMPTS}")
            try:
                raw_llm_text = await asyncio.to_thread(self._call_llm_with_timing, analysis_chain_raw,
                                                       prompt_input_initial, action_timing_data, priority=ELP0)
                raw_llm_output_from_initial_loop = raw_llm_text
                json_candidate = self._extract_json_candidate_string(raw_llm_text, log_prefix)
                if json_candidate:
                    parsed_action_json = self._programmatic_json_parse_and_fix(json_candidate, 1,
                                                                               log_prefix + f"-InitialFix{current_attempt_num}")
                    if parsed_action_json and isinstance(parsed_action_json,
                                                         dict) and "action_type" in parsed_action_json and "parameters" in parsed_action_json:
                        action_type = parsed_action_json.get("action_type")
                        logger.info(
                            f"✅ {log_prefix} Initial Action analysis successful (Attempt {current_attempt_num}): Type='{action_type}'")
                        return parsed_action_json if action_type != "no_action" else None
            except TaskInterruptedException as tie:
                raise tie
            except Exception as e:
                logger.warning(f"⚠️ {log_prefix} Initial Action analysis attempt {current_attempt_num} failed: {e}")

        if not parsed_action_json:
            logger.warning(
                f"{log_prefix} Initial attempts failed. Trying LLM re-request to fix format. Last raw output: '{raw_llm_output_from_initial_loop[:200]}...'")

            # --- START OF THE FIX ---
            # Define the example string that the new prompt expects.
            json_example_string = """{
  "action_type": "string (e.g., 'search', 'scheduling', or 'no_action')",
  "parameters": {
    "param_key_1": "string_value_1",
    "param_key_2": "string_value_2"
  },
  "explanation": "string (your reasoning for the choice)"
}"""



            reformat_prompt_input = {
                "faulty_llm_output_for_reformat": raw_llm_output_from_initial_loop,
                "\"action_type\"": "{dummy_value}",  # Provide the exact key the error asks for
                "json_structure_example": json_example_string
            }
            reformat_chain = ChatPromptTemplate.from_template(
                PROMPT_REFORMAT_TO_ACTION_JSON) | action_analysis_model | StrOutputParser()

            reformatted_llm_output_text = await asyncio.to_thread(self._call_llm_with_timing, reformat_chain,
                                                                  reformat_prompt_input, action_timing_data,
                                                                  priority=ELP0)

            if reformatted_llm_output_text and not (isinstance(reformatted_llm_output_text,
                                                               str) and "ERROR" in reformatted_llm_output_text.upper()):
                logger.info(f"{log_prefix} Received reformatted output from LLM. Parsing with fix retries...")
                json_candidate_from_reformat = self._extract_json_candidate_string(reformatted_llm_output_text,
                                                                                   log_prefix + "-ReformatExtract")
                if json_candidate_from_reformat:
                    parsed_action_json = self._programmatic_json_parse_and_fix(json_candidate_from_reformat,
                                                                               JSON_FIX_RETRY_ATTEMPTS_AFTER_REFORMAT,
                                                                               log_prefix + "-ReformatFix")
                    if parsed_action_json and isinstance(parsed_action_json,
                                                         dict) and "action_type" in parsed_action_json and "parameters" in parsed_action_json:
                        action_type = parsed_action_json.get("action_type")
                        logger.info(f"✅ {log_prefix} Reformatted Action analysis successful: Type='{action_type}'")
                        return parsed_action_json if action_type != "no_action" else None
                else:
                    logger.error(
                        f"{log_prefix} Failed to extract any JSON from LLM's reformat attempt. Output: {reformatted_llm_output_text[:200]}")
            else:
                logger.error(
                    f"{log_prefix} LLM re-request for JSON formatting failed or returned error: {reformatted_llm_output_text}")

        logger.warning(
            f"{log_prefix} All action analysis methods failed. Falling back to keyword-based default action for: '{user_input[:100]}'")
        words = re.findall(r'\b\w+\b', user_input.lower())
        stop_words = {"the", "is", "a", "to", "and", "what", "how", "who", "please", "can", "you", "tell", "me",
                      "about", "of", "for", "in", "on", "at", "an", "i", "my", "me"}
        keywords = [word for word in words if len(word) > 2 and word not in stop_words]
        fallback_params = {"original_query": user_input[:200], "extracted_keywords": list(set(keywords))[:7]}
        logger.info(f"{log_prefix} Extracted keywords for fallback: {fallback_params['extracted_keywords']}")
        final_fallback_action = {"action_type": "keyword_based_response_fallback", "parameters": fallback_params,
                                 "explanation": "Automated action analysis failed after multiple attempts. Using keyword-based fallback for general response or search."}

        try:
            await asyncio.to_thread(add_interaction, db, session_id=session_id, mode="chat",
                                    input_type="log_warning",
                                    user_input=f"[Action Analysis Fallback for: {user_input[:100]}]",
                                    llm_response=json.dumps(final_fallback_action),
                                    assistant_action_analysis_json=json.dumps(final_fallback_action),
                                    assistant_action_type=final_fallback_action["action_type"])
            await asyncio.to_thread(db.commit)
        except Exception as db_log_fallback_err:
            logger.error(f"{log_prefix} Failed to log keyword fallback action: {db_log_fallback_err}")
            await asyncio.to_thread(db.rollback)

        return final_fallback_action

    # --- NEW HELPER: Translation ---
    # AdelaideAlbertCortex -> Inside CortexThoughts class

    # --- NEW HELPER: Translation ---
    async def _translate(self, text: str, target_lang: str, source_lang: str = "auto") -> str:
        """
        Translates text using the configured translator model (ELP0).
        Handles TaskInterruptedException by re-raising it. Returns original text on other errors.
        """
        log_prefix = f"🌐 Translate|ELP0" # Add ELP0 marker
        # Session ID might not be directly available here unless passed in, using generic log for now
        # log_prefix = f"🌐 Translate|ELP0|{self.current_session_id}" if self.current_session_id else "🌐 Translate|ELP0"

        # Get the translator model instance
        translator_model = self.provider.get_model("translator")
        if not translator_model:
            logger.error(f"{log_prefix}: Translator model not available, cannot translate.")
            # Silently return original text, assuming caller handles this possibility
            return text

        logger.debug(f"{log_prefix}: Translating from '{source_lang}' to '{target_lang}': '{text[:50]}...'")
        try:
            # Prepare the prompt for the translation model
            prompt = f"Translate the following text from {source_lang} to {target_lang}:\n\n{text}"

            # --- Invoke the translation model using the timing helper with ELP0 ---
            # Prepare dummy interaction data if needed by _call_llm_with_timing for logging session
            timing_data = {"session_id": self.current_session_id, "mode": "chat"}
            message_result = await asyncio.to_thread(
                self._call_llm_with_timing, # Use the modified helper
                translator_model,           # Pass the model directly (assuming it's callable like a chain)
                prompt,                     # Input is the prompt string
                timing_data,
                priority=ELP0               # Set ELP0 priority
            )
            # ---

            # Check if the result is a message object and extract content
            # (This logic handles different ways Langchain models might return results)
            translated_text = None
            if hasattr(message_result, 'content') and isinstance(message_result.content, str):
                translated_text = message_result.content
            elif isinstance(message_result, str): # Handle if model directly returns a string
                translated_text = message_result
            else:
                # Handle unexpected return types from the translation model
                logger.error(f"{log_prefix}: Translation model returned unexpected type: {type(message_result)}. Full result: {message_result}")
                # Attempt to log this issue to the database if possible
                try:
                    db_session = SessionLocal()
                    add_interaction(db_session, session_id=self.current_session_id, mode="chat", input_type="log_error",
                                    user_input="[Translation Type Error]",
                                    llm_response=f"Unexpected type: {type(message_result)}. Data: {str(message_result)[:500]}")
                    db_session.close()
                except Exception as db_err:
                     logger.error(f"{log_prefix} Failed log translation type error: {db_err}")
                return text # Return original text on unexpected type error

            # Clean the extracted translated text
            cleaned_text = translated_text.strip() if translated_text else ""

            logger.debug(f"{log_prefix}: Translation result: '{cleaned_text[:50]}...'")
            # Return the successfully translated and cleaned text
            return cleaned_text

        except TaskInterruptedException as tie:
            # Specific handling for interruption caught by _call_llm_with_timing
            logger.warning(f"🚦 {log_prefix}: Translation INTERRUPTED: {tie}")
            # Re-raise the exception to be handled by the calling function
            raise tie

        except Exception as e:
            # Handle any other exceptions during translation
            logger.error(f"{log_prefix}: Translation failed: {e}")
            logger.exception(f"{log_prefix} Translate Traceback:") # Add traceback log
            # Attempt to log this failure to the database
            try:
                db_session = SessionLocal()
                add_interaction(db_session, session_id=self.current_session_id, mode="chat", input_type="log_error",
                                user_input="[Translation Failed]",
                                llm_response=f"Error: {e}. Original text: {text[:200]}")
                db_session.close()
            except Exception as db_err:
                 logger.error(f"{log_prefix} Failed log translation failure: {db_err}")
            # Return the original text as a fallback on error
            return text

    # --- NEW HELPER: Routing ---
    async def _route_to_specialist(self, db: Session, session_id: str, user_input_for_routing: str,
                                   prompt_input_for_router: Dict[str, Any]  # This now contains all contexts
                                   ) -> Tuple[str, str, str]:  # (chosen_model, refined_query, reasoning)

        log_prefix = f"🧠 Route|ELP0|{session_id}"  # Router typically runs at ELP0
        logger.info(f"{log_prefix} Routing request for: '{user_input_for_routing[:50]}...'")

        router_model = self.provider.get_model("router")
        default_model_key = "general"
        default_reason = "Fell back to default model after routing/parsing issues."

        if not router_model:
            logger.error(f"{log_prefix}: Router model ('router') not available! Defaulting to '{default_model_key}'.")
            # Log this failure to DB
            await asyncio.to_thread(add_interaction, db, session_id=session_id, mode="chat", input_type="log_error",
                                    user_input="[Router Model Unavailable]",
                                    llm_response=f"Router model key 'router' not configured. Defaulting to {default_model_key}.")
            await asyncio.to_thread(db.commit)
            return default_model_key, user_input_for_routing, "Router model unavailable, using default."

        router_chain_raw_output = (
                ChatPromptTemplate.from_template(PROMPT_ROUTER)  # PROMPT_ROUTER from CortexConfiguration
                | router_model
                | StrOutputParser()
        )
        router_timing_data = {"session_id": session_id, "mode": "chat", "execution_time_ms": 0}

        last_error_from_initial_loop: Optional[Exception] = None
        raw_llm_output_from_initial_loop: str = "Initial router LLM call did not yield parsable JSON."
        parsed_routing_json: Optional[Dict[str, Any]] = None

        # --- Stage 1: Initial LLM Calls and Parsing Attempts ---
        for attempt in range(DEEP_THOUGHT_RETRY_ATTEMPTS):
            current_attempt_num = attempt + 1
            logger.debug(f"{log_prefix} Router LLM call attempt {current_attempt_num}/{DEEP_THOUGHT_RETRY_ATTEMPTS}")
            raw_llm_text_this_attempt = ""
            try:
                raw_llm_text_this_attempt = await asyncio.to_thread(
                    self._call_llm_with_timing, router_chain_raw_output, prompt_input_for_router,
                    router_timing_data, priority=ELP0
                )
                raw_llm_output_from_initial_loop = raw_llm_text_this_attempt
                logger.trace(
                    f"{log_prefix} Raw LLM Router Output (Attempt {current_attempt_num}): '{raw_llm_text_this_attempt[:200]}...'")

                json_candidate_str = self._extract_json_candidate_string(raw_llm_text_this_attempt,
                                                                         log_prefix + "-ExtractInitial")
                if json_candidate_str:
                    # Try parsing with limited fixes (e.g., 1 attempt for this initial loop)
                    parsed_routing_json = self._programmatic_json_parse_and_fix(
                        json_candidate_str, 1, log_prefix + f"-InitialFixAttempt{current_attempt_num}"
                    )
                    if parsed_routing_json and isinstance(parsed_routing_json, dict) and \
                            all(k in parsed_routing_json for k in ["chosen_model", "refined_query", "reasoning"]):
                        chosen_model = str(parsed_routing_json["chosen_model"])
                        refined_query = str(parsed_routing_json["refined_query"])
                        reasoning = str(parsed_routing_json.get("reasoning", "N/A"))
                        # Basic validation of chosen_model key could be added here
                        logger.info(
                            f"✅ {log_prefix} Router successful (Attempt {current_attempt_num}): Chose '{chosen_model}'. Reason: {reasoning[:50]}...")
                        return chosen_model, refined_query, reasoning
                else:
                    last_error_from_initial_loop = ValueError(
                        f"No JSON candidate extracted from LLM router output: {raw_llm_text_this_attempt[:100]}")

            except TaskInterruptedException as tie:
                raise tie  # Propagate immediately
            except Exception as e_initial_route:
                last_error_from_initial_loop = e_initial_route

            logger.warning(
                f"⚠️ {log_prefix} Router LLM/parse attempt {current_attempt_num} failed. Error: {last_error_from_initial_loop}")
            if current_attempt_num < DEEP_THOUGHT_RETRY_ATTEMPTS: await asyncio.sleep(0.5 + attempt * 0.5)

        # --- Stage 2: LLM Re-request for Formatting (if initial attempts failed) ---
        if not parsed_routing_json:
            logger.warning(
                f"{log_prefix} Initial routing attempts failed. Trying LLM re-request to fix format. Last raw output: '{raw_llm_output_from_initial_loop[:200]}...'")

        reformat_prompt_input = {
            "faulty_llm_output_for_reformat": raw_llm_output_from_initial_loop,
            'chosen_model': "general",
            "original_user_input_placeholder": user_input_for_routing  # For the fallback in the reformat prompt
        }
        reformat_chain = ChatPromptTemplate.from_template(
            PROMPT_REFORMAT_TO_ROUTER_JSON) | router_model | StrOutputParser()  # New Prompt

        reformatted_llm_output_text = await asyncio.to_thread(
            self._call_llm_with_timing, reformat_chain, reformat_prompt_input,
            router_timing_data, priority=ELP0
        )

        if reformatted_llm_output_text and not (
                isinstance(reformatted_llm_output_text, str) and "ERROR" in reformatted_llm_output_text.upper()):
            logger.info(f"{log_prefix} Received reformatted output from LLM for router. Attempting to parse/fix...")
            json_candidate_from_reformat = self._extract_json_candidate_string(reformatted_llm_output_text,
                                                                                log_prefix + "-ReformatExtract")
            if json_candidate_from_reformat:
                # --- Stage 3: Parse/Fix Reformatted Output ---
                parsed_routing_json = self._programmatic_json_parse_and_fix(
                    json_candidate_from_reformat,
                    JSON_FIX_RETRY_ATTEMPTS_AFTER_REFORMAT,  # from CortexConfiguration
                    log_prefix + "-ReformatFix"
                )
                if parsed_routing_json and isinstance(parsed_routing_json, dict) and \
                        all(k in parsed_routing_json for k in ["chosen_model", "refined_query", "reasoning"]):
                    chosen_model = str(parsed_routing_json["chosen_model"])
                    refined_query = str(parsed_routing_json["refined_query"])
                    reasoning = str(parsed_routing_json.get("reasoning", "N/A (reformatted)"))
                    logger.info(f"✅ {log_prefix} Reformatted Routing analysis successful: Chose '{chosen_model}'.")
                    return chosen_model, refined_query, reasoning
            else:
                logger.error(
                    f"{log_prefix} Failed to extract any JSON from LLM's reformat (router) attempt. Output: {reformatted_llm_output_text[:200]}")
        else:
            logger.error(
                f"{log_prefix} LLM re-request for router JSON formatting failed or returned error: {reformatted_llm_output_text}")

        # --- Fallback if all methods failed ---
        logger.error(
            f"{log_prefix} ❌ All routing methods failed. Defaulting to '{default_model_key}'. Last error: {last_error_from_initial_loop}. Last raw output: '{raw_llm_output_from_initial_loop[:200]}...'")
        await asyncio.to_thread(add_interaction, db, session_id=session_id, mode="chat", input_type="log_error",
                                user_input=f"[Router Failed for: {user_input_for_routing[:100]}]",
                                llm_response=f"All routing attempts failed. Defaulting. Last raw: {raw_llm_output_from_initial_loop[:500]}")
        await asyncio.to_thread(db.commit)
        return default_model_key, user_input_for_routing, default_reason

    # --- generate method ---
    # AdelaideAlbertCortex -> Inside CortexThoughts class

    # --- generate (Main Async Method - Fuzzy History RAG + Direct History + Log Context + Multi-LLM Routing + VLM Preprocessing) ---
    # AdelaideAlbertCortex -> Inside CortexThoughts class

    def _sanitize_context_string(self, context_str: str) -> str:
        """A final, robust filter to remove any system-internal text from a context block."""
        if not context_str:
            return ""

        # Patterns for system logs, errors, and internal monologue markers
        patterns_to_remove = [
            r'\[Router Failed for:.*?\]',
            r'\[Action Analysis Fallback for:.*?\]',
            r'\[Self-Reflection Result for.*?\]',
            r'User: Imagination decision analysis for:.*',
            r'LLM Chain Error \(ELP\d\):.*',
            r'--- Reflection Task \(ID:.*?\):.*'
        ]

        sanitized_str = context_str
        for pattern in patterns_to_remove:
            sanitized_str = re.sub(pattern, '', sanitized_str, flags=re.DOTALL | re.IGNORECASE)

        # Also remove empty lines that might result from the substitution
        sanitized_str = "\n".join(line for line in sanitized_str.splitlines() if line.strip())

        return sanitized_str if sanitized_str else "No relevant context found."

    async def _extract_narrative_anchors(self, db: Session, user_input: str, priority: int = ELP0) -> str:
        """Uses an LLM to identify the core questions/anchors of the user's prompt."""
        log_prefix = f"AnchorExtractor|{self.current_session_id}"
        summarizer_model = self.provider.get_model("general_fast")
        if not summarizer_model: return "Could not determine anchors."

        chain = ChatPromptTemplate.from_template(
            PROMPT_NARRATIVE_ANCHOR_EXTRACTION) | summarizer_model | StrOutputParser()
        try:
            timing_data = {"session_id": self.current_session_id, "mode": "chat_helper"}
            anchors = await asyncio.to_thread(
                self._call_llm_with_timing, chain, {"user_input": user_input}, timing_data, priority=priority
            )
            logger.info(f"{log_prefix} Extracted Narrative Anchors:\n{anchors}")
            return anchors.strip()
        except Exception as e:
            logger.error(f"{log_prefix} Failed to extract narrative anchors: {e}")
            return "Focus on the user's primary request."

    async def _generate_progression_summary(self, db: Session, full_response_so_far: str, last_chunk: str, priority: int = ELP0) -> str:
        """Uses an LLM to summarize progress and suggest the next step."""
        log_prefix = f"ProgressionSummarizer|{self.current_session_id}"
        if not last_chunk: return "Begin the response."

        summarizer_model = self.provider.get_model("general_fast")
        if not summarizer_model: return "Continue the previous thought."

        chain = ChatPromptTemplate.from_template(PROMPT_PROGRESSION_SUMMARY) | summarizer_model | StrOutputParser()
        try:
            timing_data = {"session_id": self.current_session_id, "mode": "chat_helper"}
            summary = await asyncio.to_thread(
                self._call_llm_with_timing,
                chain,
                {"full_response_so_far": full_response_so_far, "last_chunk": last_chunk},
                timing_data,
                priority=priority
            )
            cleaned_summary = summary.strip().replace("\n", " ")
            logger.info(f"{log_prefix} Generated Progression Summary: '{cleaned_summary}'")
            return cleaned_summary
        except Exception as e:
            logger.error(f"{log_prefix} Failed to generate progression summary: {e}")
            return "Continue building on the last paragraph logically."

    def _extract_themes_from_think_block(self, think_content: str) -> List[str]:
        """Parses a <think> block to identify key themes mentioned."""
        themes = []
        # A simple regex to find words or phrases after "themes:" or "theme:"
        match = re.search(r'themes?:(.*?)(?:\n|$)', think_content, re.IGNORECASE)
        if match:
            theme_str = match.group(1).strip()
            # Split by comma or semicolon and clean up
            themes = [theme.strip() for theme in re.split(r'[,;]', theme_str) if theme.strip()]
        return themes

    async def _generate_topic_summary(self, db: Session, purified_history_interactions: List[Interaction], priority: int = ELP0) -> str:
        """
        Uses a fast LLM to generate a clean, one-sentence topic summary of a conversation.
        This acts as the "master topic driver" for the main generation prompt.
        This is the complete, non-abbreviated implementation.
        """
        # Create a specific log prefix for this operation for easy debugging and tracing.
        log_prefix = f"TopicDriver|{self.current_session_id}"
        logger.info(f"{log_prefix} Generating master topic summary...")

        # --- Graceful Handling of an Empty Conversation ---
        # If this is the very first turn, there's no history to summarize.
        # We return a clear, default state instead of making an unnecessary LLM call.
        if not purified_history_interactions:
            logger.debug(f"{log_prefix} No prior conversation history found. Using default initial summary.")
            return "The user has just initiated the conversation."

        # --- Prepare the Input for the Summarizer Model ---
        # We use the existing helper to format the purified history into a simple, readable text block.
        history_text = self._format_direct_history(purified_history_interactions)

        # This dictionary will be used to fill the placeholder in the prompt template.
        prompt_input = {"conversation_history": history_text}

        # --- Select a Fast, Efficient Model for this Simple Task ---
        # This is a perfect use case for the 'general_fast' model role.
        summarizer_model = self.provider.get_model("general_fast")

        # Gracefully handle the case where the model might not be configured.
        if not summarizer_model:
            logger.warning(
                f"{log_prefix} Summarizer model ('general_fast') not available. Returning a fallback summary.")
            return "(Context summary generation failed due to missing model configuration)"

        # --- Build the LangChain Execution Chain ---
        # This chain will take the prompt, send it to the model, and parse the output as a string.
        chain = ChatPromptTemplate.from_template(PROMPT_TOPIC_SUMMARY) | summarizer_model | StrOutputParser()

        # --- Execute the LLM Call with Robust Error Handling ---
        try:
            # Since chain.invoke is a synchronous (blocking) call, we must run it in a separate
            # thread using asyncio.to_thread to prevent it from stalling the main server event loop.
            logger.debug(f"{log_prefix} Calling summarizer model...")
            timing_data = {"session_id": self.current_session_id, "mode": "chat_helper"}
            summary = await asyncio.to_thread(
                self._call_llm_with_timing, chain, prompt_input, timing_data, priority=priority
            )

            # Clean up the raw output from the model: remove leading/trailing whitespace and any extraneous newlines.
            cleaned_summary = summary.strip().replace("\n", " ")

            logger.info(f"{log_prefix} Successfully generated topic summary: '{cleaned_summary}'")
            return cleaned_summary

        except Exception as e:
            # If anything goes wrong during the LLM call (e.g., worker crash, timeout),
            # we log the error and return a safe, default string. This prevents the entire
            # direct_generate process from crashing due to a failure in this helper function.
            logger.error(f"{log_prefix} An exception occurred while generating the topic summary: {e}", exc_info=True)
            return "(Context summary generation failed due to a processing error)"

    def _parse_think_speak_output(self, raw_llm_output: str) -> Tuple[str, str]:
        """
        Parses the raw output from the LLM to separate the internal monologue (`<think>`)
        from the final, user-facing conversational response.

        This function enforces the strict "Think vs. Speak" protocol defined in the system prompt.
        It is designed to be robust against malformed or incomplete outputs from the model.

        Args:
            raw_llm_output: The complete, raw string generated by the LLM, potentially
                            containing both a <think> block and the final answer.

        Returns:
            A tuple containing two strings:
            - think_content (str): The full, extracted <think>...</think> block, including the tags.
                                   Returns an empty string if no valid think block is found.
            - speak_content (str): The cleaned, user-facing text that appeared after the </think> tag.
                                   If the format is not followed, this will be the cleaned version
                                   of the entire raw LLM output as a fallback.
        """
        log_prefix = f"ThinkSpeakParser|{self.current_session_id}"

        # Initialize return variables to safe defaults.
        think_content = ""
        speak_content = ""

        # Check for a complete <think>...</think> block. The closing tag is the most reliable indicator.
        if "</think>" in raw_llm_output:
            try:
                # Split the string only on the *first* occurrence of the closing tag.
                # This is a robust way to handle cases where the model might generate additional tags.
                parts = raw_llm_output.split("</think>", 1)

                # The first part is the content of the think block, plus the opening tag.
                # We re-append the closing tag to make the `think_content` a complete, well-formed block.
                think_content = parts[0] + "</think>"

                # The second part is everything that came after. This is the user-facing speech.
                # We must strip any leading/trailing whitespace or newlines.
                speak_content = parts[1].strip()

                logger.debug(f"{log_prefix} Successfully parsed Think/Speak format.")

            except Exception as e:
                # This is a safety net in case the split operation fails for an unknown reason.
                logger.error(f"{log_prefix} Unexpected error during Think/Speak parsing: {e}. Falling back.")
                # In case of an error, we revert to the fallback behavior.
                think_content = ""
                speak_content = self._cleanup_llm_output(raw_llm_output)

        else:
            # Fallback case: The model did not follow the protocol and did not include a valid </think> tag.
            # In this scenario, we assume the entire output was intended as the user-facing response.
            logger.warning(
                f"{log_prefix} Model did not follow Think/Speak format (no closing </think> tag found). Using entire output as speak content.")
            think_content = ""  # No valid think block was found.
            speak_content = self._cleanup_llm_output(raw_llm_output)

        # Return the separated parts. The calling function (`_direct_generate_logic`) will decide
        # what to do if `speak_content` is empty.
        return think_content, speak_content

    def _truncate_string_by_tokens(self, text: str, token_limit: int) -> str:
        """Accurately truncates a string to a maximum token limit using tiktoken."""
        if not text or token_limit <= 0:
            return ""

        if not TIKTOKEN_AVAILABLE_APP or not cl100k_base_encoder_app:
            # Fallback to character-based truncation if tiktoken is not available
            return text[:token_limit * 4]

        tokens = cl100k_base_encoder_app.encode(text)
        if len(tokens) <= token_limit:
            return text

        truncated_tokens = tokens[:token_limit]
        # .decode can sometimes fail with partial tokens at the end, so we use errors='ignore'
        return cl100k_base_encoder_app.decode(truncated_tokens, errors='ignore')

    def _build_rag_context_within_token_limit(self, all_rag_docs: List[Any], token_limit: int) -> str:
        """
        Builds the RAG context string by including whole documents from the start of the list
        until the specified token limit is reached.
        """
        if not all_rag_docs or token_limit <= 0:
            return "None."

        context_parts = []
        tokens_used = 0

        for doc in all_rag_docs:
            # Ensure we can get content from the document object
            content = getattr(doc, 'page_content', '')
            if not content:
                continue

            doc_tokens = self._count_tokens(content)

            # Check if adding this document would exceed the budget
            if tokens_used + doc_tokens > token_limit:
                logger.warning(
                    f"RAG context pruning: Reached token limit ({tokens_used}/{token_limit}). "
                    f"Stopping before adding next document ({doc_tokens} tokens)."
                )
                break  # Stop adding documents

            # Format the document for inclusion
            # (This is a simplified version of _format_docs for a single doc)
            formatted_doc = f"Source Chunk (RAG):\n{content}\n---\n"
            context_parts.append(formatted_doc)
            tokens_used += self._count_tokens(formatted_doc)  # Recount with formatting

        if not context_parts:
            return "No relevant context found within the token limit."

        return "".join(context_parts)

    # In AdelaideAlbertCortex.py
    # (Ensure all necessary imports like re, asyncio, ChatPromptTemplate, StrOutputParser are at the top)

    # ==============================================================================
    # PLACE THIS HELPER FUNCTION INSIDE YOUR `CortexThoughts` CLASS
    # It should be at the same indentation level as your other helper methods like _direct_generate_logic.
    # ==============================================================================

    async def _humanize_chunk(self, clean_chunk: str, priority: int = ELP0) -> str:
        """
        Uses a fast LLM to rewrite a chunk of text to sound more natural and less robotic.
        This includes introducing subtle imperfections like typos and inconsistent capitalization,
        making the AI's persona more believable. This is the complete, non-abbreviated implementation.
        """

        # --- Input Validation ---
        # If the chunk is very short, empty, or already contains system-like brackets,
        # it's not worth the overhead to humanize it. Return it as is.
        if not clean_chunk or len(clean_chunk.split()) < 3 or '[' in clean_chunk:
            return clean_chunk

        log_prefix = f"Humanizer|{self.current_session_id}"
        logger.info(f"{log_prefix} Humanizing chunk: '{clean_chunk[:50]}...'")

        # --- Hardcoded Prompt ---
        # As requested, the prompt is defined directly here to keep CortexConfiguration.py clean
        # and focused on high-level settings.
        humanizer_prompt_template = """You are a text humanizer. Your task is to rewrite the following "perfect" AI-generated text to make it sound more like a real person typed it quickly on a messaging app.

    **RULES:**
    1.  **Introduce Subtle Imperfections**: Add one or two very minor typos (e.g., "teh" for "the", "adn" for "and", or a single missed letter). Don't overdo it.
    2.  **Vary Capitalization**: Start some sentences with a lowercase letter. It's okay to not capitalize some proper nouns if it feels natural in a casual chat.
    3.  **Use Contractions**: Aggressively change "do not" to "don't", "it is" to "it's", "I am" to "I'm", etc.
    4.  **Slightly Rephrase for Flow**: Make minor changes to the phrasing to be more conversational and less formal, but YOU MUST NOT change the core meaning or add any new information.
    5.  **Keep it Concise**: If the original is wordy, shorten it slightly.
    6.  **Absolutely NO Emojis or Internet Slang**: Keep the tone authentic and grounded, not like a caricature.
    7.  Your final output must ONLY be the rewritten text. Do not include any explanations, headers, or the original text.

    **Perfect AI Text:**
    ---
    {text_to_humanize}
    ---

    **Humanized Version:**
    """

        # --- Model and Chain Setup ---
        # This task is perfect for a fast, instruction-following model.
        humanizer_model = self.provider.get_model("general_fast")
        if not humanizer_model:
            logger.warning(f"{log_prefix} Humanizer model ('general_fast') not available. Returning original chunk.")
            return clean_chunk

        chain = ChatPromptTemplate.from_template(humanizer_prompt_template) | humanizer_model | StrOutputParser()

        # --- LLM Execution with Error Handling ---
        try:
            # Since chain.invoke is synchronous, we run it in a thread to avoid blocking.
            timing_data = {"session_id": self.current_session_id, "mode": "chat_helper"}
            humanized_text = await asyncio.to_thread(
                self._call_llm_with_timing,
                chain,
                {"text_to_humanize": clean_chunk},
                timing_data,
                priority=priority
            )
            # --- Quality Gate ---
            # As a safety check, if the humanized text is wildly different in length (more than 50% different),
            # the model may have hallucinated or failed. In that case, it's safer to fall back to the original.
            if abs(len(humanized_text) - len(clean_chunk)) > (len(clean_chunk) * 0.5):
                logger.warning(
                    f"{log_prefix} Humanized text length is substantially different from original. Falling back to preserve meaning.")
                return clean_chunk

            # If the output is empty, it's a failure.
            if not humanized_text.strip():
                logger.warning(f"{log_prefix} Humanizer returned an empty string. Falling back to original chunk.")
                return clean_chunk

            logger.info(f"{log_prefix} Humanization successful.")
            return humanized_text.strip()

        except Exception as e:
            logger.error(f"{log_prefix} An exception occurred during the humanization LLM call: {e}", exc_info=True)
            # In case of any processing error, the safest action is always to return the
            # clean, original, and factually correct chunk.
            return clean_chunk

    async def _direct_generate_logic(self, db: Session, user_input: str, session_id: str,
                                     vlm_description: Optional[str] = None,
                                     image_b64: Optional[str] = None) -> str:
        """
        CORRECTED: The definitive ELP1 logic. It now uses a dedicated, lightweight RAG
        function to ensure it meets its performance benchmark.
        """
        direct_req_id = f"dgen-logic-direct-v5-{uuid.uuid4()}"
        log_prefix = f"⚡️ {direct_req_id}|ELP1"
        logger.info(f"{log_prefix} CORRECTED DIRECT V5 Logic START -> Session: {session_id}, Input: '{user_input[:50]}...'")
        direct_start_time = time.monotonic()
        self.current_session_id = session_id

        # --- 1. CONTEXT GATHERING (ELP1 Priority) ---
        logger.info(f"{log_prefix} Gathering RAG and direct history context with ELP1 priority.")

        # <<< MODIFICATION: Call the new, lightweight RAG helper >>>
        history_rag_str = await self._get_direct_rag_context_elp1(db, user_input, session_id)

        # Get the most recent, direct conversation turns for immediate context.
        direct_history_interactions = await asyncio.to_thread(get_global_recent_interactions, db, limit=5)
        recent_direct_history_str = self._format_direct_history(direct_history_interactions)

        # --- 2. SINGLE LLM GENERATION (ELP1 Priority) ---
        logger.info(f"{log_prefix} Preparing single, large LLM call with ELP1 priority.")
        fast_model = self.provider.get_model("general_fast")
        if not fast_model:
            raise RuntimeError("Fast model 'general_fast' for direct response is not configured.")

        max_response_tokens = LLAMA_CPP_N_CTX // 2

        prompt_placeholders = {
            "history_rag": history_rag_str,
            "recent_direct_history": recent_direct_history_str,
            "input": user_input,
        }

        bound_model = fast_model.bind(max_tokens=max_response_tokens, stop=[CHATML_END_TOKEN], priority=ELP1)
        chain = ChatPromptTemplate.from_template(PROMPT_DIRECT_GENERATE) | bound_model | StrOutputParser()

        timing_data = {"session_id": session_id, "mode": "chat_direct_elp1"}
        raw_llm_output = ""
        try:
            # We must use asyncio.to_thread here because _call_llm_with_timing is a synchronous function
            raw_llm_output = await asyncio.to_thread(
                self._call_llm_with_timing,
                chain,
                prompt_placeholders,
                timing_data,
                priority=ELP1
            )
        except TaskInterruptedException as tie:
            logger.error(f"🚦 {log_prefix} Direct generation LLM call was INTERRUPTED: {tie}")
            return f"[System Error: The response generation was interrupted by a higher priority request.]"

        # --- 3. POST-PROCESSING & FINALIZATION ---
        _think_content, speak_content = self._parse_think_speak_output(raw_llm_output)
        final_response_text = speak_content

        if not final_response_text.strip():
            logger.error(f"{log_prefix} Final response is empty after parsing. Model failed to generate speak content.")
            final_response_text = "[System Error: The AI model could not generate a valid response.]"

        if FUZZY_AVAILABLE and fuzz and user_input.strip():
            normalized_user = ''.join(filter(str.isalnum, user_input.lower()))
            normalized_response = ''.join(filter(str.isalnum, final_response_text[:len(user_input) + 20].lower()))
            if len(normalized_user) > 15 and normalized_response.startswith(normalized_user):
                logger.error(f"{log_prefix} Spit-back detected! Response starts with user input. Overriding.")
                final_response_text = "[System Error: The AI model repeated the input instead of providing a valid response.]"

        # --- 4. LOGGING and RETURN ---
        final_duration_ms = (time.monotonic() - direct_start_time) * 1000.0
        interaction_data = {
            "session_id": session_id, "mode": "chat", "input_type": "text", "user_input": user_input,
            "llm_response": final_response_text, "execution_time_ms": final_duration_ms,
            "classification": "direct_response_single_call_elp1",
        }
        queue_interaction_for_batch_logging(**interaction_data)

        logger.info(f"{log_prefix} CORRECTED DIRECT V5 Logic END. Duration: {final_duration_ms:.2f}ms")
        return final_response_text

    def _convert_interactions_to_chatml_turns(self, interactions: List[Interaction]) -> List[Dict[str, str]]:
        """Helper to convert a list of Interaction DB objects to the ChatML dictionary format."""
        turns = []
        sorted_interactions = sorted(interactions, key=lambda i: i.timestamp)
        for interaction in sorted_interactions:
            if interaction.input_type == 'text' and interaction.user_input:
                turns.append({"role": "user", "content": interaction.user_input})
            elif interaction.llm_response and interaction.input_type == 'llm_response':
                turns.append({"role": "assistant", "content": interaction.llm_response})
        return turns

    async def direct_generate(self, db: Session, user_input: str, session_id: str,
                              vlm_description: Optional[str] = None,
                              image_b64: Optional[str] = None) -> str:
        """
        High-level wrapper for ELP1 generation with a deterministic timeout watchdog.
        Triggers a system-wide halt if the benchmarked time is exceeded.
        """
        req_id = f"dgen-watchdog-{uuid.uuid4()}"
        log_prefix = f"⏱️ {req_id}|ELP1"

        # Check if benchmark has run. If not, run without the watchdog.
        if BENCHMARK_ELP1_TIME_MS <= 0:
            logger.warning(f"{log_prefix}: BENCHMARK_ELP1_TIME_MS not set. Running direct_generate without timeout watchdog.")
            return await self._direct_generate_logic(db, user_input, session_id, vlm_description, image_b64)

        timeout_event = asyncio.Event()
        task_done_event = asyncio.Event()
        timeout_duration_sec = BENCHMARK_ELP1_TIME_MS / 1000.0
        logger.info(f"{log_prefix}: Starting ELP1 task with a timeout of {timeout_duration_sec:.2f} seconds.")

        async def watchdog():
            try:
                # Wait for the main task to signal completion OR for the timeout to trigger
                await asyncio.wait_for(task_done_event.wait(), timeout=timeout_duration_sec)
                logger.debug(f"{log_prefix} Watchdog: Task completed in time. Exiting cleanly.")
            except asyncio.TimeoutError:
                logger.critical(f"!!!!!!!!!!!!!! ELP1 TIMEOUT !!!!!!!!!!!!!!")
                logger.critical(f"Task exceeded benchmark of {BENCHMARK_ELP1_TIME_MS:.2f} ms.")
                logger.critical("Your processor too slow! Management Failure! Discarding queue")
                
                # Signal that a timeout occurred
                timeout_event.set()

                # Trigger the system-wide kill switch
                if cortex_backbone_provider:
                    # Run the blocking kill function in a separate thread to not block the watchdog
                    await asyncio.to_thread(cortex_backbone_provider.kill_all_workers, "ELP1 performance timeout")
                else:
                    logger.error("WATCHDOG: cortex_backbone_provider not available to kill workers!")

        # Start the watchdog as a background task
        watchdog_task = asyncio.create_task(watchdog())

        try:
            # Run the actual generation logic
            result = await self._direct_generate_logic(db, user_input, session_id, vlm_description, image_b64)
            
            # If the task finishes, signal the watchdog and wait for it to exit
            if not timeout_event.is_set():
                task_done_event.set()
                await watchdog_task
                return result
            else:
                # This case is unlikely but possible if the task finishes right as the timeout hits
                logger.warning(f"{log_prefix}: Task finished, but timeout event was already set. A kill signal may have been sent.")
                raise TaskInterruptedException("Processing was terminated due to a performance timeout.")

        except Exception as e:
            # If the main task fails for any other reason, ensure the watchdog is cleaned up
            if not task_done_event.is_set():
                task_done_event.set()
            await watchdog_task # Wait for watchdog to finish
            raise e # Re-raise the original error

    async def _get_vector_search_file_index_context(self, query: str, session_id_for_log: str, priority: int = ELP0,
                                                    stop_event_param: Optional[threading.Event] = None) -> str:
        """
        Performs a vector similarity search on the global file index vector store.
        If no vector results are found, attempts a fuzzy search on the SQL FileIndex table as a fallback.
        Formats the results. Explicitly uses _embed_texts for prioritized query embedding.

        MODIFIED: Now accepts session_id_for_log to prevent using the wrong session ID from self.current_session_id during background tasks.
        """
        log_prefix = f"🔍 FileVecSearch|ELP{priority}|{session_id_for_log or 'NoSession'}"
        logger.debug(f"{log_prefix} Attempting file search for query: '{query[:50]}...'")

        global_file_vs = get_global_file_index_vectorstore()  # Synchronous call

        # --- Vector Search Attempt ---
        vector_search_succeeded = False
        search_results_docs: List[Any] = []  # Will hold Langchain Document objects

        if not global_file_vs:
            logger.warning(f"{log_prefix} Global file index vector store not available for vector search.")
        elif not self.provider or not self.provider.embeddings:
            logger.error(f"{log_prefix} Embeddings provider not available for vector search query.")
        elif not query:
            logger.debug(f"{log_prefix} Empty query for vector search. Skipping vector part.")
        else:
            query_vector: Optional[List[float]] = None
            try:
                logger.debug(f"{log_prefix} Explicitly embedding query via _embed_texts with priority ELP{priority}...")
                if hasattr(self.provider.embeddings, '_embed_texts') and \
                        callable(getattr(self.provider.embeddings, '_embed_texts')):
                    embedding_result_list = await asyncio.to_thread(
                        self.provider.embeddings._embed_texts, [query], priority=priority  # type: ignore
                    )
                    if embedding_result_list and len(embedding_result_list) > 0:
                        query_vector = embedding_result_list[0]
                    else:
                        logger.error(f"{log_prefix} _embed_texts returned None or empty list for query.")
                else:
                    logger.error(
                        f"{log_prefix} Embeddings object missing '_embed_texts'. Cannot perform prioritized query embedding.")

                if not query_vector:
                    logger.error(f"{log_prefix} Failed to embed query for vector search (query_vector is None).")
                else:
                    logger.debug(
                        f"{log_prefix} Query embedded. Performing similarity_search_by_vector (k={RAG_FILE_INDEX_COUNT})...")
                    # Perform search using the pre-computed vector
                    search_results_docs = await asyncio.to_thread(
                        global_file_vs.similarity_search_by_vector,
                        embedding=query_vector,
                        k=RAG_FILE_INDEX_COUNT  # from CortexConfiguration
                    )
                    if search_results_docs:
                        vector_search_succeeded = True
                        logger.info(f"{log_prefix} Found {len(search_results_docs)} results from VECTOR file search.")
                    else:
                        logger.info(f"{log_prefix} No results from VECTOR file search for query '{query[:50]}...'")

            except TaskInterruptedException as tie:
                logger.warning(f"🚦 {log_prefix} Vector file search INTERRUPTED: {tie}")
                raise  # Re-raise to be handled by the caller
            except Exception as e:
                logger.error(f"❌ {log_prefix} Error during vector file search: {e}")
                logger.exception(f"{log_prefix} Vector File Search Traceback:")
                # Continue to fuzzy search fallback

        # --- Fuzzy Search Fallback ---
        fuzzy_search_results_text_list: List[str] = []
        if not vector_search_succeeded:
            if not FUZZY_AVAILABLE:
                logger.warning(
                    f"{log_prefix} Vector search failed and Fuzzy search (thefuzz) is not available. No file context.")
                return "No relevant file content found (vector search failed, fuzzy search unavailable)."

            logger.info(
                f"{log_prefix} Vector search yielded no results. Attempting FUZZY search fallback for query: '{query[:50]}...'")
            db_for_fuzzy: Optional[Session] = None
            try:
                db_for_fuzzy = SessionLocal()  # type: ignore
                if not db_for_fuzzy: raise RuntimeError("Failed to get DB session for fuzzy search.")

                # Fetch a reasonable number of candidates from SQL to perform fuzzy search on
                # Limiting this to avoid loading too much into memory.
                # We search against file_name and indexed_content (if not too long).
                # Order by last_modified_os to potentially get more relevant recent files.
                candidate_records = db_for_fuzzy.query(FileIndex).filter(
                    FileIndex.index_status.in_(['indexed_text', 'success', 'partial_vlm_error'])
                    # Only search indexed files
                ).order_by(desc(FileIndex.last_modified_os)).limit(500).all()  # Limit candidates

                if not candidate_records:
                    logger.info(f"{log_prefix} FUZZY: No candidate records in SQL DB for fuzzy search.")
                else:
                    logger.debug(f"{log_prefix} FUZZY: Found {len(candidate_records)} candidate records from SQL.")
                    fuzzy_matches: List[Tuple[FileIndex, int]] = []  # Store (record, score)

                    for record in candidate_records:
                        if stop_event_param and stop_event_param.is_set():  # Check if passed and set
                            logger.info(f"{log_prefix} FUZZY search interrupted by stop_event_param.")
                            break
                        # Text to search against: filename + content snippet
                        text_to_match_on = record.file_name or ""
                        if record.indexed_content:
                            # Use a snippet of content to keep fuzzy search performant
                            content_snippet = (record.indexed_content[:500] + "...") if len(
                                record.indexed_content) > 500 else record.indexed_content
                            text_to_match_on += " " + content_snippet

                        if not text_to_match_on.strip(): continue

                        # Use fuzz.partial_ratio for substring matching, good for finding queries within larger text
                        score = fuzz.partial_ratio(query.lower(), text_to_match_on.lower())

                        if score >= FUZZY_SEARCH_THRESHOLD_APP:  # FUZZY_SEARCH_THRESHOLD_APP from AdelaideAlbertCortex/config
                            fuzzy_matches.append((record, score))

                    if fuzzy_matches:
                        # Sort by score descending, then by last_modified_os descending
                        fuzzy_matches.sort(key=lambda x: (x[1], x[0].last_modified_os or datetime.datetime.min),
                                           reverse=True)
                        top_fuzzy_matches = fuzzy_matches[:RAG_FILE_INDEX_COUNT]  # Take top N
                        logger.info(
                            f"{log_prefix} FUZZY: Found {len(top_fuzzy_matches)} matches with score >= {FUZZY_SEARCH_THRESHOLD_APP}.")

                        for i, (record, score) in enumerate(top_fuzzy_matches):
                            content_snippet = (record.indexed_content[:300] + "...") if record.indexed_content and len(
                                record.indexed_content) > 300 else (record.indexed_content or "[No content]")
                            entry = (
                                f"--- Fuzzy File Result {i + 1} (Score: {score}) ---\n"
                                f"File: {record.file_name}\nPath Hint: ...{record.file_path[-70:]}\nModified: {record.last_modified_os.strftime('%Y-%m-%d %H:%M') if record.last_modified_os else 'N/A'}\n"
                                f"Content Snippet: {content_snippet}\n---\n"
                            )
                            fuzzy_search_results_text_list.append(entry)
                    else:
                        logger.info(
                            f"{log_prefix} FUZZY: No matches found above threshold {FUZZY_SEARCH_THRESHOLD_APP}.")

            except Exception as e_fuzzy:
                logger.error(f"❌ {log_prefix} Error during FUZZY search: {e_fuzzy}")
                logger.exception(f"{log_prefix} Fuzzy Search Traceback:")
                fuzzy_search_results_text_list.append(
                    f"[Error performing fuzzy file search: {type(e_fuzzy).__name__}]\n")
            finally:
                if db_for_fuzzy: db_for_fuzzy.close()

        # --- Format Results ---
        if vector_search_succeeded and search_results_docs:
            context_parts = []
            max_snippet_len = 300
            max_total_chars = 2000  # Max length for combined vector context
            current_chars = 0
            for i, doc in enumerate(search_results_docs):
                if not hasattr(doc, 'page_content') or not hasattr(doc, 'metadata'):
                    logger.warning(f"{log_prefix} Skipping malformed vector document: {doc}")
                    continue
                content = doc.page_content
                metadata = doc.metadata
                file_path = metadata.get("source", "UnkPath")
                file_name = metadata.get("file_name",
                                         os.path.basename(file_path) if file_path != "UnkPath" else "UnkFile")
                last_mod = metadata.get("last_modified", "UnkDate")
                # Langchain Chroma typically returns relevance_score which is distance (lower is better).
                # We can invert it or just display as is.
                relevance_score = doc.metadata.get('relevance_score', 'N/A') if isinstance(doc.metadata,
                                                                                           dict) else 'N/A'

                snippet = content[:max_snippet_len] + ("..." if len(content) > max_snippet_len else "")
                entry = (
                    f"--- Vector File Result {i + 1} (Score: {relevance_score}) ---\n"  # Score might be distance
                    f"File: {file_name}\nPath Hint: ...{file_path[-70:]}\nModified: {last_mod}\n"
                    f"Content Snippet: {snippet}\n---\n")
                if current_chars + len(entry) > max_total_chars:
                    context_parts.append("[Vector file search context truncated due to length]...\n")
                    break
                context_parts.append(entry)
                current_chars += len(entry)
            return "".join(context_parts) if context_parts else "No relevant file content found via vector search."
        elif fuzzy_search_results_text_list:
            # Combine fuzzy results, already formatted as text strings
            # Limit total length of fuzzy results string for the prompt
            combined_fuzzy_text = "".join(fuzzy_search_results_text_list)
            max_fuzzy_chars = 2000  # Max length for combined fuzzy context
            if len(combined_fuzzy_text) > max_fuzzy_chars:
                return combined_fuzzy_text[
                       :max_fuzzy_chars] + "\n[Fuzzy file search context truncated due to length]...\n"
            return combined_fuzzy_text
        else:
            # Neither vector nor fuzzy search yielded results
            return "No relevant file content found via vector or fuzzy search for the query."

    async def _extract_text_with_ocr_async(self, image_bytes: bytes) -> str:
        """
        Performs OCR on the given image bytes and returns all detected text as a single string.
        """
        log_prefix = f"👁️ OCR|{self.current_session_id}"

        if not ocr_reader:
            logger.warning(f"{log_prefix}: EasyOCR reader not available. Skipping OCR.")
            return "OCR processing is not available."

        try:
            logger.info(f"{log_prefix}: Starting OCR text extraction...")
            # EasyOCR's readtext is a blocking, CPU/GPU-bound operation. Run in a thread.
            results = await asyncio.to_thread(ocr_reader.readtext, image_bytes)

            if not results:
                logger.info(f"{log_prefix}: No text detected by OCR.")
                return "No text detected."

            # Combine all detected text snippets into a single block for the VLM.
            # We only care about the text content for this purpose.
            detected_texts = [text for (bbox, text, prob) in results]
            combined_text = "\n".join(detected_texts)

            logger.info(f"{log_prefix}: OCR successful. Found {len(detected_texts)} text fragments.")
            logger.trace(f"{log_prefix}: OCR Raw Text -> {combined_text}")

            return combined_text

        except Exception as e:
            logger.error(f"{log_prefix}: An error occurred during OCR processing: {e}", exc_info=True)
            return f"An error occurred during OCR processing: {e}"

    async def _describe_image_async(self, db: Session, session_id: str, image_b64: str,
                                    prompt_type: str = "initial_description", priority: int = ELP0,
                                    is_avif: bool = False) -> Tuple[Optional[str], Optional[str]]:
        """
        Generates a comprehensive image description using a two-stage OCR + VLM pipeline.

        This function orchestrates the entire image analysis process. It first prepares the
        image data, handling AVIF to PNG conversion if necessary. It then runs OCR to
        extract any text. Finally, it provides both the image and the extracted text to a
        Vision-Language Model (VLM) with a specialized prompt, instructing it to generate
        a description that is visually accurate and grounded by the OCR text.

        Args:
            db: The active SQLAlchemy database session.
            session_id: The session ID for the current interaction.
            image_b64: The base64 encoded string of the image (can be PNG or AVIF).
            prompt_type: A string to identify the context of the call (e.g., "initial_description").
            priority: The execution priority for the LLM call (ELP0 or ELP1).
            is_avif: A boolean flag indicating if the provided `image_b64` is in AVIF format.

        Returns:
            A tuple containing:
            - The final description string if successful, otherwise None.
            - An error message string if any part of the process fails, otherwise None.
        """
        req_id = f"vlm_desc-{uuid.uuid4()}"
        log_prefix = f"🖼️ {req_id}|ELP{priority}"
        logger.info(f"{log_prefix} Requesting augmented VLM description (type: {prompt_type}, is_avif: {is_avif}).")

        # --- Get VLM model ---
        vlm_model = self.provider.get_model("vlm")
        if vlm_model is None:
            error_msg = f"VLM model not available for image description."
            logger.error(f"❌ {log_prefix}: {error_msg}")
            try:
                add_interaction(db, session_id=session_id, mode="chat", input_type="log_error",
                                user_input=f"[VLM Desc Failed - Model Unavailable - {prompt_type}]",
                                llm_response=error_msg)
            except Exception as db_log_err:
                logger.error(f"Failed to log VLM unavailable error: {db_log_err}")
            return None, error_msg

        try:
            # --- 1. Prepare Image Bytes (Handle AVIF -> PNG) ---
            image_bytes_for_processing: Optional[bytes] = None
            png_b64_for_vlm: str = ""

            if is_avif:
                logger.info(f"{log_prefix} Input is AVIF, converting to PNG for processing...")
                converted_png_b64 = await asyncio.to_thread(self._convert_avif_to_png_b64, image_b64)
                if not converted_png_b64:
                    error_msg = "Failed to convert stored AVIF image to PNG for VLM analysis."
                    logger.error(f"{log_prefix} {error_msg}")
                    return None, error_msg
                png_b64_for_vlm = converted_png_b64
                logger.info(f"{log_prefix} AVIF to PNG conversion successful.")
            else:
                # The input is already in a VLM-compatible format (e.g., PNG from user upload)
                png_b64_for_vlm = image_b64

            # Decode the final PNG base64 to bytes for the OCR step
            image_bytes_for_processing = base64.b64decode(png_b64_for_vlm)

            # --- 2. Stage 1: Run OCR ---
            extracted_ocr_text = await self._extract_text_with_ocr_async(image_bytes_for_processing)

            # --- 3. Stage 2: Call VLM with Augmented Prompt ---
            logger.info(f"{log_prefix} Calling VLM with image and OCR data...")

            image_uri = f"data:image/png;base64,{png_b64_for_vlm}"

            # Use the augmented prompt from CortexConfiguration.py
            vlm_prompt_text = PROMPT_VLM_AUGMENTED_ANALYSIS.format(ocr_text=extracted_ocr_text)

            image_content_part = {"type": "image_url", "image_url": {"url": image_uri}}
            text_content_part = {"type": "text", "text": vlm_prompt_text}
            vlm_messages = [HumanMessage(content=[image_content_part, text_content_part])]

            vlm_chain = vlm_model | StrOutputParser()
            timing_data = {"session_id": session_id, "mode": f"vlm_ocr_description_{prompt_type}"}

            response_text = await asyncio.to_thread(
                self._call_llm_with_timing, vlm_chain, vlm_messages, timing_data, priority=priority
            )

            # --- 4. Process and Return Result ---
            if response_text and not (isinstance(response_text, str) and "ERROR" in response_text.upper()):
                description_output = self._cleanup_llm_output(response_text.strip())
                logger.success(f"{log_prefix}: VLM augmented description successful.")

                # Log the OCR text along with the final description for full traceability
                log_llm_response = f"OCR Text:\n---\n{extracted_ocr_text}\n---\n\nVLM Description:\n---\n{description_output[:2000]}"
                await asyncio.to_thread(
                    add_interaction, db, session_id=session_id, mode="chat", input_type="log_debug",
                    user_input=f"[VLM+OCR Success - {prompt_type}]", llm_response=log_llm_response
                )
                return description_output, None
            else:
                error_output = f"[VLM description call failed or returned error: {response_text}]"
                logger.warning(f"{log_prefix} {error_output}")
                await asyncio.to_thread(
                    add_interaction, db, session_id=session_id, mode="chat", input_type="log_error",
                    user_input=f"[VLM Desc Failed - {prompt_type}]", llm_response=error_output
                )
                return None, error_output

        except TaskInterruptedException as tie:
            logger.warning(f"🚦 {log_prefix} VLM description INTERRUPTED: {tie}")
            error_output = "[VLM Description Interrupted]"
            await asyncio.to_thread(
                add_interaction, db, session_id=session_id, mode="chat", input_type="log_warning",
                user_input=f"[VLM Desc Interrupted - {prompt_type}]", llm_response=str(tie)
            )
            raise  # Re-raise to propagate interruption

        except Exception as e:
            logger.error(f"{log_prefix} VLM/OCR pipeline failed: {e}", exc_info=True)
            error_output = f"[VLM/OCR Pipeline Error: {type(e).__name__}]"
            await asyncio.to_thread(
                add_interaction, db, session_id=session_id, mode="chat", input_type="log_error",
                user_input=f"[VLM/OCR Desc Error - {prompt_type}]", llm_response=str(e)
            )
            return None, error_output

    

    def _is_valid_tot_json(self, parsed_json: Any) -> bool:
        """Checks if the parsed JSON object has the required ToT structure."""
        if not isinstance(parsed_json, dict):
            return False
        required_keys = {"decomposition", "brainstorming", "evaluation", "synthesis", "confidence_score"}
        # New optional keys for spawning background task
        optional_keys_for_spawn = {"requires_background_task", "next_task_input"}

        if not required_keys.issubset(parsed_json.keys()):
            logger.warning(f"ToT JSON missing one or more required keys: {required_keys - set(parsed_json.keys())}")
            return False
        if not isinstance(parsed_json["brainstorming"], list):
            logger.warning("ToT JSON 'brainstorming' field is not a list.")
            return False
        if not isinstance(parsed_json["confidence_score"], float):
            logger.warning("ToT JSON 'confidence_score' field is not a float.")
            return False

        # Check types for new optional fields if they exist
        if "requires_background_task" in parsed_json and not isinstance(parsed_json["requires_background_task"], bool):
            logger.warning("ToT JSON 'requires_background_task' is not a boolean.")
            return False
        if "next_task_input" in parsed_json and not (
                parsed_json["next_task_input"] is None or isinstance(parsed_json["next_task_input"], str)):
            logger.warning("ToT JSON 'next_task_input' is not a string or null.")
            return False
        if parsed_json.get("requires_background_task") is True and not parsed_json.get("next_task_input"):
            logger.warning("ToT JSON 'requires_background_task' is true, but 'next_task_input' is missing or empty.")
            # This might still be "valid" structurally but logically flawed for spawning.
            # For validation purposes, we'll allow it, but the spawning logic will skip it.

        return True

    async def _run_tree_of_thought_v2(self, db: Session, input_for_tot: str,
                                      rag_context_docs: List[Any],
                                      history_rag_interactions: List[Any],
                                      log_context_str: str,
                                      recent_direct_history_str: str,
                                      file_index_context_str: str,
                                      imagined_image_context_str: str,
                                      interaction_data_for_tot_llm_call: Dict[str, Any],  # For _call_llm_with_timing
                                      original_user_input_for_log: str,
                                      triggering_interaction_id_for_log: int
                                      ) -> str:  # Returns the 'synthesis' string

        log_prefix = f"🌳 ToT_v2|ELP0|TrigID:{triggering_interaction_id_for_log}"
        current_session_id = interaction_data_for_tot_llm_call.get("session_id",
                                                                   f"tot_session_{triggering_interaction_id_for_log}")
        logger.info(f"{log_prefix} Starting ToT for original input: '{original_user_input_for_log[:50]}...'")

        tot_model = self.provider.get_model("router")  # Use router model for ToT
        if not tot_model:
            error_msg = "ToT model ('router') not available for ToT V2 execution."
            logger.error(f"{log_prefix} {error_msg}")
            await asyncio.to_thread(add_interaction, db, session_id=current_session_id,
                                    mode="internal_error", input_type="log_error",
                                    user_input=f"[ToT V2 Failed - Model Unavailable for TrigID: {triggering_interaction_id_for_log}]",
                                    llm_response=error_msg)
            await asyncio.to_thread(db.commit)
            return f"Error: ToT model unavailable for analysis."

        url_rag_context_str = self._format_docs(rag_context_docs, "URL Context")
        history_rag_context_str = self._format_docs(history_rag_interactions, "History/Reflection RAG")

        llm_input_for_tot = {
            "input": original_user_input_for_log, "context": url_rag_context_str,
            "history_rag": history_rag_context_str, "file_index_context": file_index_context_str,
            "log_context": log_context_str, "recent_direct_history": recent_direct_history_str,
            "imagined_image_context": imagined_image_context_str
        }

        chain = ChatPromptTemplate.from_template(PROMPT_TREE_OF_THOUGHTS_V2) | tot_model | StrOutputParser()

        raw_llm_output_from_initial_loop: str = "Initial ToT LLM call did not yield parsable JSON."
        parsed_tot_json: Optional[Dict[str, Any]] = None
        last_error_initial: Optional[Exception] = None

        # --- Stage 1: Initial LLM Call & Parse/Fix ---
        # For complex ToT JSON, let's keep 1 primary attempt before reformat.
        initial_llm_attempts_tot = 1
        for attempt in range(initial_llm_attempts_tot):
            logger.debug(f"{log_prefix} ToT LLM call attempt {attempt + 1}/{initial_llm_attempts_tot}")
            try:
                raw_llm_text_this_attempt = await asyncio.to_thread(
                    self._call_llm_with_timing, chain, llm_input_for_tot,
                    interaction_data_for_tot_llm_call, priority=ELP0
                )
                raw_llm_output_from_initial_loop = raw_llm_text_this_attempt
                logger.trace(
                    f"{log_prefix} Raw LLM for ToT (Attempt {attempt + 1}): '{raw_llm_text_this_attempt[:200]}...'")

                json_candidate_str = self._extract_json_candidate_string(raw_llm_text_this_attempt,
                                                                         log_prefix + "-ExtractInitial")
                if json_candidate_str:
                    parsed_tot_json = self._programmatic_json_parse_and_fix(
                        json_candidate_str, 1, log_prefix + f"-InitialFixAttempt{attempt + 1}"
                    )
                    if parsed_tot_json and self._is_valid_tot_json(parsed_tot_json):
                        logger.info(f"✅ {log_prefix} Initial ToT analysis successful (Attempt {attempt + 1}).")
                        break  # Success from initial attempt
                else:
                    last_error_initial = ValueError(
                        f"No JSON candidate from ToT LLM: {raw_llm_text_this_attempt[:100]}")

                if not (parsed_tot_json and self._is_valid_tot_json(parsed_tot_json)):  # If not broken from success
                    if parsed_tot_json:
                        last_error_initial = ValueError(f"Invalid ToT JSON structure: {str(parsed_tot_json)[:100]}")
                    elif not json_candidate_str:
                        pass  # Error already set if no candidate
                    else:
                        last_error_initial = ValueError(
                            f"Failed to parse/fix ToT JSON candidate: {json_candidate_str[:100]}")
                    parsed_tot_json = None  # Ensure it's None for next stage

            except TaskInterruptedException as tie:
                logger.warning(f"🚦 {log_prefix} ToT task INTERRUPTED: {tie}")
                raise tie  # Propagate to be handled by _run_tot_in_background_wrapper_v2
            except Exception as e_initial_tot:
                last_error_initial = e_initial_tot

            if last_error_initial and not parsed_tot_json:
                logger.warning(f"⚠️ {log_prefix} Initial ToT LLM/parse attempt failed. Error: {last_error_initial}")

        # --- Stage 2: LLM Re-request for Formatting (if initial attempt failed) ---
        if not (parsed_tot_json and self._is_valid_tot_json(parsed_tot_json)):
            logger.warning(
                f"{log_prefix} Initial ToT attempt failed. Trying LLM re-request to fix format. Last raw: '{raw_llm_output_from_initial_loop[:200]}...'")

            reformat_prompt_input = {
                "faulty_llm_output_for_reformat": raw_llm_output_from_initial_loop,
                "original_user_input_placeholder": original_user_input_for_log
            }
            reformat_chain = ChatPromptTemplate.from_template(
                PROMPT_REFORMAT_TO_TOT_JSON) | tot_model | StrOutputParser()

            reformatted_llm_output_text = await asyncio.to_thread(
                self._call_llm_with_timing, reformat_chain, reformat_prompt_input,
                interaction_data_for_tot_llm_call, priority=ELP0
            )

            if reformatted_llm_output_text and not (
                    isinstance(reformatted_llm_output_text, str) and "ERROR" in reformatted_llm_output_text.upper()):
                logger.info(f"{log_prefix} Received reformatted output from LLM for ToT. Attempting to parse/fix...")
                json_candidate_from_reformat = self._extract_json_candidate_string(reformatted_llm_output_text,
                                                                                   log_prefix + "-ReformatExtract")
                if json_candidate_from_reformat:
                    parsed_tot_json = self._programmatic_json_parse_and_fix(
                        json_candidate_from_reformat, JSON_FIX_RETRY_ATTEMPTS_AFTER_REFORMAT,
                        log_prefix + "-ReformatFix"
                    )
            else:
                logger.error(
                    f"{log_prefix} LLM re-request for ToT JSON formatting failed or returned error: {reformatted_llm_output_text}")

        # --- Process Final Result (parsed_tot_json or fallback) ---
        final_synthesis_for_caller = f"Error: ToT analysis for '{original_user_input_for_log[:30]}...' failed to produce valid structured output."
        tot_json_to_save_str: Optional[str] = None

        if parsed_tot_json and self._is_valid_tot_json(parsed_tot_json):
            logger.success(
                f"✅ {log_prefix} ToT analysis JSON successfully parsed. Synthesis snippet: '{str(parsed_tot_json.get('synthesis'))[:50]}...'")
            final_synthesis_for_caller = str(parsed_tot_json.get("synthesis", "ToT synthesis missing from JSON."))
            try:
                tot_json_to_save_str = json.dumps(parsed_tot_json, indent=2)
            except Exception as e_dump:
                logger.error(f"{log_prefix} Failed to dump parsed_tot_json: {e_dump}")
                tot_json_to_save_str = str(
                    parsed_tot_json)

            # --- New: Check if ToT requires a new background task ---
            if parsed_tot_json.get("requires_background_task") is True:
                next_task_input_str = parsed_tot_json.get("next_task_input")
                if next_task_input_str and isinstance(next_task_input_str, str) and next_task_input_str.strip():
                    logger.info(
                        f"{log_prefix} ToT synthesis requires further background task. Input: '{next_task_input_str[:70]}...'")
                    new_bg_task_session_id = f"sub_task_from_tot_{triggering_interaction_id_for_log}_{str(uuid.uuid4())[:4]}"
                    # Spawn new background_generate task (don't await it here, let it run truly in background)
                    asyncio.create_task(
                        self.background_generate(
                            db=db,  # Use current DB session for spawning, BG task will get its own
                            user_input=next_task_input_str,
                            session_id=new_bg_task_session_id,
                            classification="chat_complex",  # Assume new task is complex
                            image_b64=None,
                            update_interaction_id=None  # It's a new task
                        )
                    )
                    logger.info(
                        f"{log_prefix} Spawned new background_generate task for session {new_bg_task_session_id}")
                else:
                    logger.warning(
                        f"{log_prefix} ToT indicated 'requires_background_task' but 'next_task_input' was missing or empty.")
        else:
            logger.error(
                f"{log_prefix} ❌ All ToT attempts failed to produce valid JSON. Last raw: '{raw_llm_output_from_initial_loop[:200]}...'")
            fallback_json_content = {
                "decomposition": "N/A", "brainstorming": [], "evaluation": "N/A",
                "synthesis": final_synthesis_for_caller, "confidence_score": 0.0,
                "self_critique": f"Failed to parse LLM output for ToT. Last raw: {raw_llm_output_from_initial_loop[:200]}",
                "requires_background_task": False, "next_task_input": None
            }
            try:
                tot_json_to_save_str = json.dumps(fallback_json_content, indent=2)
            except Exception as e_dump_fallback:
                logger.error(
                    f"{log_prefix} Failed to dump fallback ToT JSON: {e_dump_fallback}")
                tot_json_to_save_str = str(
                    fallback_json_content)

        # --- Save ToT result (the JSON string) to a new Interaction record ---
        try:
            tot_result_interaction_data = {
                "session_id": current_session_id,
                "mode": "chat",
                "input_type": "tot_result",
                "user_input": f"[ToT Analysis Result for Original Query ID {triggering_interaction_id_for_log}: '{original_user_input_for_log[:100]}...']",
                "llm_response": tot_json_to_save_str,
                "classification": "tot_output_json",
                "execution_time_ms": interaction_data_for_tot_llm_call.get("execution_time_ms", 0),
                "reflection_completed": True, "tot_analysis_requested": False,
                "tot_analysis_spawned": parsed_tot_json.get("requires_background_task") if parsed_tot_json else False,
                # Log if it tried to spawn
                "tot_delivered": False
            }
            valid_keys = {c.name for c in Interaction.__table__.columns}
            db_kwargs_tot_result = {k: v for k, v in tot_result_interaction_data.items() if
                                    k in valid_keys and k != 'id'}

            new_tot_interaction = await asyncio.to_thread(add_interaction, db, **db_kwargs_tot_result)
            if new_tot_interaction and new_tot_interaction.id:
                await asyncio.to_thread(db.commit)
                logger.success(
                    f"✅ {log_prefix} Saved ToT V2 result as Interaction ID {new_tot_interaction.id} for TrigID {triggering_interaction_id_for_log}.")
            else:
                logger.error(
                    f"❌ {log_prefix} Failed to save ToT V2 result for TrigID {triggering_interaction_id_for_log}.")
                await asyncio.to_thread(db.rollback)
        except Exception as db_save_err:
            logger.error(
                f"❌ {log_prefix} Error saving ToT V2 result to DB for TrigID {triggering_interaction_id_for_log}: {db_save_err}")
            await asyncio.to_thread(db.rollback)

        return final_synthesis_for_caller

    # --- reset Method ---
    def reset(self, db: Session, session_id: str = None):
        """Resets Chat mode state for the session."""
        logger.warning(f"🔄 Resetting Chat state. (Session: {session_id})")
        self.vectorstore_url = None
        self.vectorstore_history = None
        self.current_session_id = None
        logger.info("🧹 Chat URL Vectorstore and History context cleared.")
        try:
            add_interaction(db, session_id=session_id, mode="chat", input_type='system', user_input='Chat Session Reset Requested', llm_response='Chat state cleared.')
        except Exception as db_err:
            logger.error(f"Failed to log chat reset: {db_err}")
        return "Chat state cleared."


    # --- Image/URL Processing Methods (Synchronous, Corrected Syntax) ---
    async def _run_image_latex_analysis_stream(self, db: Session, session_id: str, image_content_part: Dict, user_input: str, interaction_data: dict):
        """
        Async generator for image analysis (LaTeX/TikZ).
        Yields progress updates and final token stream as SSE-formatted data chunks.
        Parses the full response afterwards and yields the structured result.

        Args:
            db: SQLAlchemy Session object.
            session_id: The current session ID.
            image_content_part: Dictionary representing the image data for the LLM.
            user_input: The original user text query accompanying the image.
            interaction_data: Dictionary holding data about the current interaction (used for logging context).

        Yields:
            str: SSE formatted strings containing status updates, token deltas, errors,
                 or the final parsed data structure.
        """
        stream_id = f"latex-stream-{uuid.uuid4()}"
        task_start_time = time.monotonic()
        logger.info(f"📸 {stream_id}: Starting STREAMING analysis for LaTeX/TikZ. Input: '{user_input[:50]}...'")

        # Yield initial status
        try:
            yield format_sse({"status": "Initializing LaTeX/Visual Model...", "stream_id": stream_id}, event_type="progress")
        except Exception as yield_err:
             logger.error(f"Error yielding initial progress for {stream_id}: {yield_err}")
             return # Stop if we can't even yield

        # --- Get Model ---
        latex_model = self.provider.get_model("latex")
        if not latex_model:
            error_msg = "LaTeX/Visual model (e.g., LatexMind) not configured."
            logger.error(f"❌ {stream_id}: {error_msg}")
            yield format_sse({"error": error_msg, "final": True, "stream_id": stream_id}, event_type="error")
            # Attempt to log error to DB (best effort)
            try: add_interaction(db, session_id=session_id, mode="chat", input_type="error", user_input="[Image LaTeX/TikZ Init Failed]", llm_response=error_msg)
            except Exception as db_log_err: logger.error(f"Failed to log LaTeX model config error: {db_log_err}")
            return # Stop generation

        # --- Prepare LLM Call ---
        # Combine image and the specific prompt from CortexConfiguration.py
        messages = [HumanMessage(content=[image_content_part, {"type": "text", "text": PROMPT_IMAGE_TO_LATEX}])]
        # Ensure the chain uses the correct model instance
        chain = latex_model | StrOutputParser() # Assumes StrOutputParser works with stream

        yield format_sse({"status": "Sending request to LaTeX/Visual Model...", "stream_id": stream_id}, event_type="progress")
        logger.trace(f"{stream_id}: LaTeX/VLM input messages: {messages}")

        full_response_markdown = ""
        llm_call_start_time = time.monotonic()
        llm_execution_time_ms = 0
        final_status = "success" # Assume success unless error occurs

        try:
            # --- Stream LLM Response ---
            token_count = 0
            async for chunk in chain.astream(messages):
                # Ensure chunk is a string before processing
                if isinstance(chunk, str):
                    full_response_markdown += chunk
                    token_count += 1 # Approximate token count
                    # Yield token chunk (default 'data' event)
                    yield format_sse({"delta": chunk, "stream_id": stream_id})
                elif chunk is not None: # Log unexpected non-string chunks
                    logger.warning(f"{stream_id}: Received non-string chunk during stream: {type(chunk)} - {str(chunk)[:100]}")

            llm_execution_time_ms = (time.monotonic() - llm_call_start_time) * 1000
            logger.info(f"📄 {stream_id}: LaTeX/VLM Raw Stream Complete. Approx Tokens: {token_count}, Duration: {llm_execution_time_ms:.2f} ms. Final Length: {len(full_response_markdown)}")
            yield format_sse({"status": "LLM stream complete. Processing response...", "stream_id": stream_id}, event_type="progress")

            # --- Parse the Completed Markdown Response ---
            logger.debug(f"{stream_id}: Parsing full response...")
            description = full_response_markdown # Default
            latex_code = None
            tikz_code = None
            explanation = None

            # Regex to find ```latex ... ``` block
            latex_match = re.search(r"```latex\s*(.*?)\s*```", full_response_markdown, re.DOTALL)
            if latex_match:
                latex_code = latex_match.group(1).strip()
                logger.info(f"{stream_id}: Found LaTeX code block ({len(latex_code)} chars).")

            # Regex to find ```tikz ... ``` block
            tikz_match = re.search(r"```tikz\s*(.*?)\s*```", full_response_markdown, re.DOTALL)
            if tikz_match:
                tikz_code = tikz_match.group(1).strip()
                logger.info(f"{stream_id}: Found TikZ code block ({len(tikz_code)} chars).")

            # Attempt to extract text outside code blocks as description/explanation
            cleaned_response = full_response_markdown
            # Remove matched blocks to isolate remaining text
            if latex_match: cleaned_response = cleaned_response.replace(latex_match.group(0), "", 1)
            if tikz_match: cleaned_response = cleaned_response.replace(tikz_match.group(0), "", 1)
            cleaned_response = cleaned_response.strip() # Remove leading/trailing whitespace

            # Split based on potential headers (case-insensitive) - refine as needed based on model output
            parts = re.split(r'\n\s*(?:Explain|Explanation|Description)[:\s]*\n', cleaned_response, maxsplit=1, flags=re.IGNORECASE)
            if len(parts) > 1 :
                 description = parts[0].strip()
                 explanation = parts[1].strip()
                 logger.debug(f"{stream_id}: Split description and explanation.")
            else:
                 # Assume all remaining non-code text is description/explanation
                 description = cleaned_response
                 explanation = description # Set explanation to description if no clear split
                 logger.debug(f"{stream_id}: Using combined text as description/explanation.")
                 if not description and (latex_code or tikz_code):
                     description = "(Code generated, no separate description provided)" # Placeholder if only code exists

            logger.debug(f"{stream_id}: Final Parsed -> Desc:'{description[:50]}...', LaTeX:{latex_code is not None}, TikZ:{tikz_code is not None}, Explain:'{explanation[:50]}...'")

            # --- Yield Final Parsed Data ---
            # This structure can be captured by the calling route handler
            final_parsed_data = {
                "description": description,
                "latex_code": latex_code,
                "tikz_code": tikz_code,
                "explanation": explanation,
                "raw_response": full_response_markdown # Include raw for debugging if needed
            }
            yield format_sse({"status": "Parsing complete.", "parsed_data": final_parsed_data, "stream_id": stream_id}, event_type="final_parsed")

        except Exception as e:
             final_status = "error"
             error_msg = f"Error during LaTeX/Visual streaming or processing: {e}"
             logger.error(f"❌ {stream_id}: {error_msg}")
             logger.exception(f"{stream_id}: LaTeX/VLM Stream Traceback:")
             # Yield error information
             yield format_sse({"error": error_msg, "final": True, "stream_id": stream_id}, event_type="error")
             # Attempt to log error to DB
             try: add_interaction(db, session_id=session_id, mode="chat", input_type="error", user_input=f"[Image LaTeX/TikZ Failed Stream {stream_id}]", llm_response=f"{error_msg}\nRaw Response Snippet: {full_response_markdown[:500]}")
             except Exception as db_log_err: logger.error(f"Failed to log LaTeX stream error: {db_log_err}")

        finally:
             # --- Signal End of Stream ---
             total_duration_ms = (time.monotonic() - task_start_time) * 1000
             yield format_sse({"status": f"Stream ended ({final_status}).", "final": True, "stream_id": stream_id, "total_duration_ms": total_duration_ms}, event_type="end_stream")
             logger.info(f"🏁 {stream_id}: LaTeX/VLM analysis stream finished. Status: {final_status}, Duration: {total_duration_ms:.2f} ms")
             # --- DB Logging for Success ---
             # If the calling function saved an initial interaction record,
             # it would ideally update it here or after collecting the 'final_parsed' event.
             # Since this generator doesn't easily get the interaction_id back,
             # we log the final results separately here if successful.
             if final_status == "success":
                 try:
                     log_data = {
                         'session_id': session_id,
                         'mode': 'chat',
                         'input_type': 'latex_analysis_result', # Custom type
                         'user_input': f"[Result for Image LaTeX/TikZ {stream_id}]",
                         'llm_response': f"Description: {description[:200]}...\nExplanation: {explanation[:200]}...",
                         'image_description': description,
                         'latex_representation': latex_code,
                         # 'tikz_representation': tikz_code, # If DB column exists
                         'latex_explanation': explanation,
                         'execution_time_ms': llm_execution_time_ms # Log LLM time specifically
                     }
                     valid_keys = {c.name for c in Interaction.__table__.columns}
                     db_kwargs = {k: v for k, v in log_data.items() if k in valid_keys}
                     add_interaction(db, **db_kwargs)
                     logger.debug(f"{stream_id}: Logged successful LaTeX/TikZ analysis results to DB.")
                 except Exception as db_log_err:
                     logger.error(f"{stream_id}: Failed to log successful LaTeX/TikZ results: {db_log_err}")

    # --- (rest of CortexThoughts class, including the modified generate method) ---

    def process_image(self, db: Session, image_b64: str, session_id: str = None):
        """Processes image, gets description/LaTeX, returns description for non-VLM flow."""
        logger.info(f"🖼️ Processing image for session: {session_id}")
        self.current_session_id = session_id
        # Log initial interaction attempt
        interaction_data = {
            "session_id": session_id, "mode": "chat", "input_type": "image",
            "user_input": "[Image Uploaded]", "image_data": "..." # Placeholder
        }
        self.vectorstore_url = None
        logger.info("🧹 Cleared URL context due to image upload.")

        # Use VLM to get description
        vlm = self.provider.get_model("vlm")
        if not vlm:
            desc = "Error: Visual model (VLM) not available for image description."
            logger.error(desc)
            interaction_data['llm_response'] = desc
            valid_keys = {c.name for c in Interaction.__table__.columns}
            db_kwargs = {k: v for k, v in interaction_data.items() if k in valid_keys}
            add_interaction(db, **db_kwargs)
            return desc, None # Return description and None for image content

        try:
            base64.b64decode(image_b64) # Validate base64
            image_uri = f"data:image/jpeg;base64,{image_b64}"
        except Exception as e:
            desc = f"Error: Invalid image data format. {e}"
            logger.error(desc)
            interaction_data['llm_response'] = desc
            valid_keys = {c.name for c in Interaction.__table__.columns}
            db_kwargs = {k: v for k, v in interaction_data.items() if k in valid_keys}
            add_interaction(db, **db_kwargs)
            return desc, None

        # Prepare VLM input
        image_content_part = {"type": "image_url", "image_url": {"url": image_uri}}
        # Use a simple description prompt
        vlm_messages = [HumanMessage(content=[image_content_part, {"type": "text", "text": "Describe this image concisely."}])]
        vlm_chain = vlm | StrOutputParser()
        vlm_timing_data = {"session_id": session_id, "mode": "chat", "execution_time_ms": 0}

        try:
            logger.info("Calling VLM for image description...")
            image_description = self._call_llm_with_timing(vlm_chain, vlm_messages, vlm_timing_data)
            logger.info(f"🖼️ VLM Description: {image_description[:200]}...")
            # Log this VLM interaction
            interaction_data['llm_response'] = f"[VLM Description: {image_description}]"
            interaction_data['image_description'] = image_description # Store description
            interaction_data['image_data'] = image_b64 # Store image data
            interaction_data['execution_time_ms'] = vlm_timing_data['execution_time_ms']
            valid_keys = {c.name for c in Interaction.__table__.columns}
            db_kwargs = {k: v for k, v in interaction_data.items() if k in valid_keys}
            add_interaction(db, **db_kwargs)
            # Return the description to be added to the user prompt for non-VLM models
            return image_description, image_content_part # Return description and original image content part

        except Exception as e:
            desc = f"Error getting description from VLM: {e}"
            logger.error(desc)
            interaction_data['llm_response'] = desc
            valid_keys = {c.name for c in Interaction.__table__.columns}
            db_kwargs = {k: v for k, v in interaction_data.items() if k in valid_keys}
            add_interaction(db, **db_kwargs)
            return desc, None


    def process_url(self, db: Session, url: str, session_id: str = None):
        """Extracts text from URL, creates vectorstore (synchronous)."""
        logger.info(f"🔗 Processing URL: {url} (Session: {session_id})")
        self.current_session_id = session_id
        interaction_data = {"session_id": session_id, "mode": "chat", "input_type": "url", "user_input": f"[URL Submitted: {url}]", "url_processed": url}
        start_time = time.time()
        result_msg = ""
        success = False
        try:
            text = self.extract_context_through_referencePath(url)
            if not text or not text.strip():
                raise ValueError("No significant text extracted")
            self.create_vectorstore_for_url(text, url)
            if hasattr(self, 'vectorstore_url') and self.vectorstore_url:
                setattr(self.vectorstore_url, '_source_url', url)
                result_msg = f"Processed URL: {url}. Ready for questions."
                success = True
                logger.success(f"✅ URL processed.")
            else:
                result_msg = f"Failed to create vectorstore for URL: {url}"
                success = False
        except Exception as e:
            logger.error(f"❌ Failed to process URL {url}: {e}")
            result_msg = f"Error processing URL: {e}"
            success = False
        finally:
            duration = (time.time() - start_time) * 1000
            interaction_data['llm_response'] = result_msg
            interaction_data['execution_time_ms'] = duration
            valid_keys = {c.name for c in Interaction.__table__.columns}
            db_kwargs = {k: v for k, v in interaction_data.items() if k in valid_keys}
            add_interaction(db, **db_kwargs)
            return result_msg

    def extract_context_through_referencePath(self, url):
        """Extracts text from URL content (synchronous)."""
        logger.debug(f"🌐 Fetching content from {url}")
        try:
            headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
            response = requests.get(url, timeout=30, headers=headers, allow_redirects=True)
            response.raise_for_status()
            content_type = response.headers.get('content-type', '').lower()
            if 'html' not in content_type and 'xml' not in content_type and 'text/plain' not in content_type:
                 logger.warning(f"⚠️ URL content type ({content_type}) not standard text.")
                 try:
                     return response.text.strip()
                 except Exception as de:
                     logger.error(f"Could not decode: {de}")
                     return None
            soup = BeautifulSoup(response.content, "html.parser")
            unwanted = ["script", "style", "header", "footer", "nav", "aside", "form", "button", "select", "noscript", "svg", "canvas", "audio", "video", "iframe", "embed", "object"]
            for element in soup(unwanted):
                element.decompose()
            text = ' '.join(soup.stripped_strings)
            if not text:
                 logger.warning("🚫 BeautifulSoup extraction resulted in empty text.")
                 return None
            logger.debug(f"📄 Extracted ~{len(text)} characters from {url}")
            return text
        except requests.exceptions.RequestException as e:
            logger.error(f"❌ HTTP Error: {e}")
            raise
        except Exception as e:
            logger.error(f"❌ Parsing Error: {e}")
            logger.exception("Parser Traceback:")
            return None


    def create_vectorstore_for_url(self, text: str, url: str):
        """Creates in-memory Chroma vectorstore from text (synchronous)."""
        logger.info(f"🧠 Creating vectorstore for URL: {url}")
        if not self.provider.embeddings:
            logger.error("❌ Embeddings provider missing.")
            self.vectorstore_url = None
            raise ValueError("Embeddings needed")
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=VECTOR_CALC_CHUNK_BATCH_TOKEN_SIZE, chunk_overlap=CHUNK_OVERLAP)
        splits = text_splitter.split_text(text)
        if not splits:
            logger.warning("⚠️ No text splits generated.")
            self.vectorstore_url = None
            return
        logger.debug(f"📊 Split into {len(splits)} chunks.")
        try:
            self.vectorstore_url = Chroma.from_texts(splits, self.provider.embeddings)
            logger.success("✅ Vectorstore created.")
        except Exception as e:
            logger.error(f"❌ Failed Chroma create: {e}")
            logger.exception("Chroma Traceback:")
            self.vectorstore_url = None

    async def _parse_ingested_text_content(self, data_entry: Dict[str, Any]) -> Tuple[Optional[str], Optional[str], bool]:
        """
        Parses a single data entry (dict) to extract user_input_content and assistant_response_content.
        This helper is needed by the ingestion function.
        """
        user_input_content, assistant_response_content = None, None
        extracted_successfully = False

        messages = data_entry.get("messages")
        if isinstance(messages, list) and len(messages) >= 1:
            first_user_msg = next((m.get("content") for m in messages if m.get("role") == "user"), None)
            first_asst_msg = next((m.get("content") for m in messages if m.get("role") == "assistant"), None)
            if first_user_msg: user_input_content = first_user_msg
            if first_asst_msg: assistant_response_content = first_asst_msg
            if user_input_content or assistant_response_content: extracted_successfully = True
        elif "prompt" in data_entry and "completion" in data_entry:
            user_input_content = data_entry.get("prompt")
            assistant_response_content = data_entry.get("completion")
            extracted_successfully = True
        elif "user_input" in data_entry and "llm_response" in data_entry:
            user_input_content = data_entry.get("user_input")
            assistant_response_content = data_entry.get("llm_response")
            extracted_successfully = True
        elif "text" in data_entry: # Fallback for generic text entries
            user_input_content = data_entry.get("text")
            assistant_response_content = "[Ingested as single text entry]"
            extracted_successfully = True

        return user_input_content, assistant_response_content, extracted_successfully

    async def _initiate_file_ingestion_and_reflection(self,
                                                      db_session_from_caller: Session,
                                                      uploaded_file_record_id: int):
        """
        Revised to use a single DB session and transaction for the entire ingestion process.
        """
        current_job_id = f"ingest_proc_{uploaded_file_record_id}"
        logger.info(f"🚀 {current_job_id}: Starting REVISED background file ingestion and reflection.")

        bg_db_session: Optional[Session] = None
        uploaded_record_path: Optional[str] = None
        request_start_time = time.monotonic()

        try:
            bg_db_session = SessionLocal()
            if not bg_db_session:
                raise RuntimeError("Failed to create a new database session for the background ingestion task.")

            # Fetch and update the main upload record
            uploaded_record = bg_db_session.query(UploadedFileRecord).filter(
                UploadedFileRecord.id == uploaded_file_record_id).with_for_update().first()

            if not uploaded_record:
                raise FileNotFoundError(f"UploadedFileRecord ID {uploaded_file_record_id} not found.")

            uploaded_record_path = uploaded_record.stored_path
            original_filename = uploaded_record.original_filename
            file_ext = os.path.splitext(original_filename)[1].lower()

            logger.info(
                f"{current_job_id}: Processing file '{original_filename}' ({uploaded_record_path}) for ingestion.")
            uploaded_record.status = "processing"
            bg_db_session.commit()

            # Phase 1: Read the file and add all interaction stubs to the session
            # (This section is simplified; the full parsing logic from your original file would go here)
            newly_created_interaction_ids = []
            processed_rows_or_lines = 0

            # This is a simplified representation of your file-parsing logic
            # Replace this with the full if/elif block for .jsonl, .csv, .txt, etc.
            with open(uploaded_record_path, 'r', encoding='utf-8') as f:
                for i, line in enumerate(f):
                    processed_rows_or_lines += 1
                    if not line.strip(): continue
                    bg_user_input = f"{line.strip()}"
                    bg_session_id = f"ingest_{uploaded_record.ingestion_id}_entry_{i + 1}"

                    # Use the new no-commit function
                    new_interaction = add_interaction_no_commit(
                        bg_db_session,
                        session_id=bg_session_id,
                        mode="chat",
                        input_type="ingested_file_entry_raw",
                        user_input=bg_user_input,
                        llm_response="[Queued for background reflection]",
                        classification="ingested_reflection_task_queued",
                        reflection_completed=False,
                    )
                    if new_interaction:
                        # Flush to get the ID for the background task
                        bg_db_session.flush()
                        if new_interaction.id:
                            newly_created_interaction_ids.append(new_interaction.id)

            # After adding all stubs, commit the transaction
            bg_db_session.commit()
            logger.info(
                f"{current_job_id}: Committed {len(newly_created_interaction_ids)} raw interaction records to the database.")

            # Phase 2: Spawn background tasks for the committed records
            spawned_tasks_count = 0
            for interaction_id in newly_created_interaction_ids:
                asyncio.create_task(
                    self.background_generate(
                        db=None,  # The task will create its own session
                        user_input=None,
                        session_id=None,
                        classification="ingested_reflection_task",
                        image_b64=None,
                        update_interaction_id=interaction_id
                    )
                )
                spawned_tasks_count += 1

            logger.info(f"{current_job_id}: Spawned {spawned_tasks_count} background reflection tasks.")

            # Final status update for the upload record
            uploaded_record.status = "completed"
            uploaded_record.processed_entries_count = processed_rows_or_lines
            uploaded_record.spawned_tasks_count = spawned_tasks_count
            bg_db_session.commit()
            logger.success(f"{current_job_id}: Ingestion orchestration finished successfully.")

        except Exception as e:
            logger.error(f"{current_job_id}: An error occurred during the ingestion process: {e}")
            if bg_db_session:
                bg_db_session.rollback()
                try:
                    # Attempt to update the record with the error status
                    record_to_update = bg_db_session.query(UploadedFileRecord).filter(
                        UploadedFileRecord.id == uploaded_file_record_id).first()
                    if record_to_update:
                        record_to_update.status = "failed"
                        record_to_update.processing_error = str(e)[:1000]
                        bg_db_session.commit()
                except Exception as e_log:
                    logger.error(f"{current_job_id}: Could not even update the record to a failed state: {e_log}")
                    bg_db_session.rollback()

        finally:
            if bg_db_session:
                bg_db_session.close()
                logger.debug(f"{current_job_id}: Background DB session closed.")
            # Clean up the temporary file
            if uploaded_record_path and os.path.exists(uploaded_record_path):
                try:
                    os.remove(uploaded_record_path)
                    logger.info(f"{current_job_id}: Cleaned up temporary uploaded file.")
                except Exception as e_del:
                    logger.warning(f"Failed to delete temp file '{uploaded_record_path}': {e_del}")

        # In AdelaideAlbertCortex.py, inside the CortexThoughts class

    async def background_generate(self, db: Session, user_input: str, session_id: str = None,  # type: ignore
                                  classification: str = "chat_simple", image_b64: Optional[str] = None,
                                  update_interaction_id: Optional[int] = None,  # For reflection tasks
                                  stop_event_for_bg: Optional[threading.Event] = None,
                                  parent_ingestion_job_id: Optional[int] = None):  # Link to parent ingestion job

        # ======================================================================
        # 1. INITIALIZATION & SETUP
        # ======================================================================
        request_id = f"bgen-{uuid.uuid4()}"
        is_reflection_task = update_interaction_id is not None
        log_prefix_base = "🔄 REFLECT" if is_reflection_task else "💬 BGEN"
        log_prefix = f"{log_prefix_base} {request_id}|ELP0"

        created_local_db_session = False
        if db is None:
            try:
                db = SessionLocal()  # type: ignore
                created_local_db_session = True
                logger.debug(f"{log_prefix} Created local DB session for this background task.")
            except Exception as e_db_create:
                logger.error(f"{log_prefix} CRITICAL: Failed to create local DB session: {e_db_create}. Aborting.")
                return

        if session_id is None:
            session_id = (f"session_{int(time.monotonic())}" if not is_reflection_task
                          else f"reflection_on_{update_interaction_id}_{str(uuid.uuid4())[:4]}")

        original_chat_session_id = self.current_session_id
        self.current_session_id = session_id

        input_log_snippet = f"'{user_input[:50]}...'" if user_input else "'None (reflection task)'"
        logger.info(
            f"{log_prefix} START --> Session: {session_id}, Class: '{classification}', Input: {input_log_snippet}, "
            f"Img: {'Yes' if image_b64 else 'No'}, Target ID: {update_interaction_id or 'N/A'}"
        )

        request_start_time = time.monotonic()

        interaction_data: Dict[str, Any] = {
            "session_id": session_id, "mode": "chat", "input_type": "text",
            "user_input": user_input, "llm_response": "[Processing background task...]",
            "execution_time_ms": 0, "classification": classification, "classification_reason": None,
            "rag_history_ids": None, "rag_source_url": None,
            "requires_deep_thought": (classification == "chat_complex") or is_reflection_task,
            "deep_thought_reason": None, "tot_analysis_requested": False,
            "tot_analysis_spawned": False, "tot_result": None, "tot_delivered": False,
            "emotion_context_analysis": None, "image_description": None,
            "assistant_action_analysis_json": None, "assistant_action_type": None,
            "assistant_action_params": None, "assistant_action_executed": False,
            "assistant_action_result": None,
            "image_data": image_b64[:20] + "..." if image_b64 and isinstance(image_b64, str) else None,
            "imagined_image_prompt": None, "imagined_image_b64": None,
            "imagined_image_vlm_description": None,
            "reflection_completed": False,
            "reflection_indexed_in_vs": False,
            "parent_ingestion_job_id": parent_ingestion_job_id
        }
        if image_b64:
            interaction_data["input_type"] = "image+text"

        final_response_text_for_this_turn = "Error: Background processing did not complete as expected."
        existing_interaction_to_update: Optional[Interaction] = None
        task_completed_successfully = False

        max_retries_for_bg_task = LLM_CALL_ELP0_INTERRUPT_MAX_RETRIES
        retry_delay_seconds = LLM_CALL_ELP0_INTERRUPT_RETRY_DELAY

        # ======================================================================
        # 2. LOAD/CREATE INTERACTION RECORD
        # ======================================================================
        if not user_input and not image_b64 and not is_reflection_task:
            logger.warning(f"{log_prefix} Empty input (no text, no image). Aborting.")
            self.current_session_id = original_chat_session_id
            if created_local_db_session and db: db.close()
            return

        if is_reflection_task:
            try:
                existing_interaction_to_update = await asyncio.to_thread(
                    db.query(Interaction).filter(Interaction.id == update_interaction_id).first
                )
                if existing_interaction_to_update is None:
                    logger.error(
                        f"{log_prefix} CRITICAL - Target Interaction ID {update_interaction_id} not found. Aborting.")
                    self.current_session_id = original_chat_session_id
                    if created_local_db_session and db: db.close()
                    return

                user_input = existing_interaction_to_update.user_input or existing_interaction_to_update.llm_response or ""
                interaction_data["user_input"] = user_input
                interaction_data["classification_reason"] = "Updating existing record (reflection/ingested task)."
                input_log_snippet_reflection = f"'{user_input[:50]}...'" if user_input else "'[Input Content is None/Empty]'"
                logger.info(
                    f"{log_prefix} Loaded Interaction {update_interaction_id} for reflection. Using content: {input_log_snippet_reflection}")
            except Exception as e_load_orig:
                logger.error(
                    f"{log_prefix} Error loading existing Interaction ID {update_interaction_id}: {e_load_orig}. Aborting.")
                self.current_session_id = original_chat_session_id
                if created_local_db_session and db: db.close()
                return
        else:
            try:
                initial_save_data = interaction_data.copy()
                valid_keys_init = {c.name for c in Interaction.__table__.columns}
                db_kwargs_init = {k: v for k, v in initial_save_data.items() if k in valid_keys_init and k != 'id'}
                existing_interaction_to_update = await asyncio.to_thread(add_interaction, db, **db_kwargs_init)
                await asyncio.to_thread(db.commit)
                if not (existing_interaction_to_update and existing_interaction_to_update.id):
                    raise RuntimeError("Failed to save and retrieve ID for initial 'pending' interaction.")
                logger.info(
                    f"{log_prefix} Saved initial 'pending' Interaction ID {existing_interaction_to_update.id}.")
            except Exception as e_initial_save:
                logger.error(f"{log_prefix} Error saving initial 'pending' interaction: {e_initial_save}")
                if db: await asyncio.to_thread(db.rollback)
                if created_local_db_session and db: db.close()
                return

        # ======================================================================
        # 3. MAIN PROCESSING BLOCK (WITH SEMAPHORE & RETRIES)
        # ======================================================================
        semaphore_acquired_for_task = False
        try:
            background_generate_task_semaphore.acquire()
            semaphore_acquired_for_task = True
            logger.info(f"{log_prefix} Acquired background_generate_task_semaphore.")

            was_busy_waiting = False
            while server_is_busy_event.is_set():
                if stop_event_for_bg and stop_event_for_bg.is_set():
                    raise TaskInterruptedException("Background task stopped during politeness wait.")
                if not was_busy_waiting:
                    logger.info(f"🚦 {log_prefix} Server busy, pausing background task start...")
                    was_busy_waiting = True
                await asyncio.sleep(1.0)
            if was_busy_waiting: logger.info(f"🟢 {log_prefix} Server free, resuming background task.")

            current_attempt = 0
            while current_attempt <= max_retries_for_bg_task:
                current_attempt += 1
                if current_attempt > 1:
                    logger.warning(
                        f"{log_prefix} Retrying entire background generate process (Attempt {current_attempt})...")
                    await asyncio.sleep(retry_delay_seconds)

                try:
                    if stop_event_for_bg and stop_event_for_bg.is_set():
                        raise TaskInterruptedException("Background task stopped by external signal before attempt.")

                    # ==========================================================
                    # 3.1 CORE ANALYSIS: "SHOULD I IMAGINE?" DECISION & EXECUTION
                    # ==========================================================
                    current_input_for_analysis = user_input
                    if image_b64:
                        vlm_desc, _ = await self._describe_image_async(db, session_id, image_b64, "initial_description",
                                                                       ELP0)
                        if vlm_desc:
                            current_input_for_analysis = f"[Image Context from User: {vlm_desc}]\n\nUser Query: {user_input or '(No text query)'}"
                            interaction_data['image_description'] = vlm_desc

                    logger.info(f"{log_prefix} Deciding whether to generate an image...")
                    temp_history = await asyncio.to_thread(get_global_recent_interactions, db, limit=3)
                    imagine_context_summary = self._format_direct_history(temp_history)

                    should_imagine = await self._should_i_imagine_async(db, session_id, current_input_for_analysis,
                                                                        imagine_context_summary)

                    if should_imagine:
                        logger.info(f"{log_prefix} Decision is YES. Initiating image generation pipeline...")
                        idea_to_viz = f"A visualization based on the user's request: {current_input_for_analysis}"
                        temp_history_rag_docs = await asyncio.to_thread(get_recent_interactions, db, 10, session_id,
                                                                        "chat")
                        temp_history_rag_str = self._format_docs(temp_history_rag_docs, "History RAG")
                        temp_direct_history_str = self._format_direct_history(temp_history)

                        img_prompt = await self._generate_image_generation_prompt_async(
                            db, session_id, user_input, idea_to_viz, temp_history_rag_str, "", temp_direct_history_str,
                            "", ""
                        )
                        interaction_data['imagined_image_prompt'] = img_prompt
                        await asyncio.to_thread(add_interaction, db, session_id=session_id,
                                                input_type='log_info_imagine_prompt', llm_response=img_prompt,
                                                classification='internal_cognitive_step')
                        await asyncio.to_thread(db.commit)

                        if img_prompt:
                            img_data_list, img_err = await self.provider.generate_image_async(prompt=img_prompt,
                                                                                              priority=ELP0)
                            if img_err:
                                logger.error(f"{log_prefix} Image generation worker failed: {img_err}")
                            elif img_data_list and img_data_list[0].get("b64_avif"):
                                img_avif_b64_gen = img_data_list[0]["b64_avif"]
                                interaction_data['imagined_image_avif_b64'] = img_avif_b64_gen
                                gen_vlm_desc, _ = await self._describe_image_async(db, session_id, img_avif_b64_gen,
                                                                                   "describe_generated_image", ELP0,
                                                                                   is_avif=True)
                                interaction_data['imagined_image_vlm_description'] = gen_vlm_desc
                                await asyncio.to_thread(
                                    add_interaction, db, session_id=session_id, mode="chat",
                                    input_type="image_generated_by_ai",
                                    user_input=f"[AI Generated Image from prompt: {img_prompt[:500]}]",
                                    imagined_image_avif_b64=img_avif_b64_gen,
                                    image_description=gen_vlm_desc,
                                    classification="vlm_analysis_of_internal_image"
                                )
                                await asyncio.to_thread(db.commit)
                                logger.info(
                                    f"{log_prefix} Logged generated AVIF image and its VLM description to the database.")
                            else:
                                logger.warning(
                                    f"{log_prefix} Image worker returned no data, no error, or missing 'b64_avif' key.")

                    # ==========================================================
                    # 3.2 CORE ANALYSIS: GATHER COMPREHENSIVE CONTEXT
                    # ==========================================================
                    logger.info(f"{log_prefix} Gathering final comprehensive context for response...")
                    wrapped_rag_res = await asyncio.to_thread(self._get_rag_retriever_thread_wrapper, db,
                                                              current_input_for_analysis, ELP0)
                    if wrapped_rag_res.get("status") != "success": raise RuntimeError(
                        f"Final RAG retrieval failed: {wrapped_rag_res.get('error_message')}")

                    url_ret_obj, sess_hist_ret_obj, refl_chunk_ret_obj, sess_chat_rag_ids = wrapped_rag_res.get("data")
                    interaction_data['rag_history_ids'] = sess_chat_rag_ids
                    url_docs, session_docs, reflection_docs = [], [], []
                    if url_ret_obj: url_docs = await asyncio.to_thread(url_ret_obj.invoke, current_input_for_analysis)
                    if sess_hist_ret_obj: session_docs = await asyncio.to_thread(sess_hist_ret_obj.invoke,
                                                                                 current_input_for_analysis)
                    if refl_chunk_ret_obj: reflection_docs = await asyncio.to_thread(refl_chunk_ret_obj.invoke,
                                                                                     current_input_for_analysis)

                    global_hist_final = await asyncio.to_thread(get_global_recent_interactions, db, limit=5)
                    direct_hist_prompt_final = self._format_direct_history(global_hist_final)
                    log_entries_final = await asyncio.to_thread(get_recent_interactions, db, RAG_HISTORY_COUNT * 2,
                                                                session_id, "chat", True)
                    log_ctx_prompt_final = self._format_log_history(log_entries_final)
                    emotion_analysis_str_final = await asyncio.to_thread(self._run_emotion_analysis, db, user_input,
                                                                         interaction_data)

                    vec_file_ctx_result_str = await self._get_vector_search_file_index_context(
                        current_input_for_analysis, session_id, ELP0, stop_event_for_bg)
                    url_context_str = self._format_docs(url_docs, "URL Context")
                    history_rag_str = self._format_docs(session_docs + reflection_docs, "History/Reflection RAG")

                    # ==========================================================
                    # 3.3 CORE ANALYSIS: AGENTIC ACTION OR TEXT SYNTHESIS
                    # ==========================================================
                    initial_synthesis_or_action_result: str = ""

                    logger.info(f"{log_prefix} Analyzing for agentic actions (search, scripts, etc.)...")
                    action_payload_ctx = {"history_summary": emotion_analysis_str_final,
                                          "log_context": log_ctx_prompt_final,
                                          "recent_direct_history": direct_hist_prompt_final,
                                          "file_index_context": vec_file_ctx_result_str}
                    action_details = await self._analyze_assistant_action(db, current_input_for_analysis, session_id,
                                                                          action_payload_ctx)
                    detected_action_type = action_details.get("action_type",
                                                              "no_action") if action_details and isinstance(
                        action_details, dict) else "no_action"
                    if action_details:
                        interaction_data.update({'assistant_action_analysis_json': json.dumps(action_details),
                                                 'assistant_action_type': detected_action_type,
                                                 'assistant_action_params': json.dumps(
                                                     action_details.get("parameters", {}))})

                    if detected_action_type != "no_action" and action_details:
                        logger.info(f"{log_prefix} Action '{detected_action_type}' detected. Executing tool...")
                        if not existing_interaction_to_update: raise RuntimeError(
                            "Missing interaction context for action execution.")
                        initial_synthesis_or_action_result = await self._execute_assistant_action(db, session_id,
                                                                                                  action_details,
                                                                                                  existing_interaction_to_update)
                        interaction_data['assistant_action_executed'] = True
                    else:
                        logger.info(
                            f"{log_prefix} No agentic action detected. Proceeding with standard text generation.")
                        router_payload = {
                            "input": current_input_for_analysis, "recent_direct_history": direct_hist_prompt_final,
                            "context": url_context_str, "history_rag": history_rag_str,
                            "file_index_context": vec_file_ctx_result_str, "log_context": log_ctx_prompt_final,
                            "emotion_analysis": emotion_analysis_str_final, "pending_tot_result": "None.",
                            "imagined_image_vlm_description": interaction_data.get('imagined_image_vlm_description',
                                                                                   'None.')
                        }
                        role, query, reason = await self._route_to_specialist(db, session_id,
                                                                              current_input_for_analysis,
                                                                              router_payload)
                        interaction_data['classification_reason'] = f"Routed to {role}: {reason}"
                        specialist_model = self.provider.get_model(role)
                        if not specialist_model: raise ValueError(f"Specialist model '{role}' not found.")

                        specialist_payload = router_payload.copy()
                        specialist_payload["input"] = query
                        specialist_chain = (self.text_prompt_template | specialist_model | StrOutputParser())
                        timing_data_specialist = {"session_id": session_id, "mode": f"chat_specialist_{role}"}
                        draft_response = await asyncio.to_thread(self._call_llm_with_timing, specialist_chain,
                                                                 specialist_payload, timing_data_specialist,
                                                                 priority=ELP0)
                        initial_synthesis_or_action_result = await self._correct_response(db, session_id,
                                                                                          current_input_for_analysis,
                                                                                          specialist_payload,
                                                                                          draft_response)

                    # ==========================================================
                    # 3.4 ITERATIVE ELABORATION PHASE
                    # ==========================================================
                    logger.info(
                        f"{log_prefix} Starting iterative elaboration based on initial synthesis (len: {len(initial_synthesis_or_action_result)}).")
                    elaborated_chunks_array = [initial_synthesis_or_action_result]
                    is_elaboration_finished = False
                    elaboration_model = self.provider.get_model("router")

                    if not initial_synthesis_or_action_result.strip():
                        logger.warning(f"{log_prefix} Initial synthesis was empty. Skipping elaboration phase.")
                        is_elaboration_finished = True

                    for chunk_num in range(BACKGROUND_MAX_CHUNKS):
                        if is_elaboration_finished: break

                        logger.info(f"{log_prefix} Elaborating chunk {chunk_num + 1}/{BACKGROUND_MAX_CHUNKS}...")
                        current_response_so_far = "".join(elaborated_chunks_array)
                        dynamic_rag_query = f"Original Query: '{user_input}'\nElaboration in progress: '{current_response_so_far[-1000:]}'"
                        dynamic_rag_docs = []
                        if sess_hist_ret_obj:
                            dynamic_rag_docs = await asyncio.to_thread(sess_hist_ret_obj.invoke, dynamic_rag_query)
                        dynamic_rag_context_str = self._format_docs(dynamic_rag_docs, "Dynamic RAG")

                        elaboration_prompt_input = {
                            "initial_synthesis": initial_synthesis_or_action_result,
                            "dynamic_rag_context": dynamic_rag_context_str,
                            "current_response_so_far": current_response_so_far,
                            "self_termination_token": SELF_TERMINATION_TOKEN,
                        }

                        bound_elaboration_model = elaboration_model.bind(max_tokens=BACKGROUND_MAX_TOKENS_PER_CHUNK,
                                                                         stop=[CHATML_END_TOKEN,
                                                                               SELF_TERMINATION_TOKEN])
                        elaboration_chain = ChatPromptTemplate.from_template(
                            PROMPT_BACKGROUND_ELABORATE_CONCLUSION) | bound_elaboration_model | StrOutputParser()
                        timing_data_chunk = {"session_id": session_id, "mode": f"chat_bg_elaborate_chunk_{chunk_num}"}

                        generated_chunk_raw = await asyncio.to_thread(self._call_llm_with_timing, elaboration_chain,
                                                                      elaboration_prompt_input, timing_data_chunk,
                                                                      priority=ELP0)

                        if SELF_TERMINATION_TOKEN in generated_chunk_raw:
                            is_elaboration_finished = True
                            generated_chunk_raw = generated_chunk_raw.split(SELF_TERMINATION_TOKEN)[0]

                        clean_chunk_to_add = generated_chunk_raw.strip()
                        if clean_chunk_to_add:
                            elaborated_chunks_array.append(" " + clean_chunk_to_add)
                        else:
                            is_elaboration_finished = True

                    final_response_text_for_this_turn = "".join(elaborated_chunks_array).strip()

                    # --- NEW: Multi-language Summarization and Translation ---
                    logger.info(f"{log_prefix} Starting multi-language summarization and translation.")

                    # 1. Determine original language of the user input
                    # In a real scenario, you'd use a library like `langdetect` or `fasttext` here.
                    # For the purpose of this task, we'll assume English for the original query language.
                    # If a language detection library were available, it would be used like:
                    # try:
                    #     from langdetect import detect
                    #     original_query_lang_code = detect(user_input)
                    # except ImportError:
                    #     logger.warning("langdetect not installed. Defaulting original query language to 'en'.")
                    # except Exception as e:
                    #     logger.warning(f"Language detection failed: {e}. Defaulting to 'en'.")
                    original_query_lang_code = "en" # Default to English for now.

                    # 2. Prepare prompt for summarization and translation
                    summary_prompt_input = {
                        "text_to_summarize": final_response_text_for_this_turn
                    }

                    # 3. Call the translator model
                    translator_model = self.provider.get_model("translator")
                    if not translator_model:
                        logger.error(f"{log_prefix} Translator model not available for summarization. Skipping multi-language summary.")
                    else:
                        summary_chain = ChatPromptTemplate.from_template(PROMPT_MULTI_LANGUAGE_SUMMARY) | translator_model | StrOutputParser()
                        summary_timing_data = {"session_id": session_id, "mode": "multi_lang_summary"}

                        try:
                            raw_summary_output = await asyncio.to_thread(
                                self._call_llm_with_timing, summary_chain, summary_prompt_input, summary_timing_data, priority=ELP0
                            )

                            # Parse the JSON output
                            json_candidate = self._extract_json_candidate_string(raw_summary_output, log_prefix + "-SummaryExtract")
                            if json_candidate:
                                parsed_summaries = self._programmatic_json_parse_and_fix(json_candidate, log_prefix=log_prefix + "-SummaryFix")
                                if parsed_summaries and isinstance(parsed_summaries, dict):
                                    # Store the summaries in the interaction_data
                                    # Assuming new fields are added to the Interaction model in database.py
                                    # e.g., summary_original_lang, summary_en, summary_zh
                                    interaction_data["summary_original_lang"] = parsed_summaries.get("summary_original_lang", "")
                                    interaction_data["summary_en"] = parsed_summaries.get("summary_en", "")
                                    interaction_data["summary_zh"] = parsed_summaries.get("summary_zh", "")

                                    logger.info(f"{log_prefix} Multi-language summaries generated and stored.")
                                else:
                                    logger.warning(f"{log_prefix} Failed to parse multi-language summary JSON. Raw: {raw_summary_output[:200]}")
                            else:
                                logger.warning(f"{log_prefix} No JSON candidate found for multi-language summary. Raw: {raw_summary_output[:200]}")

                        except TaskInterruptedException as tie:
                            logger.warning(f"🚦 {log_prefix} Multi-language summarization INTERRUPTED: {tie}")
                            # Do not re-raise, just log and continue
                        except Exception as e:
                            logger.error(f"❌ {log_prefix} Error during multi-language summarization: {e}", exc_info=True)
                    # --- END NEW: Multi-language Summarization and Translation ---

                    # ==========================================================
                    # 3.5 SPAWN TREE OF THOUGHTS (IF NEEDED)
                    # ==========================================================
                    if not is_reflection_task and interaction_data.get("requires_deep_thought"):
                        trigger_id_for_tot = existing_interaction_to_update.id if existing_interaction_to_update else None
                        if trigger_id_for_tot:
                            logger.info(f"{log_prefix} Spawning ToT for Interaction ID: {trigger_id_for_tot}.")
                            tot_payload = {
                                "db_session_factory": SessionLocal,
                                "original_input_for_tot": current_input_for_analysis,
                                "rag_context_docs": url_docs,
                                "history_rag_interactions": session_docs + reflection_docs,
                                "log_context_str": log_ctx_prompt_final,
                                "recent_direct_history_str": direct_hist_prompt_final,
                                "file_index_context_str": vec_file_ctx_result_str,
                                "triggering_interaction_id": trigger_id_for_tot,
                                "imagined_image_context_str": interaction_data.get('imagined_image_vlm_description', 'None.')
                            }
                            asyncio.create_task(self._run_tot_in_background_wrapper_v2(**tot_payload))
                            interaction_data['tot_analysis_spawned'] = True
                        else:
                            logger.warning(f"{log_prefix} Could not spawn ToT, no trigger ID.")

                    # ==========================================================
                    # 3.6 POST-GENERATION: CREATE AUTOMATION HOOK?
                    # ==========================================================
                    task_completed_successfully = True  # Mark success before hook check
                    if task_completed_successfully and not is_reflection_task:
                        try:
                            logger.info(f"{log_prefix} Post-generation check: Should this be automated with a hook?")
                            final_rag_context_for_hook = f"URL Context:\n{url_context_str}\n\nHistory Context:\n{history_rag_str}\n\nFile Context:\n{vec_file_ctx_result_str}"
                            should_create, reason = await self._should_i_create_a_hook_async(db, session_id, user_input,
                                                                                             final_rag_context_for_hook,
                                                                                             final_response_text_for_this_turn)
                            if should_create:
                                logger.info(
                                    f"{log_prefix} Spawning background task to generate automation hook. Reason: {reason}")
                                asyncio.create_task(self._generate_stella_icarus_hook_async(db, session_id, user_input,
                                                                                            final_rag_context_for_hook,
                                                                                            final_response_text_for_this_turn))
                        except Exception as hook_logic_err:
                            logger.error(f"{log_prefix} Error in the hook creation logic block: {hook_logic_err}",
                                         exc_info=True)

                    break  # Exit the retry loop on success

                except TaskInterruptedException as tie:
                    logger.warning(f"🚦 {log_prefix} Attempt {current_attempt} INTERRUPTED: {tie}")
                    final_response_text_for_this_turn = f"[Task interrupted: {tie}]"
                    interaction_data.update(
                        {'llm_response': final_response_text_for_this_turn, 'classification': "task_failed_interrupted",
                         'input_type': 'log_warning'})
                    if current_attempt >= max_retries_for_bg_task:
                        task_completed_successfully = False
                        break
                except Exception as e_inner:
                    logger.error(
                        f"❌❌ {log_prefix} Attempt {current_attempt} FAILED with unhandled exception: {e_inner}")
                    logger.exception(f"{log_prefix} Attempt {current_attempt} Traceback:")
                    final_response_text_for_this_turn = f"Error in processing: {type(e_inner).__name__} - {str(e_inner)}"
                    interaction_data.update(
                        {'llm_response': final_response_text_for_this_turn[:4000], 'input_type': 'error'})
                    task_completed_successfully = False
                    break

        except Exception as e_outer:
            logger.critical(f"🔥🔥 {log_prefix} CRITICAL UNHANDLED exception in outer block: {e_outer}")
            logger.exception(f"{log_prefix} Outer Traceback:")
            final_response_text_for_this_turn = f"Critical Error: {type(e_outer).__name__}"
            interaction_data.update({'llm_response': final_response_text_for_this_turn[:4000], 'input_type': 'error'})
            task_completed_successfully = False

        # ======================================================================
        # 4. FINALIZATION & DB UPDATE
        # ======================================================================
        finally:
            if semaphore_acquired_for_task:
                background_generate_task_semaphore.release()
                logger.info(f"{log_prefix} Released background_generate_task_semaphore.")

            final_db_data_to_save = interaction_data.copy()
            final_db_data_to_save['llm_response'] = self._cleanup_llm_output(final_response_text_for_this_turn)
            final_db_data_to_save['execution_time_ms'] = (time.monotonic() - request_start_time) * 1000.0

            try:
                if existing_interaction_to_update:
                    for key, value in final_db_data_to_save.items():
                        if hasattr(existing_interaction_to_update, key) and key != 'id':
                            setattr(existing_interaction_to_update, key, value)

                    if is_reflection_task:
                        was_interrupted_or_errored = final_db_data_to_save.get('classification', '').startswith(
                            "task_failed") or final_db_data_to_save.get('input_type') == 'error'
                        if not was_interrupted_or_errored:
                            existing_interaction_to_update.reflection_completed = True
                            if existing_interaction_to_update.input_type == "ingested_file_entry_raw":
                                existing_interaction_to_update.input_type = "ingested_file_entry_processed"
                                existing_interaction_to_update.classification = "ingested_reflection_task_completed"
                        else:
                            existing_interaction_to_update.reflection_completed = False

                    await asyncio.to_thread(db.commit)
                    logger.info(
                        f"{log_prefix} Updated Interaction ID {existing_interaction_to_update.id} with final results.")

                    if is_reflection_task and task_completed_successfully and not was_interrupted_or_errored:
                        if not attributes.instance_state(existing_interaction_to_update).session:
                            existing_interaction_to_update = db.merge(existing_interaction_to_update)
                        await asyncio.to_thread(index_single_reflection, existing_interaction_to_update, self.provider,
                                                db, ELP0)
                else:
                    logger.warning(f"{log_prefix} No target interaction record to update. Saving as a new record.")
                    valid_keys_final_new = {c.name for c in Interaction.__table__.columns}
                    db_kwargs_final_new = {k: v for k, v in final_db_data_to_save.items() if
                                           k in valid_keys_final_new and k != 'id'}
                    await asyncio.to_thread(add_interaction, db, **db_kwargs_final_new)
                    await asyncio.to_thread(db.commit)

            except Exception as final_db_save_err:
                logger.error(f"❌ {log_prefix} CRITICAL error during final DB save/update: {final_db_save_err}")
                if db: await asyncio.to_thread(db.rollback)

            self.current_session_id = original_chat_session_id
            logger.info(
                f"{log_prefix} END. Duration: {final_db_data_to_save.get('execution_time_ms', 0):.2f}ms. Success: {task_completed_successfully}"
            )

            if created_local_db_session and db:
                db.close()
                logger.debug(f"{log_prefix} Closed local DB session.")


def sanitize_filename(name: str, max_length: int = 200, replacement_char: str = '_') -> str:
    """
    Cleans a string to be suitable for use as a filename.

    Removes potentially problematic characters, replaces whitespace,
    and truncates to a maximum length.
    """
    if not isinstance(name, str):
        name = str(name) # Attempt to convert non-strings

    # Remove leading/trailing whitespace
    name = name.strip()

    # Replace problematic characters with the replacement character
    # Characters to remove/replace include path separators, control chars, etc.
    # Keeping alphanumeric, hyphen, underscore, period.
    # Removing: / \ : * ? " < > | and control characters (0-31)
    sanitized = re.sub(r'[<>:"/\\|?*\x00-\x1F]', replacement_char, name)

    # Replace multiple consecutive replacement characters or spaces with a single one
    sanitized = re.sub(f'[{re.escape(replacement_char)}\s]+', replacement_char, sanitized)

    # Remove leading/trailing replacement characters that might result from substitutions
    sanitized = sanitized.strip(replacement_char)

    # Truncate to maximum length
    if len(sanitized) > max_length:
        # Try truncating at the last replacement char before max_length to avoid cutting words
        try:
            trunc_point = sanitized[:max_length].rindex(replacement_char)
            sanitized = sanitized[:trunc_point]
        except ValueError:
            # If no replacement char found, just truncate hard
            sanitized = sanitized[:max_length]
        # Ensure it doesn't end with the replacement char after truncation
        sanitized = sanitized.strip(replacement_char)

    # Handle empty string case after sanitization
    if not sanitized:
        return "sanitized_empty_name"

    return sanitized

def download_content_sync(url: str, download_dir: str, filename_prefix: str, timeout: int = 30) -> bool:
    """
    Downloads content from a URL synchronously and saves it to a directory.

    Args:
        url: The URL to download from.
        download_dir: The directory to save the file in.
        filename_prefix: A prefix (usually derived from title) for the filename.
        timeout: Request timeout in seconds.

    Returns:
        True if download and save were successful, False otherwise.
    """
    download_logger = logger.bind(task="download_sync", url=url)
    download_logger.info(f"Attempting download...")

    try:
        headers = {'User-Agent': get_random_user_agent()} # Use helper if available, or hardcode one
        # Use stream=True to handle potentially large files without loading all into memory
        with requests.get(url, headers=headers, stream=True, timeout=timeout, allow_redirects=True) as response:
            response.raise_for_status() # Raise HTTPError for bad responses (4xx or 5xx)

            # --- Determine Filename ---
            content_type = response.headers.get('content-type', '').split(';')[0].strip()
            content_disposition = response.headers.get('content-disposition')
            final_filename = None

            # 1. Try Content-Disposition header
            if content_disposition:
                disp_parts = content_disposition.split(';')
                for part in disp_parts:
                    if part.strip().lower().startswith('filename='):
                        final_filename = part.split('=', 1)[1].strip().strip('"')
                        # Sanitize filename from header, using prefix as fallback base
                        final_filename = sanitize_filename(final_filename or filename_prefix)
                        download_logger.debug(f"Using filename from Content-Disposition: {final_filename}")
                        break

            # 2. If no Content-Disposition filename, generate from prefix and type/URL
            if not final_filename:
                # Guess extension from content-type
                extension = mimetypes.guess_extension(content_type) if content_type else None
                # If no extension from type, try getting from URL path
                if not extension:
                     try:
                         parsed_url = urlparse(url)
                         path_part = parsed_url.path
                         _, potential_ext = os.path.splitext(path_part)
                         if potential_ext and len(potential_ext) < 10: # Basic check for valid-looking extension
                             extension = potential_ext
                     except Exception: pass # Ignore errors parsing URL path extension

                # Fallback extension
                if not extension:
                    if 'html' in content_type: extension = '.html'
                    elif 'pdf' in content_type: extension = '.pdf'
                    elif 'xml' in content_type: extension = '.xml'
                    elif 'json' in content_type: extension = '.json'
                    elif 'plain' in content_type: extension = '.txt'
                    else: extension = '.download' # Generic fallback

                # Combine sanitized prefix and extension
                final_filename = f"{filename_prefix}{extension}"
                download_logger.debug(f"Generated filename: {final_filename} (Type: {content_type}, Ext: {extension})")

            # --- Save File ---
            save_path = os.path.join(download_dir, final_filename)
            download_logger.info(f"Saving content to: {save_path}")

            # Create directory if it doesn't exist (should already exist from _trigger_web_search)
            os.makedirs(download_dir, exist_ok=True)

            # Write content in chunks
            chunk_count = 0
            total_bytes = 0
            with open(save_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk: # filter out keep-alive new chunks
                        f.write(chunk)
                        chunk_count += 1
                        total_bytes += len(chunk)

            download_logger.success(f"Download complete. Saved {total_bytes} bytes in {chunk_count} chunks.")
            return True

    except requests.exceptions.Timeout:
        download_logger.error(f"Request timed out after {timeout} seconds.")
        return False
    except requests.exceptions.RequestException as e:
        download_logger.error(f"Download failed: {e}")
        return False
    except IOError as e:
        download_logger.error(f"File saving failed: {e}")
        # Attempt to clean up partially written file
        if 'save_path' in locals() and os.path.exists(save_path):
            try: os.remove(save_path); download_logger.warning("Removed partial file after save error.")
            except Exception as rm_err: download_logger.error(f"Failed to remove partial file: {rm_err}")
        return False
    except Exception as e:
        download_logger.error(f"An unexpected error occurred during download: {e}")
        logger.exception("Download Unexpected Error Traceback:") # Log full traceback for unexpected errors
        return False

# --- Global SSE Notification Queue ---
# A thread-safe queue to hold messages that need to be pushed to the client.
sse_notification_queue = queue.Queue()
# Helper to format SSE data (can be reused)
def format_sse(data: Dict[str, Any], event_type: Optional[str] = None) -> str:
    """Formats data as a Server-Sent Event string."""
    json_data = json.dumps(data)
    sse_string = f"data: {json_data}\n"
    if event_type:
        sse_string = f"event: {event_type}\n{sse_string}"
    return sse_string + "\n"


def format_sse_notification(data: dict, event: str = None) -> str:
    """Formats a dictionary into a Server-Sent Event string for the notification stream."""
    msg = f"data: {json.dumps(data)}\n"
    if event:
        msg = f"event: {event}\n{msg}"
    return f"{msg}\n"



# --- ollama helper function formatting


def _format_ollama_chat_response_nonstream(
    response_text: str,
    model_name: str,
    total_duration_ns: int,
    eval_duration_ns: int
) -> Dict[str, Any]:
    """
    Formats a complete response into the Ollama non-streaming JSON structure.
    """
    return {
        "model": model_name,
        "created_at": datetime.datetime.now(datetime.timezone.utc).isoformat(),
        "message": {
            "role": "assistant",
            "content": response_text
        },
        "done": True,
        "total_duration": total_duration_ns,
        "load_duration": 1,  # Placeholder, not measured
        "prompt_eval_count": 0,  # Placeholder, not measured
        "prompt_eval_duration": 1,  # Placeholder, not measured
        "eval_count": 0,  # Placeholder, not measured
        "eval_duration": eval_duration_ns,
    }


def _ollama_pseudo_stream_sync_generator(
        full_response_text: str,
        model_name: str,
        total_duration_ns: int,
        eval_duration_ns: int
):
    """
    Takes a completed text string and yields it word-by-word in the
    Ollama-compatible streaming SSE format. This version is robustly designed
    to prevent client-side 'reading content of undefined' errors.
    """
    logger.info(f"OLLAMA_STREAM_V3: Streaming pre-generated text ({len(full_response_text)} chars).")

    try:
        # --- Defensive Check for Empty or Invalid Input ---
        # If the AI somehow produced no text, send a single valid message and finish.
        if not full_response_text or not full_response_text.strip():
            logger.warning("OLLAMA_STREAM_V3: The generated response was empty. Sending a placeholder.")
            full_response_text = "[Empty Response]"

        # --- Stream the content word by word ---
        words = full_response_text.split(' ')
        for i, word in enumerate(words):
            # The content for this chunk is the word plus a space (unless it's the last word)
            chunk_content = word + (' ' if i < len(words) - 1 else '')

            chunk = {
                "model": model_name,
                "created_at": datetime.datetime.now(datetime.timezone.utc).isoformat(),
                "message": {
                    "role": "assistant",
                    "content": chunk_content
                },
                "done": False
            }
            yield json.dumps(chunk) + "\n"
            time.sleep(0.02)  # Optional delay for smoother appearance

        # --- Send the Final "Done" Chunk ---
        # This chunk is critical. It must contain "done: true" and, to satisfy
        # buggy clients, it should also contain a final, empty "message" object.
        final_chunk = {
            "model": model_name,
            "created_at": datetime.datetime.now(datetime.timezone.utc).isoformat(),
            "message": {
                "role": "assistant",
                "content": ""
            },
            "done": True,
            "total_duration": total_duration_ns,
            "load_duration": 1,
            "prompt_eval_count": 0,
            "prompt_eval_duration": 1,
            "eval_count": 0,
            "eval_duration": eval_duration_ns,
        }
        yield json.dumps(final_chunk) + "\n"

    except GeneratorExit:
        logger.warning("OLLAMA_STREAM_V3: Client disconnected from stream.")
    except Exception as e:
        logger.exception("OLLAMA_STREAM_V3: Unhandled exception during stream generation.")
        # Attempt to send a final error message if an exception occurs mid-stream
        try:
            error_chunk = {
                "model": model_name,
                "created_at": datetime.datetime.now(datetime.timezone.utc).isoformat(),
                "done": True,
                "error": f"Server-side streaming error: {type(e).__name__}"
            }
            yield json.dumps(error_chunk) + "\n"
        except:
            pass  # The generator may already be closed
    finally:
        logger.info("OLLAMA_STREAM_V3: Finished sending all chunks.")


# --- OpenAI Response Formatting Helpers ---


# ----- Mesh Logic Helper functions ---- (Duplicates but it serves or have different logic than the ordinary pipeline it's more low priority)


async def _log_sanitized_interaction(
        original_query: Any,
        final_response: Any,
        session_id: str,
        mode: str
):
    """
    FIRE-AND-FORGET PRIVACY HELPER.
    This function runs in the background and does not block or return to the user.
    It takes the raw query/response from an interaction, uses an LLM to sanitize it,
    and saves the anonymized record to the database for safe, long-term learning.
    """
    request_id = session_id.replace("mesh_task_", "")  # Get a base ID for logging
    log_prefix = f"SANITIZE_LOG|{request_id}"
    db: Optional[Session] = None

    try:
        # Step 1: Create a new, independent database session for this background task.
        db = SessionLocal()
        if not db:
            raise RuntimeError("DB session failed for sanitizer background task.")

        logger.info(f"{log_prefix}: Starting sanitization for session {session_id}, mode '{mode}'.")

        # Step 2: Convert the raw query/response into simple text for the LLM.
        # This handles the different data types from different services.
        query_text = ""
        response_text = ""

        if mode in ["chat_mesh", "vlm_chat_mesh"]:
            # For chat, the query is the last user message content.
            if isinstance(original_query, dict) and "content" in original_query:
                # Handle complex content (text + image parts)
                if isinstance(original_query["content"], list):
                    query_text = " ".join(
                        [part.get("text", "") for part in original_query["content"] if part.get("type") == "text"])
                    query_text += " [User also provided an image]"
                else:
                    query_text = str(original_query.get("content", ""))
            else:
                query_text = str(original_query)
            response_text = str(final_response)

        elif mode == "asr_mesh":
            query_text = f"[Audio Transcription Request for file: {original_query.get('filename', 'unknown')}]"
            response_text = str(final_response)

        elif mode == "tts_mesh":
            query_text = str(original_query)
            response_text = str(final_response)  # e.g., "[Audio generated successfully]"

        elif mode == "image_gen_mesh":
            query_text = str(original_query)
            response_text = str(final_response)  # e.g., "[1 image(s) generated successfully]"

        # Step 3: Call the LLM to perform the sanitization.
        # This is an ELP0 task. We use `direct_generate` as a convenient way to get a response.
        logger.debug(f"{log_prefix}: Calling LLM with sanitization prompt.")
        sanitizer_prompt = PROMPT_SANITIZE_FOR_LOGGING.format(
            original_query_text=query_text,
            original_response_text=response_text
        )

        sanitization_session_id = f"sanitize_{session_id}"

        # This `direct_generate` call will create its own temporary log in the DB, which is fine.
        # The important record is the one we save *after* this step.
        sanitized_json_str = await cortex_text_interaction.direct_generate(
            db, sanitizer_prompt, sanitization_session_id
        )

        # Step 4: Parse the sanitized result and prepare for saving.
        sanitized_query_to_save = query_text  # Fallback to original if parsing fails
        sanitized_response_to_save = response_text  # Fallback

        try:
            # The robust JSON extractors are in the CortexThoughts class
            json_candidate = cortex_text_interaction._extract_json_candidate_string(sanitized_json_str, log_prefix)
            if json_candidate:
                sanitized_data = cortex_text_interaction._programmatic_json_parse_and_fix(json_candidate,
                                                                                          log_prefix=log_prefix)
                if sanitized_data:
                    sanitized_query_to_save = sanitized_data.get("sanitized_query", sanitized_query_to_save)
                    sanitized_response_to_save = sanitized_data.get("sanitized_response", sanitized_response_to_save)
                    logger.info(f"{log_prefix}: Sanitization successful.")
        except Exception as e_parse:
            logger.warning(
                f"{log_prefix}: Failed to parse sanitizer JSON response. Logging will use less-sanitized data. Error: {e_parse}")

        # Step 5: Save the final, sanitized record to the database for learning.
        add_interaction(
            db,
            session_id=session_id,
            mode=mode,  # e.g., "chat_mesh", "asr_mesh"
            input_type="sanitized_learning_record",
            user_input=sanitized_query_to_save,
            llm_response=sanitized_response_to_save,
            classification="community_mesh_interaction"
        )
        db.commit()
        logger.success(f"{log_prefix}: Successfully logged sanitized interaction for session {session_id}.")

    except Exception as e:
        logger.error(f"{log_prefix}: A critical error occurred during the sanitization and logging task: {e}")
        logger.exception(f"{log_prefix} Traceback:")
        if db:
            db.rollback()
    finally:
        if db:
            db.close()


async def _process_mesh_chat_request_and_get_result(
        request_data: dict,
        db_session: Session,
        session_id: str,
        request_id: str
) -> str:
    """
    Handles the full ELP0 Chat/VLM pipeline for a mesh request and returns the final text result.
    This function simulates the core logic of `background_generate` but is designed to
    be awaited and return a final value. All sub-tasks are run at ELP0 priority.
    """
    log_prefix = f"MESH_CHAT_PROCESSOR|{request_id}"
    logger.info(f"{log_prefix}: Starting ELP0 Chat/VLM processing for session {session_id}.")

    # --- Step 1: Parse and Validate Input ---
    messages = request_data.get("messages", [])
    if not messages:
        raise ValueError("'messages' key is required for a chat request.")

    if not cortex_text_interaction:
        raise RuntimeError("Critical: cortex_text_interaction instance not available.")
    cortex_text_interaction.current_session_id = session_id

    user_input_text = ""
    image_b64 = None
    last_user_msg = next((msg for msg in reversed(messages) if msg.get("role") == "user"), None)

    if not last_user_msg:
        raise ValueError("No message with role 'user' found in the request.")

    content = last_user_msg.get("content")
    if isinstance(content, str):
        user_input_text = content
    elif isinstance(content, list):
        for part in content:
            if part.get("type") == "text":
                user_input_text += part.get("text", "")
            elif part.get("type") == "image_url":
                image_b64 = part.get("image_url", {}).get("url", "").split(",", 1)[-1]

    if not user_input_text and not image_b64:
        raise ValueError("No text or image content was provided in the user message.")

    # If an image is present, get its VLM description to augment the text input
    current_input_for_analysis = user_input_text
    if image_b64:
        logger.info(f"{log_prefix}: Image detected. Getting VLM description at ELP0 priority.")
        vlm_desc, vlm_err = await cortex_text_interaction._describe_image_async(
            db_session, session_id, image_b64, "mesh_request_vlm_analysis", ELP0
        )
        if vlm_err:
            logger.warning(f"{log_prefix}: VLM description failed: {vlm_err}. Proceeding with text only.")
        elif vlm_desc:
            current_input_for_analysis = f"[Image Context: {vlm_desc}]\n\nUser Query: {user_input_text or '(No text query)'}"

    # --- Step 2: Execute the ELP0 Deep Reasoning Pipeline ---
    logger.info(f"{log_prefix}: Executing the full, sequential ELP0 pipeline to get a result.")

    # 2.1: Gather Comprehensive Context (at ELP0)
    logger.debug(f"{log_prefix}: Gathering comprehensive RAG context at ELP0...")
    wrapped_rag_res = await asyncio.to_thread(
        cortex_text_interaction._get_rag_retriever_thread_wrapper,
        db_session,
        current_input_for_analysis,
        ELP0  # <--- CRITICAL: Run RAG at low priority
    )
    if wrapped_rag_res.get("status") != "success":
        raise RuntimeError(f"RAG retrieval failed: {wrapped_rag_res.get('error_message')}")

    url_ret, sess_hist_ret, refl_chunk_ret, _ = wrapped_rag_res.get("data", (None, None, None, None))

    url_docs, session_docs, reflection_docs = [], [], []
    if url_ret: url_docs = await asyncio.to_thread(url_ret.invoke, current_input_for_analysis)
    if sess_hist_ret: session_docs = await asyncio.to_thread(sess_hist_ret.invoke, current_input_for_analysis)
    if refl_chunk_ret: reflection_docs = await asyncio.to_thread(refl_chunk_ret.invoke, current_input_for_analysis)

    url_context_str = cortex_text_interaction._format_docs(url_docs, "URL Context")
    history_rag_str = cortex_text_interaction._format_docs(session_docs + reflection_docs, "History/Reflection RAG")
    # ... (Other context gathering like direct history, logs, etc.)
    direct_hist_interactions = await asyncio.to_thread(get_global_recent_interactions, db_session, limit=5)
    recent_direct_history_str = cortex_text_interaction._format_direct_history(direct_hist_interactions)

    # 2.2: Perform Routing to find the best specialist model (at ELP0)
    logger.debug(f"{log_prefix}: Routing to specialist model at ELP0...")
    # Build the full payload the router expects
    router_payload = {
        "input": current_input_for_analysis,
        "recent_direct_history": recent_direct_history_str,
        "context": url_context_str,
        "history_rag": history_rag_str,
        "file_index_context": "N/A for this endpoint",  # Or implement if needed
        "log_context": "N/A for this endpoint",
        "emotion_analysis": "N/A for this endpoint",
        "pending_tot_result": "N/A for this endpoint",
        "imagined_image_vlm_description": "N/A for this endpoint"
    }

    role, query, reason = await cortex_text_interaction._route_to_specialist(
        db_session, session_id, current_input_for_analysis, router_payload
    )

    # 2.3: Call the chosen Specialist Model and the Corrector (at ELP0)
    logger.debug(f"{log_prefix}: Generating draft response with '{role}' model at ELP0...")
    specialist_model = cortex_text_interaction.provider.get_model(role)
    if not specialist_model:
        raise ValueError(f"Specialist model '{role}' not found after routing.")

    specialist_payload = router_payload.copy()
    specialist_payload["input"] = query  # Use the (potentially refined) query from the router
    specialist_chain = (cortex_text_interaction.text_prompt_template | specialist_model | StrOutputParser())
    timing_data_specialist = {"session_id": session_id, "mode": f"mesh_chat_specialist_{role}"}

    draft_response = await asyncio.to_thread(
        cortex_text_interaction._call_llm_with_timing,
        specialist_chain,
        specialist_payload,
        timing_data_specialist,
        priority=ELP0  # <--- CRITICAL: Run specialist at low priority
    )

    logger.debug(f"{log_prefix}: Correcting draft response at ELP0...")
    final_analyzed_text = await cortex_text_interaction._correct_response(
        db_session, session_id, current_input_for_analysis, specialist_payload, draft_response
    )

    # Note: We are intentionally omitting the multi-chunk elaboration loop and ToT spawning
    # for this "wait for result" path to ensure it completes in a reasonable timeframe.

    logger.success(f"{log_prefix}: ELP0 Chat/VLM processing complete.")

    # --- Step 3: Return the Final, Unsanitized Text ---
    return final_analyzed_text


async def _process_mesh_tts_request_and_get_result(
        request_data: dict,
        request_id: str
) -> Tuple[bytes, str]:
    """
    Handles the full ELP0 Text-to-Speech pipeline for a mesh request and returns the
    final audio bytes and its MIME type.

    This function is designed to be called and awaited by the main mesh processor route.
    All worker calls within this function are executed with ELP0 priority.
    """
    log_prefix = f"MESH_TTS_PROCESSOR|{request_id}"
    logger.info(f"{log_prefix}: Starting ELP0 TTS processing.")

    # --- Step 1: Parse and Validate Input ---
    # This logic is copied and adapted from the main `handle_openai_tts` handler.
    input_text = request_data.get("input")
    model_requested = request_data.get("model")  # Logged for consistency, but not used.
    voice_requested = request_data.get("voice")
    response_format_requested = request_data.get("response_format", "mp3").lower()

    logger.debug(
        f"{log_prefix}: TTS Request Parsed - Input: '{str(input_text)[:50]}...', "
        f"Voice: {voice_requested}, Format: {response_format_requested}"
    )

    if not input_text or not isinstance(input_text, str):
        raise ValueError("'input' field is required and must be a non-empty string for TTS.")
    if not voice_requested or not isinstance(voice_requested, str):
        raise ValueError("'voice' field (MeloTTS speaker ID, e.g., EN-US) is required for TTS.")

    # --- Step 2: Prepare the Audio Worker Command ---
    # Determine the language for the MeloTTS model from the voice ID.
    melo_language = "EN"  # Default to English
    try:
        lang_part = voice_requested.split('-')[0].upper()
        supported_melo_langs = ["EN", "ZH", "JP", "ES", "FR", "KR", "DE"]
        if lang_part in supported_melo_langs:
            melo_language = lang_part
    except Exception:
        logger.warning(f"{log_prefix}: Could not infer language from voice '{voice_requested}'. Defaulting to EN.")

    # Construct the command to execute the worker script.
    audio_worker_script_path = os.path.join(SCRIPT_DIR, "audio_worker.py")
    if not os.path.exists(audio_worker_script_path):
        raise RuntimeError(f"Audio worker script is missing at {audio_worker_script_path}")

    temp_audio_dir = os.path.join(SCRIPT_DIR, "temp_audio_worker_files")
    # No need to `await` this makedirs call as the main handler already did it,
    # but it's safe to include for robustness.
    os.makedirs(temp_audio_dir, exist_ok=True)

    worker_command = [
        APP_PYTHON_EXECUTABLE, audio_worker_script_path,
        "--task-type", "tts",
        "--model-lang", melo_language,
        "--device", "auto",
        "--model-dir", WHISPER_MODEL_DIR,
        "--temp-dir", temp_audio_dir
    ]
    worker_request_data = {
        "input": input_text,
        "voice": voice_requested,
        "response_format": response_format_requested,
        "request_id": request_id
    }

    # --- Step 3: Execute the Worker with ELP0 Priority and Wait for the Result ---
    logger.info(f"{log_prefix}: Executing audio worker with ELP0 priority and waiting for completion...")

    # This `await` call will hold the connection open while the audio is generated.
    # The task itself runs at low priority and can be interrupted by ELP1 tasks.
    parsed_response_from_worker, error_string_from_worker = await asyncio.to_thread(
        _execute_audio_worker_with_priority,
        worker_command=worker_command,
        request_data=worker_request_data,
        priority=ELP0,  # <--- CRITICAL: Run TTS at low priority
        worker_cwd=SCRIPT_DIR,
        timeout=TTS_WORKER_TIMEOUT
    )

    # --- Step 4: Process the Worker's Response ---
    if error_string_from_worker:
        # If the execution helper returned an error string, raise it.
        raise RuntimeError(f"Audio generation failed: {error_string_from_worker}")

    elif parsed_response_from_worker and "result" in parsed_response_from_worker and "audio_base64" in \
            parsed_response_from_worker["result"]:
        # If the execution was successful, extract the audio data.
        audio_info = parsed_response_from_worker["result"]
        audio_b64_data = audio_info["audio_base64"]
        actual_audio_format = audio_info.get("format", "mp3")
        response_mime_type = audio_info.get("mime_type", f"audio/{actual_audio_format}")

        logger.success(
            f"{log_prefix}: Audio successfully generated. Format: {actual_audio_format}, Length (b64): {len(audio_b64_data)}"
        )

        # Decode the base64 string into raw bytes.
        audio_bytes = base64.b64decode(audio_b64_data)

        # --- Step 5: Return the Final Audio Data ---
        return (audio_bytes, response_mime_type)
    else:
        # Handle cases where the worker exited cleanly but returned an invalid JSON structure.
        raise RuntimeError(f"Audio worker returned an invalid or incomplete response: {parsed_response_from_worker}")


async def _process_mesh_image_gen_request_and_get_result(
        request_data: dict,
        db_session: Session,
        session_id: str,
        request_id: str
) -> List[Dict[str, str]]:
    """
    Handles the full ELP0 Image Generation pipeline for a mesh request and returns
    the final image data list.

    This function is designed to be called and awaited by the main mesh processor route.
    All AI/worker calls within this function are executed with ELP0 priority.
    """
    log_prefix = f"MESH_IMAGE_PROCESSOR|{request_id}"
    logger.info(f"{log_prefix}: Starting ELP0 Image Generation processing for session {session_id}.")

    # --- Step 1: Parse and Validate Input ---
    # This logic is copied and adapted from the main `handle_openai_image_generations` handler.
    prompt_from_user = request_data.get("prompt")
    n_images = int(request_data.get("n", 1))  # Default to 1 image for mesh requests

    if not prompt_from_user or not isinstance(prompt_from_user, str):
        raise ValueError("'prompt' field is required and must be a non-empty string for image generation.")

    if not cortex_text_interaction:
        raise RuntimeError("Critical: cortex_text_interaction instance not available.")
    cortex_text_interaction.current_session_id = session_id

    # --- Step 2: Refine User Prompt using RAG Context at ELP0 ---
    logger.info(f"{log_prefix}: Refining user prompt with RAG context at ELP0 priority...")

    # Get RAG context at low priority
    wrapped_rag_result = await asyncio.to_thread(
        cortex_text_interaction._get_rag_retriever_thread_wrapper,
        db_session,
        prompt_from_user,
        ELP0  # <--- CRITICAL: Run RAG at low priority
    )
    if wrapped_rag_result.get("status") != "success":
        raise RuntimeError(f"RAG for image prompt refinement failed: {wrapped_rag_result.get('error_message')}")

    _, session_hist_retriever, _, _ = wrapped_rag_result.get("data", (None, None, None, None))

    retrieved_history_docs = []
    if session_hist_retriever:
        retrieved_history_docs = await asyncio.to_thread(session_hist_retriever.invoke, prompt_from_user)

    history_rag_str = cortex_text_interaction._format_docs(retrieved_history_docs, source_type="History RAG")

    # Get direct history for additional context
    direct_hist_interactions_list = await asyncio.to_thread(get_global_recent_interactions, db_session, limit=3)
    recent_direct_history_str = cortex_text_interaction._format_direct_history(direct_hist_interactions_list)

    # Call the prompt refiner LLM at low priority
    refined_prompt = await cortex_text_interaction._refine_direct_image_prompt_async(
        db=db_session,
        session_id=session_id,
        user_image_request=prompt_from_user,
        history_rag_str=history_rag_str,
        recent_direct_history_str=recent_direct_history_str,
        priority=ELP0  # <--- CRITICAL: Run prompt refinement at low priority
    )

    if not refined_prompt:
        logger.warning(f"{log_prefix}: Prompt refinement failed. Using original user prompt.")
        refined_prompt = prompt_from_user
    else:
        logger.info(f"{log_prefix}: Using refined prompt for generation: '{refined_prompt[:100]}...'")

    # --- Step 3: Generate Image(s) with ELP0 Priority and Wait for the Result ---
    logger.info(f"{log_prefix}: Requesting {n_images} image(s) from CortexEngine at ELP0 priority...")

    all_generated_image_data = []
    for i in range(n_images):
        logger.debug(f"{log_prefix}: Generating image {i + 1}/{n_images}...")

        # This `await` call will hold the connection open while the image is generated.
        # The task itself runs at low priority.
        list_of_one_image_dict, image_gen_err = await cortex_backbone_provider.generate_image_async(
            prompt=refined_prompt,
            priority=ELP0  # <--- CRITICAL: Run image generation at low priority
        )

        if image_gen_err:
            raise RuntimeError(f"Image generation failed for attempt {i + 1}: {image_gen_err}")
        elif list_of_one_image_dict and list_of_one_image_dict[0]:
            all_generated_image_data.append(list_of_one_image_dict[0])
        else:
            raise RuntimeError(f"Image generation attempt {i + 1} returned no data and no error.")

    if not all_generated_image_data:
        raise RuntimeError("Image generation pipeline failed to produce any valid image data.")

    logger.success(f"{log_prefix}: Successfully generated {len(all_generated_image_data)} image(s).")

    # --- Step 4: Format and Return the Final Image Data ---
    # The expected format is a list of dictionaries, e.g., [{"b64_json": "..."}]
    # We will filter to ensure only the required format is returned.
    response_data_list = []
    for img_data in all_generated_image_data:
        if "b64_json" in img_data:
            response_data_list.append({"b64_json": img_data["b64_json"]})

    return response_data_list

#ASR help all run in ELP0 for mesh network

async def _log_sanitized_asr_interaction(
        raw_transcription: str,
        analyzed_response: str,
        session_id: str,
        request_id: str
):
    """
    FIRE-AND-FORGET HELPER for privacy.
    Takes the raw/final data, sanitizes it, and saves it to the DB.
    Does not block or return anything to the user-facing request.
    """
    log_prefix = f"SANITIZE_LOG|{request_id}"
    db: Optional[Session] = None
    try:
        db = SessionLocal()
        if not db:
            raise RuntimeError("DB session failed for sanitizer.")

        logger.info(f"{log_prefix}: Sanitizing ASR interaction for session {session_id}.")
        sanitizer_prompt = PROMPT_SANITIZE_FOR_LOGGING.format(
            original_query_text="(Audio Transcription)",
            original_response_text=f"RAW_TRANSCRIPT: {raw_transcription}\n\nFINAL_ANALYSIS: {analyzed_response}"
        )
        sanitization_session_id = f"sanitize_{session_id}"

        # Use a `direct_generate` call that DOES NOT SAVE to get the sanitized text.
        # This requires a new helper or a modification to direct_generate.
        # For now, we'll assume a conceptual `generate_text_without_saving` method.
        # Let's use `direct_generate` and just accept it creates a temporary log record.
        sanitized_json_str = await cortex_text_interaction.direct_generate(db, sanitizer_prompt,
                                                                           sanitization_session_id)

        sanitized_query = "(Sanitized Audio Transcription)"
        sanitized_response = analyzed_response  # Fallback
        try:
            sanitized_data = json.loads(sanitized_json_str)
            sanitized_query = sanitized_data.get("sanitized_query", sanitized_query)
            sanitized_response = sanitized_data.get("sanitized_response", sanitized_response)
        except (json.JSONDecodeError, TypeError):
            logger.warning(f"{log_prefix}: Failed to parse sanitizer JSON. Logging raw-like data.")

        # Now, save the FINAL, SANITIZED record for long-term learning.
        add_interaction(
            db,
            session_id=session_id,
            mode="asr_mesh",
            input_type="sanitized_audio_transcription",
            user_input=sanitized_query,
            llm_response=sanitized_response,
            classification="sanitized_learning_record"
        )
        db.commit()
        logger.success(f"{log_prefix}: Successfully logged sanitized ASR interaction for session {session_id}.")

    except Exception as e:
        logger.error(f"{log_prefix}: Failed to sanitize and log ASR interaction: {e}")
    finally:
        if db:
            db.close()


async def _process_mesh_asr_request_and_get_result(
        audio_file_path: str,
        language: str,
        session_id: str,
        request_id: str
) -> str:
    """
    Handles the complete, sequential, ELP0 ASR pipeline for a user.
    1. Transcribes audio using raw data.
    2. Performs deep analysis using raw data.
    3. Returns the RAW, unsanitized final result to the user.
    4. Spawns a separate, fire-and-forget task to sanitize and log the interaction for learning.
    """
    log_prefix = f"MESH_ASR_FOR_USER|{request_id}"
    logger.info(f"{log_prefix}: Starting user-facing ELP0 ASR pipeline for file: {audio_file_path}")

    db: Optional[Session] = None
    raw_transcription: str = ""

    # This will hold the final, UNSANITIZED text to be returned to the user.
    final_analyzed_text_for_user: str = "[ASR pipeline did not produce a final analysis]"

    try:
        db = SessionLocal()
        if not db:
            raise RuntimeError("Failed to create DB session for ASR pipeline.")

        # --- STEP 1: Attempt Transcription with Low-Latency Model at ELP0 ---
        logger.info(
            f"{log_prefix}: Step 1 - Attempting Low-Latency ASR at ELP0 (Model: {WHISPER_LOW_LATENCY_MODEL_FILENAME})...")
        asr_worker_script = os.path.join(SCRIPT_DIR, "audio_worker.py")
        temp_audio_dir = os.path.dirname(audio_file_path)

        ll_asr_cmd = [
            APP_PYTHON_EXECUTABLE, asr_worker_script,
            "--task-type", "asr",
            "--model-dir", WHISPER_MODEL_DIR,
            "--temp-dir", temp_audio_dir
        ]
        ll_asr_req_data = {
            "input_audio_path": audio_file_path,
            "whisper_model_name": WHISPER_LOW_LATENCY_MODEL_FILENAME,
            "language": language,
            "request_id": f"{request_id}-mesh-llasr"
        }

        # Execute the low-latency worker process
        ll_asr_response, ll_asr_err = await asyncio.to_thread(
            _execute_audio_worker_with_priority,
            ll_asr_cmd,
            ll_asr_req_data,
            ELP0,
            SCRIPT_DIR,
            ASR_WORKER_TIMEOUT
        )

        # Handle potential critical errors from the worker call itself
        if ll_asr_err:
            logger.warning(
                f"{log_prefix}: Low-latency ASR worker returned an error: {ll_asr_err}. Proceeding to high-quality model as a fallback.")
            raw_transcription = ""  # Force failover by setting transcription to empty
        else:
            # Safely extract the text from the successful response
            raw_transcription = (ll_asr_response.get("result", {}).get("text") or "").strip()

        # --- STEP 2: Sanity Check and Conditional High-Quality Retry at ELP0 ---
        is_transcription_valid = True
        if not raw_transcription or len(
                raw_transcription) < 3 or raw_transcription.lower() in WHISPER_GARBAGE_OUTPUTS:
            is_transcription_valid = False

        if is_transcription_valid:
            logger.info(f"{log_prefix}: Step 2 - Low-latency ASR produced valid text. Using this result.")
            asr_model_used = WHISPER_LOW_LATENCY_MODEL_FILENAME
        else:
            # This block is triggered if the low-latency model failed or produced garbage
            logger.warning(
                f"{log_prefix}: Step 2 - Low-latency ASR produced invalid text ('{raw_transcription}'). Retrying with High-Quality model at ELP0.")

            # Prepare the command for the high-quality model
            hq_asr_cmd = [
                APP_PYTHON_EXECUTABLE, asr_worker_script,
                "--task-type", "asr",
                "--model-dir", WHISPER_MODEL_DIR,
                "--temp-dir", temp_audio_dir
            ]
            hq_asr_req_data = {
                "input_audio_path": audio_file_path,
                "whisper_model_name": WHISPER_DEFAULT_MODEL_FILENAME,  # <-- Use the better model
                "language": language,
                "request_id": f"{request_id}-mesh-hqasr"
            }

            # Execute the high-quality worker process
            hq_asr_response, hq_asr_err = await asyncio.to_thread(
                _execute_audio_worker_with_priority,
                hq_asr_cmd,
                hq_asr_req_data,
                ELP0,
                SCRIPT_DIR,
                ASR_WORKER_TIMEOUT + 120  # Give the slower model more time
            )

            if hq_asr_err:
                logger.error(
                    f"{log_prefix}: High-quality ASR failover also failed: {hq_asr_err}. Aborting pipeline.")
                raw_transcription = ""  # Final result is an empty string, indicating total failure
            else:
                raw_transcription = (hq_asr_response.get("result", {}).get("text") or "").strip()
                asr_model_used = WHISPER_DEFAULT_MODEL_FILENAME
                # Final sanity check on the high-quality output
                if not raw_transcription or len(
                        raw_transcription) < 3 or raw_transcription.lower() in WHISPER_GARBAGE_OUTPUTS:
                    logger.warning(
                        f"{log_prefix}: High-quality ASR also produced invalid speech ('{raw_transcription}'). Final result will be empty.")
                    raw_transcription = ""

        # --- STEP 3: Deep Analysis using RAW transcript ---
        if raw_transcription:
            logger.info(f"{log_prefix}: Step 3 - Submitting RAW transcript for final analysis for the user.")

            # This is the "direct_generate that do not do save" part.
            # We are using the main `direct_generate` function. It WILL create a temporary
            # log record for its own execution, but that's okay. The FINAL learning record
            # will be the sanitized one.
            final_analysis_prompt = f"Provide a detailed analysis and summary of the following transcribed text: \"{raw_transcription}\""

            final_analyzed_text_for_user = await cortex_text_interaction.direct_generate(
                db,
                final_analysis_prompt,
                session_id,
            )

            logger.success(f"{log_prefix}: Deep analysis for user is complete.")
        else:
            final_analyzed_text_for_user = "[Transcription failed or audio was silent after processing.]"

    except Exception as e:
        logger.exception(f"{log_prefix}: A critical error occurred in the user-facing ASR pipeline:")
        final_analyzed_text_for_user = f"[Error in ASR Pipeline: {type(e).__name__}]"

    finally:
        # --- STEP 4: Spawn the Fire-and-Forget Privacy Logging Task ---
        # This happens REGARDLESS of success or failure, to log what happened.
        logger.info(f"{log_prefix}: Spawning background task to sanitize and log the interaction.")
        asyncio.create_task(
            _log_sanitized_asr_interaction(
                raw_transcription=raw_transcription,
                analyzed_response=final_analyzed_text_for_user,
                session_id=session_id,
                request_id=request_id
            )
        )

        # --- STEP 5: Cleanup ---
        # Cleanup can also be moved into the background logging task
        if db:
            db.close()
        if audio_file_path and os.path.exists(audio_file_path):
            try:
                await asyncio.to_thread(os.remove, audio_file_path)
                logger.info(f"{log_prefix}: Cleaned up temporary audio file.")
            except Exception as e_del:
                logger.warning(f"{log_prefix}: Failed to clean up temp audio file: {e_del}")

    # --- STEP 6: Return the RAW, UNSANITIZED result to the user ---
    return final_analyzed_text_for_user

async def _process_mesh_chat_pipeline_elp0(
    messages: list,
    session_id: str,
    request_id: str
):
    """
    Handles the complete, sequential, two-stage ELP0 Chat/VLM pipeline.
    1. Runs an initial `background_generate` for a comprehensive first-pass answer.
    2. Spawns a second `background_generate` for "reinterpretation" or deep reflection on the first pass.
    """
    # All the sequential logic will go inside this function.
    pass

# ---- Helper function method----
def _async_compatible_openai_streamer(
        full_response_text: str,
        model_name: str = "Amaryllis-AdelaidexAlbert-MetacognitionArtificialQuellia-Stream"
):
    """
    A synchronous generator for streaming OpenAI-compatible SSE chunks.
    This version is designed to be returned directly from an async Flask route
    without using `stream_with_context`, making it safe for ASGI servers.
    """
    resp_id = f"chatcmpl-asyncstream-{uuid.uuid4()}"
    timestamp = int(time.time())
    logger.info(f"ASYNC_STREAMER {resp_id}: Streaming pre-generated text ({len(full_response_text)} chars).")

    def yield_chunk(delta_content: Optional[str] = None, role: Optional[str] = None,
                    finish_reason: Optional[str] = None):
        """Helper to format data into the OpenAI SSE chunk structure."""
        delta = {}
        if role: delta["role"] = role
        if delta_content is not None: delta["content"] = delta_content
        chunk_payload = {
            "id": resp_id, "object": "chat.completion.chunk", "created": timestamp,
            "model": model_name, "choices": [{"index": 0, "delta": delta, "finish_reason": finish_reason}]
        }
        return f"data: {json.dumps(chunk_payload)}\n\n"

    try:
        # Yield the initial role chunk
        yield yield_chunk(role="assistant")

        # Handle error messages gracefully
        if "Error:" in full_response_text or "interrupted" in full_response_text.lower():
            yield yield_chunk(delta_content=full_response_text)
            yield yield_chunk(finish_reason="error")
            return

        # Stream the content word by word
        words = full_response_text.split(' ')
        for i, word in enumerate(words):
            chunk_content = word + (' ' if i < len(words) - 1 else '')
            yield yield_chunk(delta_content=chunk_content)
            time.sleep(0.01) # Small delay for typing effect

        # Yield the final stop chunk
        yield yield_chunk(finish_reason="stop")

    except GeneratorExit:
        logger.warning(f"ASYNC_STREAMER {resp_id}: Client disconnected from stream.")
    finally:
        # Always send the [DONE] signal
        yield "data: [DONE]\n\n"
        logger.info(f"ASYNC_STREAMER {resp_id}: Finished sending all chunks and [DONE] signal.")

def _pseudo_stream_sync_generator(
        full_response_text: str,
        model_name: str = "Amaryllis-AdelaidexAlbert-MetacognitionArtificialQuellia-Stream"
):
    """
    Takes a completed text string and yields it word-by-word in the
    OpenAI-compatible SSE format. This function is fully synchronous and
    avoids any asyncio context conflicts with Flask.
    """
    resp_id = f"chatcmpl-syncstream-{uuid.uuid4()}"
    timestamp = int(time.time())
    logger.info(f"SYNC_STREAM {resp_id}: Streaming pre-generated text ({len(full_response_text)} chars).")

    def yield_chunk(delta_content: Optional[str] = None, role: Optional[str] = None,
                    finish_reason: Optional[str] = None):
        """Helper to format data into the OpenAI SSE chunk structure."""
        delta = {}
        if role:
            delta["role"] = role
        if delta_content is not None:
            delta["content"] = delta_content

        chunk_payload = {
            "id": resp_id, "object": "chat.completion.chunk", "created": timestamp,
            "model": model_name,
            "choices": [{"index": 0, "delta": delta, "finish_reason": finish_reason}]
        }
        return f"data: {json.dumps(chunk_payload)}\n\n"

    try:
        yield yield_chunk(role="assistant", delta_content="")

        if "Error:" in full_response_text or "interrupted" in full_response_text.lower():
            logger.error(f"SYNC_STREAM {resp_id}: Streaming an error message provided by the main handler.")
            yield yield_chunk(delta_content=full_response_text)
            yield yield_chunk(finish_reason="error")
            return

        words = full_response_text.split(' ')
        for i, word in enumerate(words):
            chunk_content = word + (' ' if i < len(words) - 1 else '')
            yield yield_chunk(delta_content=chunk_content)
            time.sleep(0.02)

        yield yield_chunk(finish_reason="stop")

    except GeneratorExit:
        logger.warning(f"SYNC_STREAM {resp_id}: Client disconnected from stream.")
    finally:
        yield "data: [DONE]\n\n"
        logger.info(f"SYNC_STREAM {resp_id}: Finished sending all chunks and [DONE] signal.")


def _create_openai_error_response(message: str, err_type="internal_error", code=None, status_code=500):
    """Creates an OpenAI-like error JSON response."""
    error_obj = {
        "message": message,
        "type": err_type,
        "param": None,
        "code": code,
    }
    return {"error": error_obj}, status_code

def _parse_ingested_text_content(data_entry: Dict[str, Any]) -> Tuple[Optional[str], Optional[str], bool]:
    user_input_content, assistant_response_content = None, None
    extracted_successfully = False

    messages = data_entry.get("messages")
    if isinstance(messages, list) and len(messages) >= 1:
        first_user_msg = next((m.get("content") for m in messages if m.get("role") == "user"), None)
        first_asst_msg = next((m.get("content") for m in messages if m.get("role") == "assistant"), None)
        if first_user_msg: user_input_content = first_user_msg
        if first_asst_msg: assistant_response_content = first_asst_msg
        if user_input_content or assistant_response_content: extracted_successfully = True
    elif "prompt" in data_entry and "completion" in data_entry:
        user_input_content = data_entry.get("prompt")
        assistant_response_content = data_entry.get("completion")
        extracted_successfully = True
    elif "user_input" in data_entry and "llm_response" in data_entry:
        user_input_content = data_entry.get("user_input")
        assistant_response_content = data_entry.get("llm_response")
        extracted_successfully = True
    elif "text" in data_entry: # Fallback for generic text entries
        user_input_content = data_entry.get("text")
        assistant_response_content = "[Ingested as single text entry]"
        extracted_successfully = True

    return user_input_content, assistant_response_content, extracted_successfully


def _format_openai_chat_response(response_text: str, model_name: str = "Amaryllis-Adelaide-IdioticRecursiveLearner-LegacyMoEArch") -> Dict[str, Any]:
    """Formats a simple text response into OpenAI ChatCompletion structure."""
    resp_id = f"chatcmpl-{uuid.uuid4()}"
    timestamp = int(time.time())
    return {
        "id": resp_id,
        "object": "chat.completion",
        "created": timestamp,
        "model": model_name,
        "choices": [
            {
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": response_text,
                },
                "finish_reason": "stop", # Assume completion for non-streaming direct response
            }
        ],
        "usage": { # Placeholder token usage
            "prompt_tokens": 0,
            "completion_tokens": 0,
            "total_tokens": 0,
        },
        "system_fingerprint": None,
    }


GENERATION_DONE_SENTINEL = object()


def _stream_openai_chat_response_generator_flask(
        session_id: str,
        user_input: str,
        classification: str,
        image_b64: Optional[str],
        model_name: str = "Amaryllis-AdelaidexAlbert-MetacognitionArtificialQuellia-Stream"
):
    """
    The definitive Server-Sent Events (SSE) generator for Flask. It orchestrates the
    dual-generate process by running the fast ELP1 logic in a background thread while
    streaming results. Features configurable live log streaming or a processing animation,
    and robustly parses the final <think>/<speak> output.
    """
    resp_id = f"chatcmpl-{uuid.uuid4()}"
    timestamp = int(time.time())
    logger.debug(f"FLASK_STREAM_V3 {resp_id}: Starting definitive generator for session {session_id}")

    message_queue = queue.Queue()
    background_thread: Optional[threading.Thread] = None
    final_result_data = {
        "text": "Error: Generation failed to return a result from the background thread.",
        "finish_reason": "error",
        "error": None
    }
    sink_id_holder = [None]  # Use a list to pass the sink ID by reference

    def yield_chunk(delta_content: Optional[str] = None, role: Optional[str] = None,
                    finish_reason: Optional[str] = None):
        """Helper to format data into the OpenAI SSE chunk structure."""
        delta = {}
        if role: delta["role"] = role
        if delta_content is not None: delta["content"] = delta_content
        chunk_payload = {
            "id": resp_id, "object": "chat.completion.chunk", "created": timestamp,
            "model": model_name, "choices": [{"index": 0, "delta": delta, "finish_reason": finish_reason}]
        }
        return f"data: {json.dumps(chunk_payload)}\n\n"

    def run_async_generate_in_thread(
            q: queue.Queue,
            sess_id: str,
            u_input: str,
            classi_param: str,
            img_b64_param: Optional[str]
    ):
        """
        Target function for the background thread. Runs the async direct_generate logic
        in a new event loop, captures logs, and puts the final result onto the queue.
        """
        log_session_id = f"{sess_id}-{threading.get_ident()}"
        thread_final_text = "Error: Processing failed within the background thread."
        thread_final_reason = "error"
        thread_final_error_obj = None
        db_session: Optional[Session] = None

        try:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)

            def log_sink(message):
                record = message.record
                if record["extra"].get("request_session_id") == log_session_id:
                    log_entry = f"[{record['time'].strftime('%H:%M:%S.%f')[:-3]} {record['level'].name}] {record['message']}"
                    try:
                        q.put_nowait(("LOG", log_entry))
                    except queue.Full:
                        pass

            sink_id_holder[0] = logger.add(log_sink, level=LOG_SINK_LEVEL, format=LOG_SINK_FORMAT,
                                           filter=lambda r: r["extra"].get("request_session_id") == log_session_id)

            async def run_generate_with_logging_inner():
                nonlocal db_session, thread_final_text, thread_final_reason, thread_final_error_obj
                try:
                    db_session = SessionLocal()
                    if not db_session: raise RuntimeError("Failed to create DB session for direct_generate.")

                    with logger.contextualize(request_session_id=log_session_id):
                        # Use the global cortex_text_interaction instance
                        result_text = await cortex_text_interaction.direct_generate(
                            db=db_session, user_input=u_input, session_id=sess_id,
                            vlm_description=None, image_b64=img_b64_param
                        )
                        thread_final_text = result_text
                        thread_final_reason = "stop"
                except Exception as e:
                    with logger.contextualize(request_session_id=log_session_id):
                        logger.error(f"Async direct_generate task EXCEPTION: {e}", exc_info=True)
                    thread_final_error_obj = e
                    thread_final_text = f"[Error during direct generation for streaming: {type(e).__name__} - {e}]"
                    thread_final_reason = "error"
                finally:
                    if db_session:
                        db_session.close()

            loop.run_until_complete(run_generate_with_logging_inner())

        except Exception as e:
            thread_final_error_obj = e
            thread_final_text = f"Error in background thread setup: {e}"
            thread_final_reason = "error"
        finally:
            q.put(("RESULT", (thread_final_text, thread_final_reason, thread_final_error_obj)))
            q.put(GENERATION_DONE_SENTINEL)
            if sink_id_holder[0] is not None:
                logger.remove(sink_id_holder[0])
            loop.close()

    # --- Main Generator Logic (Runs in the Flask Request Thread) ---
    try:
        background_thread = threading.Thread(
            target=run_async_generate_in_thread,
            args=(message_queue, session_id, user_input, classification, image_b64),
            daemon=True
        )
        background_thread.start()

        yield yield_chunk(role="assistant", delta_content="<think>\n")

        processing_complete = False
        result_received = False
        animation_frame = 0

        while not processing_complete:
            try:
                queue_item = message_queue.get(timeout=STREAM_ANIMATION_DELAY_SECONDS)

                if queue_item is GENERATION_DONE_SENTINEL:
                    processing_complete = True
                    continue

                message_type, message_data = queue_item
                if message_type == "LOG":
                    if STREAM_INTERNAL_LOGS:
                        yield yield_chunk(delta_content=message_data + "\n")
                elif message_type == "RESULT":
                    result_received = True
                    final_result_data["text"], final_result_data["finish_reason"], final_result_data[
                        "error"] = message_data

            except queue.Empty:
                if not STREAM_INTERNAL_LOGS and not result_received:
                    char = STREAM_ANIMATION_CHARS[animation_frame]
                    yield yield_chunk(delta_content=f"{char} ")
                    animation_frame = (animation_frame + 1) % len(STREAM_ANIMATION_CHARS)

            if not processing_complete and not background_thread.is_alive():
                logger.error(f"FLASK_STREAM_V3 {resp_id}: Background thread died unexpectedly.")
                if not result_received:
                    final_result_data["error"] = RuntimeError("Background thread failed.")
                    final_result_data["text"] = "[Critical Error: Background processing failed prematurely]"
                processing_complete = True

        yield yield_chunk(delta_content="\nLog stream complete.\n</think>\n\n")

        final_text_to_stream = final_result_data["text"]
        final_reason_to_send = final_result_data["finish_reason"]
        if final_result_data["error"]: final_reason_to_send = "error"

        # Use the robust Think/Speak parser on the final result
        _, speak_content = cortex_text_interaction._parse_think_speak_output(final_text_to_stream)

        if speak_content:
            logger.info(f"FLASK_STREAM_V3 {resp_id}: Streaming final 'speak' content ({len(speak_content)} chars).")
            words = speak_content.split(' ')
            for i, word in enumerate(words):
                yield yield_chunk(delta_content=word + (' ' if i < len(words) - 1 else ''))
                time.sleep(0.01)
        else:
            logger.warning(f"FLASK_STREAM_V3 {resp_id}: Final speak content is empty. Reporting error to client.")
            yield yield_chunk(delta_content="[System Error: Failed to generate a valid response.]")
            final_reason_to_send = "error"

        yield yield_chunk(finish_reason=final_reason_to_send)
        yield "data: [DONE]\n\n"

    except GeneratorExit:
        logger.warning(f"FLASK_STREAM_V3 {resp_id}: Client disconnected from stream.")
    except Exception as e:
        logger.error(f"FLASK_STREAM_V3 {resp_id}: Unhandled error in streaming generator: {e}", exc_info=True)
        yield yield_chunk(delta_content=f"\n\n[STREAMING ORCHESTRATION ERROR: {e}]", finish_reason="error")
        yield "data: [DONE]\n\n"
    finally:
        logger.debug(f"FLASK_STREAM_V3 {resp_id}: Generator function fully finished.")


@contextlib.contextmanager
def managed_webdriver(no_images=False):
    """Context manager for initializing and quitting the WebDriver (synchronous)."""
    # Use specific logger
    wd_logger = logger.bind(task="webdriver")
    driver = None
    service = None
    if not SELENIUM_AVAILABLE:
         wd_logger.error("Selenium not available, cannot create WebDriver.")
         yield None # Yield None if Selenium couldn't be imported
         return

    try:
        wd_logger.info("Initializing WebDriver (Chrome)...")
        options = webdriver.ChromeOptions()
        # Try to make it appear less automated
        options.add_argument("--disable-blink-features=AutomationControlled")
        options.add_experimental_option("excludeSwitches", ["enable-automation"])
        options.add_experimental_option('useAutomationExtension', False)
        # Headless option - uncomment if desired, but CAPTCHAs might be harder
        # options.add_argument("--headless")
        # options.add_argument("--window-size=1920,1080")
        options.add_argument("--log-level=3") # Reduce browser console noise

        if no_images:
            wd_logger.info("Disabling image loading.")
            # Preferences to disable images
            prefs = {"profile.managed_default_content_settings.images": 2}
            options.add_experimental_option("prefs", prefs)

        try:
             # Use webdriver-manager to automatically handle driver download/update
             wd_logger.debug("Installing/updating ChromeDriver via webdriver-manager...")
             service = ChromeService(ChromeDriverManager().install())
             wd_logger.debug("ChromeDriver service ready.")
             driver = webdriver.Chrome(service=service, options=options)
             # Set user agent after driver is created
             # driver.execute_cdp_cmd('Network.setUserAgentOverride', {"userAgent": get_random_user_agent()}) # Needs helper
             wd_logger.success("WebDriver initialized successfully.")
             yield driver # Provide the driver instance to the 'with' block
        except Exception as setup_exc:
             wd_logger.error(f"WebDriver Initialization Failed: {setup_exc}")
             wd_logger.exception("WebDriver Setup Traceback:")
             # If setup fails, yield None so the caller can handle it
             yield None
             return # Exit context manager if setup failed

    finally:
        # This block executes when exiting the 'with' statement
        if driver:
            wd_logger.info("Shutting down WebDriver...")
            try:
                driver.quit()
                wd_logger.success("WebDriver shut down.")
            except Exception as quit_exc:
                wd_logger.error(f"Error shutting down WebDriver: {quit_exc}")
        # Service doesn't usually need explicit stopping if driver.quit() works
        # if service and service.process:
        #    service.stop()
        #    wd_logger.info("ChromeDriver service stopped.")
    
def get_random_user_agent():
    """Returns a random User-Agent string."""
    return random.choice(USER_AGENTS)

def _format_legacy_completion_response(response_text: str, model_name: str = META_MODEL_NAME_NONSTREAM) -> Dict[str, Any]:
    """Formats a simple text response into the legacy OpenAI Completion structure."""
    resp_id = f"cmpl-{uuid.uuid4()}" # Different prefix often used for legacy
    timestamp = int(time.time())
    return {
        "id": resp_id,
        "object": "text_completion", # Legacy object type
        "created": timestamp,
        "model": model_name, # Use the non-streaming meta model name
        "choices": [
            {
                "text": response_text, # The generated text
                "index": 0,
                "logprobs": None, # Not supported here
                "finish_reason": "stop", # Assume stop if successful
            }
        ],
        "usage": { # Placeholder token usage
            "prompt_tokens": 0,
            "completion_tokens": 0,
            "total_tokens": 0,
        }
    }


def _execute_audio_worker_with_priority(
        worker_command: list[str],
        request_data: Dict[str, Any],
        priority: int,
        worker_cwd: str,
        timeout: int = 120
) -> Tuple[Optional[Dict[str, Any]], Optional[str]]:
    # Ensure cortex_backbone_provider and its _priority_quota_lock are accessible.
    # This might be self.cortex_backbone_provider if this function is part of a class that has it,
    # or a global cortex_backbone_provider instance. For this example, assuming global cortex_backbone_provider.
    # If cortex_backbone_provider is an instance variable (e.g., self.cortex_backbone_provider), adjust accordingly.
    global cortex_backbone_provider  # Assuming cortex_backbone_provider is a global instance initialized elsewhere

    shared_priority_lock: Optional[PriorityQuotaLock] = getattr(cortex_backbone_provider, '_priority_quota_lock', None)

    request_id = request_data.get("request_id", "audio-worker-unknown")
    log_prefix = f"AudioExec|ELP{priority}|{request_id}"
    logger.debug(f"{log_prefix}: Attempting to execute audio worker.")

    if not shared_priority_lock:
        logger.error(f"{log_prefix}: Shared PriorityQuotaLock not available/initialized! Cannot run audio worker.")
        return None, "Shared resource lock not available."

    lock_acquired = False
    worker_process = None  # Initialize to None
    start_lock_wait = time.monotonic()
    logger.debug(f"{log_prefix}: Acquiring shared resource lock (Priority: ELP{priority})...")

    # Assuming acquire method exists and works as previously discussed
    lock_acquired = shared_priority_lock.acquire(priority=priority, timeout=None)
    lock_wait_duration = time.monotonic() - start_lock_wait

    if lock_acquired:
        logger.info(f"{log_prefix}: Lock acquired (waited {lock_wait_duration:.2f}s). Starting audio worker.")
        try:
            start_time = time.monotonic()
            worker_process = subprocess.Popen(
                worker_command,
                stdin=subprocess.PIPE,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True,
                encoding='utf-8',
                errors='replace',  # Handle potential encoding errors in output
                cwd=worker_cwd
            )

            # If ELP0, register process with lock for potential interruption
            if priority == ELP0 and worker_process:
                if hasattr(shared_priority_lock, 'set_holder_process'):
                    shared_priority_lock.set_holder_process(worker_process)
                else:
                    logger.warning(
                        f"{log_prefix}: Lock does not have set_holder_process method. ELP0 process cannot be registered for interruption by this lock instance.")

            input_json = json.dumps(request_data)
            logger.debug(f"{log_prefix}: Sending input JSON (len={len(input_json)}) to audio worker stdin...")
            stdout_data, stderr_data = "", ""  # Initialize

            try:
                stdout_data, stderr_data = worker_process.communicate(input=input_json, timeout=timeout)
                logger.debug(f"{log_prefix}: Audio worker communicate() finished.")
            except subprocess.TimeoutExpired:
                logger.error(f"{log_prefix}: Audio worker process timed out after {timeout}s.")
                if worker_process and worker_process.poll() is None: worker_process.kill()  # Ensure kill on timeout
                # Try to get final outputs after kill
                try:
                    stdout_data, stderr_data = worker_process.communicate()
                except:
                    pass  # Best effort
                logger.error(f"{log_prefix}: Worker timed out. Stderr: {stderr_data.strip() if stderr_data else 'N/A'}")
                return None, "Audio worker process timed out."
            except BrokenPipeError:
                logger.warning(
                    f"{log_prefix}: Broken pipe with audio worker. Likely interrupted by higher priority task.")
                if worker_process and worker_process.poll() is None:  # Check if process exists and is running
                    try:
                        worker_process.wait(timeout=0.5)  # Brief wait
                    except subprocess.TimeoutExpired:
                        worker_process.kill()  # Force kill if wait times out
                # Attempt to get any remaining output after ensuring process is dealt with
                try:
                    stdout_data_bp, stderr_data_bp = "", ""
                    if worker_process:  # Only if worker_process was successfully created
                        stdout_data_bp, stderr_data_bp = worker_process.communicate()
                    stdout_data += stdout_data_bp  # Append if any
                    stderr_data += stderr_data_bp
                except Exception as e_bp_comm:
                    logger.warning(f"{log_prefix}: Error getting final output after BrokenPipe: {e_bp_comm}")
                return None, "Audio worker task interrupted by higher priority request."  # Use the consistent marker
            except Exception as comm_err:  # Other communication errors
                logger.error(f"{log_prefix}: Error communicating with audio worker: {comm_err}")
                if worker_process and worker_process.poll() is None:
                    try:
                        worker_process.kill(); worker_process.communicate()  # Best effort cleanup
                    except:
                        pass
                return None, f"Communication error with audio worker: {comm_err}"

            exit_code = worker_process.returncode if worker_process else -1  # Handle if worker_process is None
            duration = time.monotonic() - start_time
            logger.info(f"{log_prefix}: Audio worker finished. Exit Code: {exit_code}, Duration: {duration:.2f}s")

            if stderr_data:  # Log stderr regardless of exit code for diagnostics
                log_level_stderr = "ERROR" if exit_code != 0 else "DEBUG"
                stderr_snippet = (stderr_data[:2000] + '...[TRUNCATED]') if len(stderr_data) > 2000 else stderr_data
                logger.log(log_level_stderr,
                           f"{log_prefix}: Audio Worker STDERR:\n-------\n{stderr_snippet.strip()}\n-------")

            if exit_code == 0:
                if not stdout_data or not stdout_data.strip():  # Check if stdout is empty or just whitespace
                    logger.error(
                        f"{log_prefix}: Audio worker exited cleanly but no stdout or stdout is empty/whitespace.")
                    return None, "Audio worker produced no parsable output."

                json_string_to_parse = None  # Initialize
                try:
                    # Find the first '{' which should mark the beginning of our JSON object
                    json_start_index = stdout_data.find('{')
                    if json_start_index == -1:
                        logger.error(f"{log_prefix}: No JSON object start ('{{') found in audio worker stdout.")
                        logger.error(f"{log_prefix}: Raw stdout from worker (first 1000 chars):\n{stdout_data[:1000]}")
                        return None, "Audio worker did not produce valid JSON output (no '{' found)."

                    json_string_to_parse = stdout_data[json_start_index:]
                    parsed_json = json.loads(json_string_to_parse)
                    logger.debug(f"{log_prefix}: Parsed audio worker JSON response successfully.")

                    if isinstance(parsed_json,
                                  dict) and "error" in parsed_json:  # Check if worker itself reported an error in JSON
                        logger.error(f"{log_prefix}: Audio worker reported internal error: {parsed_json['error']}")
                        return None, f"Audio worker error: {parsed_json['error']}"
                    return parsed_json, None  # Success

                except json.JSONDecodeError as json_err:
                    logger.error(f"{log_prefix}: Failed to decode audio worker stdout JSON: {json_err}")
                    problematic_string_snippet = json_string_to_parse[
                                                 :500] if json_string_to_parse is not None else stdout_data[:500]
                    logger.error(
                        f"{log_prefix}: String snippet attempted for parsing:\n{problematic_string_snippet}...")
                    logger.error(
                        f"{log_prefix}: Original raw stdout from worker (first 1000 chars):\n{stdout_data[:1000]}")
                    return None, f"Failed to decode audio worker response: {json_err}"
            else:  # Worker exited with non-zero code
                err_msg = f"Audio worker process failed (exit code {exit_code})."
                # Stderr already logged above if present
                logger.error(f"{log_prefix}: {err_msg}")
                return None, err_msg

        except Exception as e:  # Catch-all for unexpected errors in this function's try block
            logger.error(f"{log_prefix}: Unexpected error managing audio worker: {e}")
            logger.exception(f"{log_prefix} Audio Worker Management Traceback:")
            if worker_process and worker_process.poll() is None:  # If Popen succeeded but later error
                try:
                    worker_process.kill(); worker_process.communicate()  # Best effort cleanup
                except:
                    pass
            return None, f"Error managing audio worker: {e}"
        finally:
            if lock_acquired:  # Only release if it was acquired
                logger.info(f"{log_prefix}: Releasing shared resource lock.")
                shared_priority_lock.release()
    else:  # Lock acquisition failed
        logger.error(f"{log_prefix}: FAILED to acquire shared resource lock for audio worker.")
        return None, "Failed to acquire execution lock for audio worker."


async def _run_background_high_quality_asr(
        original_audio_path: str,
        elp1_transcription: str,  # The (potentially corrected/diarized) text returned to user
        session_id_for_log: str,
        request_id: str,  # For cohesive logging
        language_for_asr: str
):
    """
    Runs high-quality ASR in the background (ELP0) and logs comparison.
    Needs its own DB session as it runs in a separate thread/task.
    """
    bg_asr_log_prefix = f"ASR_BG_ELP0|{request_id}"
    logger.info(f"{bg_asr_log_prefix}: Starting background high-quality ASR for audio: {original_audio_path}")
    db_bg_task: Optional[Session] = None  # Initialize db_bg_task to None

    # Create a new DB session for this background task
    if SessionLocal is None:  # Ensure SessionLocal is imported from database.py
        logger.error(f"{bg_asr_log_prefix}: SessionLocal is None. Cannot create DB session for background ASR.")
        return

    db_bg_task = SessionLocal()  # type: ignore
    if not db_bg_task:
        logger.error(f"{bg_asr_log_prefix}: Failed to create DB session for background ASR.")
        return

    try:
        asr_worker_script_bg = os.path.join(SCRIPT_DIR, "audio_worker.py")
        # Command for the high-quality (default) Whisper model at ELP0
        asr_worker_cmd_bg = [
            APP_PYTHON_EXECUTABLE, asr_worker_script_bg,
            "--task-type", "asr",
            "--model-dir", WHISPER_MODEL_DIR,  # from CortexConfiguration
            "--temp-dir", os.path.join(SCRIPT_DIR, "temp_audio_worker_files")  # Consistent temp dir
        ]
        asr_request_data_bg = {
            "input_audio_path": original_audio_path,  # Original audio path
            "whisper_model_name": WHISPER_DEFAULT_MODEL_FILENAME,  # High-quality model from CortexConfiguration
            "language": language_for_asr,  # Language used for initial ASR
            "request_id": f"{request_id}-bg-asr"
        }

        logger.info(f"{bg_asr_log_prefix}: Executing audio worker for high-quality ASR (ELP0)...")
        # _execute_audio_worker_with_priority is synchronous, run in thread from async context
        hq_asr_response, hq_asr_err = await asyncio.to_thread(
            _execute_audio_worker_with_priority,  # This helper is in AdelaideAlbertCortex
            worker_command=asr_worker_cmd_bg,
            request_data=asr_request_data_bg,
            priority=ELP0,  # Run this transcription at ELP0
            worker_cwd=SCRIPT_DIR,
            timeout=ASR_WORKER_TIMEOUT + 120  # Potentially longer timeout for higher quality model
        )

        elp0_transcription: Optional[str] = None
        if hq_asr_err or not (
                hq_asr_response and isinstance(hq_asr_response.get("result"), dict) and "text" in hq_asr_response[
            "result"]):
            logger.error(
                f"{bg_asr_log_prefix}: Background high-quality ASR step failed: {hq_asr_err or 'Invalid ASR worker response'}")
            elp0_transcription = f"[High-Quality ASR Failed: {hq_asr_err or 'Invalid ASR worker response'}]"
        else:
            elp0_transcription = hq_asr_response["result"]["text"]
            logger.info(
                f"{bg_asr_log_prefix}: Background high-quality ASR successful (Snippet: '{elp0_transcription[:100]}...').")

        # Log the comparison
        comparison_text = (
            f"--- ASR Comparison for Request {request_id} ---\n"
            f"Low-Latency Whisper ({WHISPER_LOW_LATENCY_MODEL_FILENAME}) - ELP1 Output (returned to client):\n"
            f"\"\"\"\n{elp1_transcription}\n\"\"\"\n\n"
            f"High-Quality Whisper ({WHISPER_DEFAULT_MODEL_FILENAME}) - ELP0 Background Output:\n"
            f"\"\"\"\n{elp0_transcription}\n\"\"\"\n"
        )

        add_interaction(
            db_bg_task, session_id=session_id_for_log, mode="asr_service",
            input_type="asr_comparison_log",
            user_input=f"[ASR Comparison Log for original request {request_id}]",
            llm_response=comparison_text[:4000],  # Ensure it fits DB field
            classification="asr_quality_comparison"
        )
        db_bg_task.commit()
        logger.info(f"{bg_asr_log_prefix}: Logged ASR comparison to database.")

    except Exception as e_bg_asr:
        logger.error(f"{bg_asr_log_prefix}: Error in background high-quality ASR task: {e_bg_asr}")
        logger.exception(f"{bg_asr_log_prefix} Background ASR Traceback:")
        if db_bg_task:
            try:
                add_interaction(db_bg_task, session_id=session_id_for_log, mode="asr_service", input_type="log_error",
                                user_input=f"[Background ASR Error for Req {request_id}]",
                                llm_response=str(e_bg_asr)[:2000])
                db_bg_task.commit()
            except Exception as e_db_log:
                logger.error(f"{bg_asr_log_prefix}: Failed to log background ASR error to DB: {e_db_log}")
                if db_bg_task: db_bg_task.rollback()
    finally:
        if db_bg_task:
            db_bg_task.close()
            logger.debug(f"{bg_asr_log_prefix}: Background ASR DB session closed.")
        logger.info(f"{bg_asr_log_prefix}: Background high-quality ASR task finished.")


async def _run_background_asr_and_translation_analysis(
        original_audio_path: str,  # This is the path to the initially uploaded temp file
        elp1_transcription_final_for_client: str,
        elp1_translation_final_for_client: Optional[str],  # The quick translation returned to client
        session_id_for_log: str,
        request_id: str,
        language_asr: str,
        target_language_translation: Optional[str]  # Target language code for translation (e.g., "es")
):
    bg_log_prefix = f"ASR_Translate_BG_ELP0|{request_id}"
    logger.info(f"{bg_log_prefix}: Starting background tasks for audio: {original_audio_path}")

    db_bg_task: Optional[Session] = None
    high_quality_transcribed_text: Optional[str] = None

    try:
        if SessionLocal is None:
            logger.error(f"{bg_log_prefix}: SessionLocal is None. Cannot create DB session.")
            return
        db_bg_task = SessionLocal()  # type: ignore
        if not db_bg_task:
            logger.error(f"{bg_log_prefix}: Failed to create DB session.")
            return

        # === Step 1: High-Quality ASR (ELP0) ===
        asr_worker_script_bg = os.path.join(SCRIPT_DIR, "audio_worker.py")
        asr_worker_cmd_bg = [
            APP_PYTHON_EXECUTABLE, asr_worker_script_bg,
            "--task-type", "asr", "--model-dir", WHISPER_MODEL_DIR,
            "--temp-dir", os.path.join(SCRIPT_DIR, "temp_audio_worker_files")  # For ffmpeg in worker
        ]
        asr_request_data_bg = {
            "input_audio_path": original_audio_path,  # Use the original uploaded file
            "whisper_model_name": WHISPER_DEFAULT_MODEL_FILENAME,  # High-quality model
            "language": language_asr,
            "request_id": f"{request_id}-bg-hq-asr"
        }
        logger.info(f"{bg_log_prefix}: Executing audio worker for high-quality ASR (ELP0) on {original_audio_path}...")
        hq_asr_response, hq_asr_err = await asyncio.to_thread(
            _execute_audio_worker_with_priority,
            asr_worker_cmd_bg, asr_request_data_bg, ELP0, SCRIPT_DIR, ASR_WORKER_TIMEOUT + 180
        )

        if hq_asr_err or not (
                hq_asr_response and isinstance(hq_asr_response.get("result"), dict) and "text" in hq_asr_response[
            "result"]):
            logger.error(
                f"{bg_log_prefix}: Background high-quality ASR failed: {hq_asr_err or 'Invalid ASR worker response'}")
            high_quality_transcribed_text = f"[High-Quality ASR (ELP0) Failed: {hq_asr_err or 'Invalid ASR response'}]"
        else:
            high_quality_transcribed_text = hq_asr_response["result"]["text"]
            logger.info(
                f"{bg_log_prefix}: Background high-quality ASR successful. Snippet: '{high_quality_transcribed_text[:100]}...'")

        # Log ASR comparison
        asr_comparison_log_text = (
            f"--- ASR Comparison for Request {request_id} ---\n"
            f"Quick Whisper ({WHISPER_LOW_LATENCY_MODEL_FILENAME}) ELP1 output (processed, sent to client):\n"
            f"\"\"\"\n{elp1_transcription_final_for_client}\n\"\"\"\n\n"
            f"Default Whisper ({WHISPER_DEFAULT_MODEL_FILENAME}) ELP0 background output:\n"
            f"\"\"\"\n{high_quality_transcribed_text}\n\"\"\"\n"
        )
        add_interaction(db_bg_task, session_id=session_id_for_log, mode="asr_service",
                        input_type="asr_comparison_log",
                        user_input=f"[ASR Comparison Log for {request_id}]",
                        llm_response=asr_comparison_log_text[:4000],
                        classification="internal_asr_comparison")
        db_bg_task.commit()
        logger.info(f"{bg_log_prefix}: Logged ASR comparison to database.")

        # === Step 2: If it was a translation request, do background "Grokking Generate" for translation ===
        if target_language_translation and high_quality_transcribed_text and \
                not high_quality_transcribed_text.startswith("[High-Quality ASR (ELP0) Failed"):

            logger.info(
                f"{bg_log_prefix}: Performing background 'Grokking Generate' translation of high-quality transcript to '{target_language_translation}'...")

            source_lang_full = langcodes.Language.make(
                language=language_asr).display_name() if language_asr != "auto" else "auto-detected"
            target_lang_full = langcodes.Language.make(language=target_language_translation).display_name()

            deep_translation_prompt = PROMPT_DEEP_TRANSLATION_ANALYSIS.format(
                source_language_code=language_asr,
                source_language_full_name=source_lang_full,
                target_language_code=target_language_translation,
                target_language_full_name=target_lang_full,
                high_quality_transcribed_text=high_quality_transcribed_text
            )

            # Use background_generate for a potentially more complex/thorough translation
            # This will create its own new interaction chain.
            deep_translation_session_id = f"deep_translate_{request_id}"
            logger.info(
                f"{bg_log_prefix}: Spawning background_generate for deep translation. Session: {deep_translation_session_id}")
            # background_generate itself handles ELP0 and its own logging.
            # It's async, so we can await it if this function is called via asyncio.create_task
            # or just let it run if this is already a background thread.
            # Since _run_background_asr_and_translation_analysis is async, we can await.
            await cortex_text_interaction.background_generate(  # type: ignore
                db=db_bg_task,  # Pass the DB session for this background task
                user_input=deep_translation_prompt,
                session_id=deep_translation_session_id,
                classification="deep_translation_task",
                image_b64=None,
                update_interaction_id=None  # It's a new root task for this session
            )
            # The result of this deep translation is stored as a new interaction by background_generate.
            # We log a comparison note pointing to this.

            translation_comparison_log_text = (
                f"--- Translation Comparison for Request {request_id} ---\n"
                f"Quick Translation (ELP1, from Low-Latency ASR, sent to client):\n"
                f"\"\"\"\n{elp1_translation_final_for_client or 'N/A (Not a translation request or ELP1 translation failed)'}\n\"\"\"\n\n"
                f"Deep Translation (ELP0 Background task using High-Quality ASR) was initiated.\n"
                f"Input to Deep Translation (High-Quality ASR Text, Lang: {language_asr}):\n"
                f"\"\"\"\n{high_quality_transcribed_text[:500]}...\n\"\"\"\n"
                f"Result will be logged under session_id starting with '{deep_translation_session_id}'."
            )
            add_interaction(db_bg_task, session_id=session_id_for_log, mode="translation_service",
                            input_type="translation_comparison_log",
                            user_input=f"[Translation Comparison Log for {request_id}]",
                            llm_response=translation_comparison_log_text[:4000],
                            classification="internal_translation_comparison")
            db_bg_task.commit()
            logger.info(f"{bg_log_prefix}: Logged translation comparison info and spawned deep translation.")
        elif target_language_translation:
            logger.warning(
                f"{bg_log_prefix}: Skipped deep translation because high-quality ASR failed or produced no text.")

    except Exception as e_bg_task:
        logger.error(f"{bg_log_prefix}: Error in background task: {e_bg_task}")
        logger.exception(f"{bg_log_prefix} Background Task Traceback:")
        if db_bg_task:
            try:
                add_interaction(db_bg_task, session_id=session_id_for_log, mode="asr_service", input_type="log_error",
                                user_input=f"[Background ASR/Translate Error for Req {request_id}]",
                                llm_response=str(e_bg_task)[:2000])
                db_bg_task.commit()
            except Exception as e_db_log_bg_err:
                logger.error(f"{bg_log_prefix}: Failed to log background task error to DB: {e_db_log_bg_err}")
                if db_bg_task: db_bg_task.rollback()
    finally:
        # This background task is now responsible for the original_audio_path
        if original_audio_path and os.path.exists(original_audio_path):
            try:
                await asyncio.to_thread(os.remove, original_audio_path)  # Use await for async context
                logger.info(f"{bg_log_prefix}: Deleted original temporary input audio file: {original_audio_path}")
            except Exception as e_del_orig:
                logger.warning(
                    f"{bg_log_prefix}: Failed to delete original temporary input audio file '{original_audio_path}': {e_del_orig}")
        if db_bg_task:
            db_bg_task.close()
            logger.debug(f"{bg_log_prefix}: Background task DB session closed.")
        logger.info(f"{bg_log_prefix}: Background task finished.")


async def run_startup_benchmark():
    """
    Runs a benchmark on the direct_generate function at startup and measures it in milliseconds.
    """
    global BENCHMARK_ELP1_TIME_MS # Declare that we are modifying the global variable
    logger.info("--- Running Startup Benchmark for direct_generate() or ELP1 ---")
    if not cortex_text_interaction:
        logger.error("DETERMENISM_ELP1_CALIBRATION: (CortexThoughts) instance not available. Skipping benchmark.")
        return

    benchmark_db_session: Optional[Session] = None
    try:
        benchmark_db_session = SessionLocal()
        if not benchmark_db_session:
            raise RuntimeError("Failed to create a database session for the benchmark.")

        # The story prompt for the benchmark
        story_prompt = "Tell me a short, epic story about a brave knight and a wise dragon who become friends."
        benchmark_session_id = f"benchmark_startup_{int(time.time())}"

        logger.info(f"DETERMENISM_ELP1_CALIBRATION: Prompt: '{story_prompt}'")
        start_time = time.monotonic()

        # Calling the direct_generate function to be benchmarked
        response_text = await cortex_text_interaction.direct_generate(
            db=benchmark_db_session,
            user_input=story_prompt,
            session_id=benchmark_session_id
        )

        end_time = time.monotonic()
        # Calculate duration and convert to milliseconds
        duration_ms = (end_time - start_time) * 1000

        # Store the result in the global variable
        BENCHMARK_ELP1_TIME_MS = duration_ms

        logger.info("--- Startup Benchmark Result ---")
        logger.info(f"Response Snippet: '{response_text[:150]}...'")
        logger.info(f"direct_generate() execution time and will be expected for ELP1 to response within: {BENCHMARK_ELP1_TIME_MS:.2f} ms") # Display in milliseconds
        logger.info("---------------------------------")

    except Exception as e:
        logger.error(f"DETERMENISM_ELP1_CALIBRATION: An error occurred during the startup DETERMENISM_ELP1_CALIBRATION: {e}")
        logger.exception("Benchmark Execution Traceback:")
    finally:
        if benchmark_db_session:
            benchmark_db_session.close()


async def build_ada_daemons():
    """
    Runs the build process for all discovered Ada daemons in a separate thread
    to avoid blocking the async event loop.
    """
    if not ENABLE_STELLA_ICARUS_DAEMON:
        logger.info("Skipping Ada daemon build: feature disabled in configuration.")
        return

    logger.info("... Scheduling build for StellaIcarus Ada daemons in background thread ...")
    # Run the synchronous build method in a thread
    await asyncio.to_thread(stella_icarus_daemon_manager.build_all)
    logger.info("... Ada daemon build process has completed ...")


_sim_lock = threading.Lock()
# Holds the persistent state of the simulated aircraft.
_sim_state = {
    # Flight Dynamics
    "pitch": 0.0, "roll": 0.0, "yaw": 180.0, "last_update": time.monotonic(),
    "pitch_rate": 0.0, "roll_rate": 0.0, "yaw_rate": 0.0,
    "mach": 0.2, "keas": 120.0, "vertical_speed": 0.0,
    "altitude": 5000.0,
    "g_force": 1.0, "angle_of_attack": 4.5, "sideslip_angle": 0.0,

    # Navigation
    "flight_director": {"command_pitch": 2.0, "command_roll": -1.0},
    "hsi": {
        "selected_course": 185.0, "course_deviation": 0.1, "hac_turn_angle": 90.0,
        "waypoints": [
            {"id": "WAYPOINT_1", "label": "*", "bearing": 190.0, "range": 10.5}
            # Can add more waypoints here if needed for simulation
        ]
    },

    # Systems
    "systems_status": {"dap_mode": "Auto", "throttle_mode": "Auto", "attitude_ref": "GND",
                       "flight_controller_mode": "MAN", "yaw_damper_on": True},
    "flight_controls": {
        "elevons": {"left_deg": 0.0, "right_deg": 0.0},
        "body_flap": {"actual_pct": 0.0, "commanded_pct": 0.0},
        "rudder": {"actual_deg": 0.0, "commanded_deg": 0.0},
        "speedbrake": {"actual_pct": 0.0, "commanded_pct": 0.0}
    }
}


def _generate_simulated_avionics_data() -> Dict[str, Any]:
    """
    Generates a single, detailed frame of simulated avionics data, matching
    the new hierarchical JSON structure.
    """
    with _sim_lock:
        global _sim_state
        now = time.monotonic()
        delta_t = now - _sim_state["last_update"]
        if delta_t == 0: delta_t = 1.0 / 60.0  # Prevent division by zero if called too quickly
        _sim_state["last_update"] = now

        # --- Simulate Flight Dynamics ---
        last_roll = _sim_state["roll"]
        last_pitch = _sim_state["pitch"]
        last_yaw = _sim_state["yaw"]

        # Attitude random walk
        _sim_state["roll"] = max(-60, min(60, last_roll + random.uniform(-1, 1) * delta_t * 2))
        _sim_state["pitch"] = max(-30, min(30, last_pitch + random.uniform(-0.5, 0.5) * delta_t * 1))

        # Calculate rates based on attitude change
        _sim_state["roll_rate"] = (_sim_state["roll"] - last_roll) / delta_t
        _sim_state["pitch_rate"] = (_sim_state["pitch"] - last_pitch) / delta_t
        # Simplified yaw rate based on roll and pitch (co-ordinated turn)
        _sim_state["yaw_rate"] = _sim_state["roll_rate"] * math.sin(math.radians(_sim_state["pitch"]))
        _sim_state["yaw"] = (_sim_state["yaw"] + _sim_state["yaw_rate"] * delta_t) % 360
        _sim_state["sideslip_angle"] = _sim_state["yaw_rate"] * -0.1  # Simplified sideslip

        # G-force based on bank angle
        _sim_state["g_force"] = 1.0 / math.cos(math.radians(_sim_state["roll"])) if abs(
            _sim_state["roll"]) < 85 else 6.0

        # Velocity and Position
        _sim_state["vertical_speed"] = max(-4000, min(4000, _sim_state["vertical_speed"] + random.uniform(-50,
                                                                                                          50) * delta_t * 5))
        _sim_state["altitude"] += _sim_state["vertical_speed"] * delta_t / 60.0
        _sim_state["keas"] = max(60, min(700, _sim_state["keas"] + random.uniform(-2, 2) * delta_t * 5))
        try:
            # Calculate the raw Mach number based on physics
            raw_mach = _sim_state["keas"] / (661.47 * (1.0 - _sim_state["altitude"] / 145442.0) ** 2.5)
            # Clamp the final value to the desired range
            _sim_state["mach"] = max(0.5, min(20.0, raw_mach))
        except (ZeroDivisionError, ValueError): # Added ValueError for potential math domain errors
            # If calculation fails, default to a safe value within the range
            _sim_state["mach"] = 0.8
        _sim_state["angle_of_attack"] = 2.5 + _sim_state["pitch"] / 4.0 - (_sim_state["keas"] - 150.0) / 50.0

        # --- Simulate Flight Controls (mirroring pilot input) ---
        _sim_state["flight_controls"]["elevons"]["left_deg"] = _sim_state["pitch"] * -0.5 + _sim_state["roll"] * 0.25
        _sim_state["flight_controls"]["elevons"]["right_deg"] = _sim_state["pitch"] * -0.5 - _sim_state["roll"] * 0.25
        _sim_state["flight_controls"]["rudder"]["actual_deg"] = _sim_state["yaw_rate"] * -0.5
        _sim_state["flight_controls"]["rudder"]["commanded_deg"] = _sim_state["flight_controls"]["rudder"]["actual_deg"]

        # --- Construct the Final JSON Payload ---
        payload = {
            "timestamp": datetime.datetime.now(datetime.timezone.utc).isoformat(),
            "flight_dynamics": {
                "attitude": {
                    "pitch": round(_sim_state["pitch"], 2),
                    "roll": round(_sim_state["roll"], 2),
                    "yaw": round(_sim_state["yaw"], 2)
                },
                "rates": {
                    "pitch_rate": round(_sim_state["pitch_rate"], 2),
                    "roll_rate": round(_sim_state["roll_rate"], 2),
                    "yaw_rate": round(_sim_state["yaw_rate"], 2)
                },
                "velocity": {
                    "mach": round(_sim_state["mach"], 2),
                    "keas": round(_sim_state["keas"], 2),
                    "vertical_speed": round(_sim_state["vertical_speed"], 2)
                },
                "position": {
                    "altitude": round(_sim_state["altitude"], 2)
                },
                "aero": {
                    "g_force": round(_sim_state["g_force"], 2),
                    "angle_of_attack": round(_sim_state["angle_of_attack"], 2),
                    "sideslip_angle": round(_sim_state["sideslip_angle"], 2)
                }
            },
            "navigation": {
                "flight_director": {
                    "command_pitch": round(_sim_state["pitch"] + 2.0, 2),
                    "command_roll": round(_sim_state["roll"] - 1.0, 2)
                },
                "hsi": _sim_state["hsi"]
            },
            "systems": {
                "status": _sim_state["systems_status"],
                "flight_controls": _sim_state["flight_controls"]
            }
        }
        return payload



def get_current_configurable_settings():
    """
    Reads the current values of all adjustable settings from the CortexConfiguration module.
    Excludes paths, lists, and other non-user-facing constants.
    """
    settings = {}
    # Directly reference the imported CortexConfiguration module's variables
    import CortexConfiguration as config
    
    # --- COMPLETE LIST of all user-adjustable variables ---
    adjustable_vars = [
        # General & Core
        "PROVIDER",
        "MEMORY_SIZE",
        "ANSWER_SIZE_WORDS",
        "TOPCAP_TOKENS",
        "DEFAULT_LLM_TEMPERATURE",
        "DEEP_THOUGHT_RETRY_ATTEMPTS",
        "RESPONSE_TIMEOUT_MS",
        
        # RAG, Vector & Indexing
        "VECTOR_CALC_CHUNK_BATCH_TOKEN_SIZE",
        "CHUNK_OVERLAP",
        "RAG_FILE_INDEX_COUNT",
        "RAG_URL_COUNT",
        "FUZZY_SEARCH_THRESHOLD",
        "TOT_SIMILARITY_THRESHOLD",
        "ENABLE_FILE_INDEXER",
        "FILE_INDEX_MAX_SIZE_MB",
        "FILE_INDEX_MIN_SIZE_KB",
        "FILE_INDEXER_IDLE_WAIT_SECONDS",

        # Self-Reflection
        "ENABLE_SELF_REFLECTION",
        "SELF_REFLECTION_HISTORY_COUNT",
        "SELF_REFLECTION_MAX_TOPICS",
        "REFLECTION_BATCH_SIZE",
        "IDLE_WAIT_SECONDS",
        "ACTIVE_CYCLE_PAUSE_SECONDS",
        "ENABLE_PROACTIVE_RE_REFLECTION",
        "PROACTIVE_RE_REFLECTION_CHANCE",
        "MIN_AGE_FOR_RE_REFLECTION_DAYS",

        # Concurrency & Performance
        "MAX_CONCURRENT_BACKGROUND_GENERATE_TASKS",
        "SEMAPHORE_ACQUIRE_TIMEOUT_SECONDS",
        "BENCHMARK_ELP1_TIME_MS",
        "AGENTIC_RELAXATION_MODE",
        "AGENTIC_RELAXATION_PERIOD_SECONDS",
        
        # Llama.cpp Specific
        "LLAMA_CPP_N_GPU_LAYERS",
        "LLAMA_CPP_N_CTX",
        "LLAMA_CPP_VERBOSE",
        "LLAMA_WORKER_TIMEOUT",
        
        # Image Generation (FLUX & Refiner)
        "IMAGE_GEN_WORKER_TIMEOUT",
        "IMAGE_GEN_DEVICE",
        "IMAGE_GEN_RNG_TYPE",
        "IMAGE_GEN_N_THREADS",
        "IMAGE_GEN_DEFAULT_SAMPLE_STEPS",
        "IMAGE_GEN_DEFAULT_CFG_SCALE",
        "REFINEMENT_MODEL_ENABLED",
        "REFINEMENT_STRENGTH",
        "REFINEMENT_CFG_SCALE",
        "REFINEMENT_ADD_NOISE_STRENGTH",
        
        # StellaIcarus Hooks & Daemons
        "ENABLE_STELLA_ICARUS_HOOKS",
        "ENABLE_STELLA_ICARUS_DAEMON",
        "INSTRUMENT_STREAM_RATE_HZ",

        # Database Snapshots
        "ENABLE_DB_SNAPSHOTS",
        "DB_SNAPSHOT_INTERVAL_MINUTES",
        "DB_SNAPSHOT_RETENTION_COUNT",

        # Batch Logging
        "LOG_BATCH_SIZE",
        "LOG_FLUSH_INTERVAL_SECONDS"
    ]
    
    for var_name in adjustable_vars:
        if hasattr(config, var_name):
            settings[var_name] = getattr(config, var_name)
        else:
            logger.warning(f"Config API: Variable '{var_name}' not found in CortexConfiguration.py")
            
    return settings
async def _get_and_process_proactive_interaction():
    """
    This is a synchronous function that performs all the blocking DB and LLM work.
    It will be run in a separate thread.
    """
    db = SessionLocal()
    try:
        # 1. Blocking Database Query
        past_interaction = db.query(Interaction).filter(
            Interaction.input_type == 'text',
            Interaction.user_input.isnot(None),
            Interaction.llm_response.isnot(None)
        ).order_by(func.random()).first()

        if not past_interaction:
            logger.info("💡 No suitable past interaction found to revisit.")
            return None # Return None if nothing was found

        # 2. Blocking LLM Call (invoke is synchronous)
        decision_chain = ChatPromptTemplate.from_template(PROMPT_XMPP_SHOULD_I_RECALL) | cortex_text_interaction.provider.get_model("general_fast") | StrOutputParser()
        decision = decision_chain.invoke({
            "past_user_input": past_interaction.user_input,
            "past_ai_response": past_interaction.llm_response
        })
        
        if "yes" not in decision.lower():
            logger.info(f"💡 AI decided not to revisit this interaction. verb: {decision.lower()}")
            return None

        # 3. Another Blocking LLM Call
        revisit_prompt = PROMPT_XMPP_PROACTIVE_REVISIT.format(
            past_user_input=past_interaction.user_input,
            past_ai_response=past_interaction.llm_response
        )
        proactive_message = await cortex_text_interaction.direct_generate(db, revisit_prompt, "proactive_thought_session")
        
        is_logical = True
        for bad_phrase in XMPP_PROACTIVE_BAD_RESPONSE_MARKERS:
            if fuzz.partial_ratio(bad_phrase.lower(), proactive_message.lower()) > 85:
                is_logical = False
                break
        
        if proactive_message and is_logical:
            # 4. Blocking Database Write
            add_interaction(db, session_id=f"proactive_sse", mode="proactive_event",
                            input_type="proactive_message",
                            user_input=f"[REVISITING ID: {past_interaction.id}]",
                            llm_response=proactive_message)
            db.commit()
            
            # Return the data needed by the async part
            return {
                "message": proactive_message, 
                "source_interaction_id": past_interaction.id
            }

    finally:
        db.close()
    
    return None


async def _run_proactive_sse_push_loop():
    """
    A background loop that has a random chance to revisit old conversations
    and PUSHES the thought into the global SSE queue.
    """
    if not ENABLE_SSE_NOTIFICATIONS:
        logger.info("Proactive SSE push loop is disabled by configuration.")
        return

    logger.info("💡 Proactive SSE push loop started (Two-Step Recall Logic).")
    while True:
        try:
            logger.info(f"💡 awaiting {PROACTIVE_MESSAGE_CYCLE_SECONDS} seconds")
            await asyncio.sleep(PROACTIVE_MESSAGE_CYCLE_SECONDS)
            logger.info(f"💡 Proactive SSE wait passed")

            if True: # Using 'if True' for testing
                logger.info("💡 Proactive message chance MET. Finding and processing interaction in background thread...")
                
                # Run the entire blocking function in a separate thread
                #result = await asyncio.to_thread(_get_and_process_proactive_interaction)
                result = await _get_and_process_proactive_interaction()

                if result:
                    logger.info(f"💡 Pushing proactive message to SSE queue: '{result['message'][:70]}...'")
                    
                    sse_event = format_sse_notification(
                        data=result,
                        event="proactive_thought"
                    )
                    # sse_notification_queue.put() is synchronous, which is fine
                    # for a queue.Queue, but if it's an asyncio.Queue, use await .put()
                    sse_notification_queue.put(sse_event)

        except asyncio.CancelledError:
            logger.info("Proactive SSE push loop cancelled.")
            break
        except Exception as e:
            # It's good practice to log the traceback for better debugging
            logger.exception(f"Error in proactive SSE push loop: {e}")

def _create_personal_assistant_stub_response(endpoint_name: str, method: str, resource_id: Optional[str] = None, custom_status: str = "not_applicable_personal_assistant"):
    """
    Creates a standardized JSON response for Assistants API stubs.
    """
    message = (
        f"The endpoint '{method} {endpoint_name}' for managing separate assistants/threads "
        "is not implemented in the standard OpenAI way. This system operates as an integrated personal assistant. "
        "Functionalities like memory, context management, and tool use are embedded within its direct interaction flow "
        "and internal self-reflection or background generation processes."
    )
    if resource_id:
        message += f" Operation on resource ID '{resource_id}' is not applicable."

    response_body = {
        "object": "api.stub_response",
        "endpoint_called": endpoint_name,
        "method": method,
        "resource_id_queried": resource_id,
        "status": custom_status,
        "message": message,
        "note": "This is a placeholder response indicating a conceptual difference in system design."
    }
    # OpenAI often returns 200 OK even for informative non-errors, or specific errors for non-found resources.
    # For these stubs explaining a different design, 200 OK with the message can be user-friendly.
    # Or, you could choose 501 Not Implemented if you prefer to signal it more strongly.
    return jsonify(response_body), 200


def _create_assistants_api_stub_response(
        endpoint_name: str,
        method: str,
        resource_ids: Optional[Dict[str, str]] = None,
        operation_specific_message: str = "",
        additional_details: Optional[Dict[str, Any]] = None,
        object_type_for_response: str = "api.stub_information"  # Custom object type for these stubs
):
    """
    Creates a standardized JSON response for Assistants API stubs,
    explaining the system's personal assistant design.
    """
    message = (
        f"This system ('Zephy/Adelaide') is designed as an integrated personal assistant. "
        f"The standard OpenAI Assistants API endpoint '{method} {endpoint_name}' is handled differently. "
        "Core functionalities like memory, context management, and tool use (agentic mode) are embedded "
        "within its direct interaction flow and asynchronous background processes, rather than through explicit, "
        "multi-step Assistant/Thread/Run objects managed via this API."
    )
    if operation_specific_message:
        message += f" Regarding '{method} {endpoint_name}': {operation_specific_message}"

    response_body = {
        "object": object_type_for_response,
        "message": message,
        "details": {
            "endpoint_called": f"{method} {endpoint_name}",
            "system_paradigm": "Integrated Personal Assistant with Asynchronous Agentic Mode",
        }
    }
    if resource_ids:
        response_body["details"]["resource_ids_queried"] = resource_ids  # type: ignore
    if additional_details:
        response_body["details"].update(additional_details)

    # Using 200 OK with an informative message, as these are informational stubs.
    return jsonify(response_body), 200

# === Flask Routes (Async) ===

# ====== Server Root =======
@app.route("/", methods=["GET", "POST", "HEAD"])
async def handle_interaction():
    """
    Async version of the main endpoint. It intelligently handles three types of requests:
    - GET/HEAD from an Ollama client: A successful health check.
    - GET/HEAD from a browser: A redirect to a special URL.
    - POST from any client: The universal fast-path AI response, now handled asynchronously.
    """
    # --- Handler for GET/HEAD (Health Checks and Browsers) ---
    # This part of the logic is synchronous and does not require any changes.
    # It will execute correctly within an async function.
    if request.method in ["GET", "HEAD"]:
        user_agent = request.headers.get('User-Agent', '')

        browser_fingerprint = "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36"

        if FUZZY_AVAILABLE and fuzz and user_agent:
            ollama_score = fuzz.partial_ratio('ollama', user_agent.lower())
            if ollama_score >= 85:
                logger.info(
                    f"Ollama Compatibility: Responding 200 OK to {request.method} from User-Agent '{user_agent}' (Score: {ollama_score})")
                return Response("Adelaide/Zephy is running (Ollama compatibility mode).", status=200,
                                mimetype="text/plain")

            browser_score = fuzz.token_set_ratio(browser_fingerprint, user_agent)
            if browser_score >= 60:
                logger.info(
                    f"Browser Detected: Redirecting {request.method} from User-Agent '{user_agent}' (Score: {browser_score})")
                return redirect("https://www.youtube.com/watch?v=dQw4w9WgXcQ")

        logger.info(f"Default Redirect: Redirecting unknown GET/HEAD request from User-Agent: '{user_agent}'")
        return redirect("https://www.youtube.com/watch?v=dQw4w9WgXcQ")

    # --- ASYNCHRONOUS LOGIC FOR ALL POST REQUESTS ---
    start_req = time.monotonic()
    db: Session = g.db
    resp: Response  # Define the variable to ensure it's available in all paths

    try:
        # Request parsing is synchronous
        request_data = request.get_json()
        if not request_data:
            return Response("Empty request payload.", status=400, mimetype="text/plain")

        prompt = request_data.get("prompt")
        if not prompt:
            logger.warning("POST request missing 'prompt'.")
            return Response("Request must include a 'prompt'.", status=400, mimetype="text/plain")

        session_id = request_data.get("session_id", f"direct_session_{int(time.time())}")

        logger.info(f"🚀 Async POST Request: Routing to direct_generate. Session: {session_id}, Prompt: '{prompt[:50]}...'")

        # --- CORE CHANGE: Use `await` instead of `asyncio.run()` ---
        # This is the key to making the endpoint non-blocking.
        response_text = await cortex_text_interaction.direct_generate(db, prompt, session_id)

        resp = Response(response_text, status=200, mimetype="text/plain; charset=utf-8")

    except Exception as e:
        logger.exception("🔥🔥 Unhandled exception in async main POST request handler:")
        response_text = f"Internal Server Error: {e}"
        resp = Response(response_text, status=500, mimetype="text/plain; charset=utf-8")

    finally:
        duration_req = (time.monotonic() - start_req) * 1000
        logger.info(f"🏁 Async POST Request handled in {duration_req:.2f} ms.")

    return resp


@app.route("/meshCommunityServeProcessor", methods=["POST"])
async def handle_mesh_community_serve_processor():

    """
    An ELP0-ONLY asynchronous meta-endpoint for the ZephyMesh community.
    It accepts various task types (ASR, Chat, VLM, TTS, Image Gen), queues them
    for low-priority background processing, and immediately returns an "Accepted"
    response to the client without waiting for the task to complete.
    """
    request_id = f"req-mesh-elp0-{uuid.uuid4()}"
    logger.info(f"🚀 ZephyMesh ELP0 Processor Request ID: {request_id}")
    db: Optional[Session] = None
    resp_obj: Optional[Response] = None
    final_status_code: int = 500
    start_req_time = time.monotonic()
    # This session_id is for the new background task we are creating.
    session_id_for_task: str = f"mesh_task_{request_id}"

    try:
        # --- Step 1: Manual DB Session (used only for initial validation if needed) ---
        db = SessionLocal()
        if not db:
            raise RuntimeError("Failed to create a database session for this request.")

        task_type_for_response = "unknown"

        # ==============================================================================
        # === ROUTE 1: AUDIO TRANSCRIPTION (MULTIPART/FORM-DATA) =======================
        # ==============================================================================
        if request.content_type and request.content_type.startswith('multipart/form-data'):
            logger.info(f"{request_id}: Routing to ASR handler (will wait for ELP0 result).")

            # --- Step 1.1: Validate and Extract Request Data ---
            audio_file_storage = request.files.get('file')
            if not audio_file_storage or not audio_file_storage.filename:
                raise ValueError("A 'file' part with a filename is required for audio transcription.")

            # Extract parameters from the form part of the request.
            language = request.form.get('language', 'auto')

            # --- Step 1.2: Securely Save the Uploaded Audio File ---
            # Use `secure_filename` to prevent directory traversal attacks.
            uploaded_filename = secure_filename(audio_file_storage.filename)

            # Create a dedicated, temporary directory if it doesn't exist.
            temp_audio_dir = os.path.join(SCRIPT_DIR, "temp_audio_worker_files")
            await asyncio.to_thread(os.makedirs, temp_audio_dir, exist_ok=True)

            _, file_extension = os.path.splitext(uploaded_filename)

            # Create a unique temporary file path to avoid name collisions.
            temp_fd, temp_input_audio_path = tempfile.mkstemp(
                prefix=f"mesh_asr_{request_id}_",
                suffix=file_extension,
                dir=temp_audio_dir
            )
            os.close(temp_fd)  # We only need the path, `save` will handle the writing.

            # Save the file to disk. This is a blocking I/O call, so we run it in a thread.
            await asyncio.to_thread(audio_file_storage.save, temp_input_audio_path)
            logger.info(f"{request_id}: Mesh ASR audio saved to temporary path: {temp_input_audio_path}")

            # --- Step 1.3: Delegate to the Main Pipeline and Wait for the Result ---
            # This `await` will hold the connection open until the entire ASR pipeline is complete.
            final_result_text = await _process_mesh_asr_request_and_get_result(
                audio_file_path=temp_input_audio_path,
                language=language,
                session_id=session_id_for_task,
                request_id=request_id
            )

            # --- Step 1.4: Format and Return the Final Result to the Client ---
            # We format the text result into the standard OpenAI chat response structure.
            resp_obj = jsonify(_format_openai_chat_response(final_result_text))
            final_status_code = 200
            return resp_obj

        # ==============================================================================
        # === ROUTE 2: JSON-BASED SERVICES (CHAT, VLM, TTS, IMAGE GEN) =================
        # ==============================================================================
        elif request.content_type and request.content_type.startswith('application/json'):
            # This block handles all non-file-upload requests.
            request_data = request.get_json()
            if not request_data:
                raise ValueError("Empty JSON payload.")

            session_id_for_task = request_data.get("session_id", session_id_for_task)

            # --- Sub-Router for JSON types ---

            # --- ROUTE 2a: CHAT COMPLETIONS & VLM ---
            if "messages" in request_data and isinstance(request_data.get("messages"), list):
                has_image = "image_url" in str(request_data)
                mode_for_log = "vlm_chat_mesh" if has_image else "chat_mesh"
                logger.info(f"{request_id}: Routing to ELP0 {mode_for_log} handler (waits for result).")

                # Delegate to the specific helper and wait for the raw result
                final_result_text = await _process_mesh_chat_request_and_get_result(
                    request_data=request_data,
                    db_session=db,
                    session_id=session_id_for_task,
                    request_id=request_id
                )

                # Prepare the data for the decoupled privacy logging
                # The original query is the last user message. The response is the final text.
                original_query_for_log = request_data.get("messages", [{}])[-1]
                final_response_for_log = final_result_text

                # Format the final, raw response for the user
                resp_obj = jsonify(_format_openai_chat_response(final_result_text))
                final_status_code = 200

            # --- ROUTE 2b: TEXT-TO-SPEECH ---
            elif "input" in request_data and "voice" in request_data:
                mode_for_log = "tts_mesh"
                logger.info(f"{request_id}: Routing to ELP0 Text-to-Speech handler (waits for result).")

                # Delegate to the specific helper and wait for the raw audio bytes
                audio_bytes, mime_type = await _process_mesh_tts_request_and_get_result(
                    request_data=request_data,
                    request_id=request_id
                )

                # Prepare data for privacy logging
                original_query_for_log = request_data.get("input")
                final_response_for_log = f"[Audio generated successfully in {mime_type} format]"

                # Format the final, raw response for the user
                resp_obj = Response(audio_bytes, mimetype=mime_type)
                final_status_code = 200

            # --- ROUTE 2c: IMAGE GENERATION ---
            elif "prompt" in request_data:
                mode_for_log = "image_gen_mesh"
                logger.info(f"{request_id}: Routing to ELP0 Image Generation handler (waits for result).")

                # Delegate to the specific helper and wait for the raw image data
                image_data_list = await _process_mesh_image_gen_request_and_get_result(
                    request_data=request_data,
                    db_session=db,
                    session_id=session_id_for_task,
                    request_id=request_id
                )

                # Prepare data for privacy logging
                original_query_for_log = request_data.get("prompt")
                final_response_for_log = f"[{len(image_data_list)} image(s) generated successfully]"

                # Format the final, raw response for the user
                response_body = {"created": int(time.time()), "data": image_data_list}
                resp_obj = jsonify(response_body)
                final_status_code = 200
            else:
                raise ValueError("JSON payload does not match any supported service format (chat, tts, image_gen).")

            if final_status_code == 200:
                logger.info(f"{request_id}: Spawning background task for sanitized logging (Mode: {mode_for_log}).")
                asyncio.create_task(
                    _log_sanitized_interaction(
                        original_query=original_query_for_log,
                        final_response=final_response_for_log,
                        session_id=session_id_for_task,
                        mode=mode_for_log
                    )
                )

        # --- Step 3: Return the "Task Acknowledged" Response ---
        # If we reach here, it means one of the routes was matched and its task was spawned.
        # We now immediately return a success response to the client.
        response_body = {
            "status": "processing_queued",
            "task_type": task_type_for_response,
            "session_id": session_id_for_task,
            "message": "Your request has been successfully queued for low-priority (ELP0) background processing."
        }
        resp_obj = jsonify(response_body)
        final_status_code = 202  # HTTP 202 Accepted is the perfect status code for this

        return resp_obj

    except (ValueError, RuntimeError) as e:
        logger.warning(f"{request_id}: Invalid Mesh request: {e}")
        final_status_code = 400
        error_body = {"error": {"code": "bad_request_or_unsupported_format", "message": str(e),
                                "supported_formats": [
                                    "OpenAI Chat/VLM: application/json with 'messages' key.",
                                    "OpenAI TTS: application/json with 'input' and 'voice' keys.",
                                    "OpenAI Image Gen: application/json with 'prompt' key.",
                                    "OpenAI ASR: multipart/form-data with a 'file' field."
                                ]}}
        return jsonify(error_body), final_status_code
    except Exception as e:
        logger.exception(f"{request_id}: 🔥🔥 Unhandled exception in Mesh processor:")
        final_status_code = 500
        resp_data, _ = _create_openai_error_response(f"Internal server error: {type(e).__name__}", status_code=500)
        return jsonify(resp_data), final_status_code
    finally:
        if db:
            db.close()
        duration_req = (time.monotonic() - start_req_time) * 1000
        logger.info(
            f"🏁 ZephyMesh ELP0 Processor Request {request_id} handled in {duration_req:.2f} ms. Final Status: {final_status_code}")

# === NEW: Zephy Cortex Configuration API ===
@app.route("/ZephyCortexConfig", methods=["GET", "POST"])
def handle_cortex_config():
    """
    GET: Returns the current adjustable configuration.
    POST: Updates the configuration by writing to a .env file.
          Requires application restart to take effect.
    """
    req_id = f"req-config-{uuid.uuid4()}" # Assumes uuid is imported
    
    if request.method == "GET":
        logger.info(f"🚀 {req_id}: Received GET /ZephyCortexConfig")
        try:
            current_settings = get_current_configurable_settings()
            return jsonify(current_settings), 200
        except Exception as e:
            logger.error(f"❌ {req_id}: Error fetching current config: {e}")
            return jsonify({"error": "Failed to retrieve current configuration."}), 500

    elif request.method == "POST":
        logger.info(f"🚀 {req_id}: Received POST /ZephyCortexConfig")
        try:
            new_settings = request.get_json()
            if not isinstance(new_settings, dict):
                raise ValueError("Invalid JSON payload. Expected an object.")

            # Define constraints based on your comments
            constraints = {
                "MEMORY_SIZE": {"type": int, "max": 20},
                "ANSWER_SIZE_WORDS": {"type": int},
                "TOPCAP_TOKENS": {"type": int, "max": 32768},
                "DEFAULT_LLM_TEMPERATURE": {"type": float, "max": 1.0},
                "RAG_FILE_INDEX_COUNT": {"type": int},
                "FILE_INDEX_MAX_SIZE_MB": {"type": int, "max": 512},
                "FUZZY_SEARCH_THRESHOLD": {"type": int, "max": 85},
                "RAG_URL_COUNT": {"type": int, "max": 10},
                # Add other constraints as needed
            }

            env_file_content = ""
            for key, value in new_settings.items():
                # Only process keys that are adjustable
                if key in constraints:
                    constraint = constraints[key]
                    # Validate and cast type
                    try:
                        value = constraint["type"](value)
                    except (ValueError, TypeError):
                        return jsonify({"error": f"Invalid type for '{key}'. Expected {constraint['type'].__name__}."}), 400
                    
                    # Validate max value
                    if "max" in constraint and value > constraint["max"]:
                        return jsonify({"error": f"Value for '{key}' ({value}) exceeds maximum of {constraint['max']}."}), 400
                
                # Append to .env file content
                env_file_content += f"{key.upper()}={value}\n"

            # Write to .env file in the same directory as CortexConfiguration.py
            env_file_path = os.path.join(MODULE_DIR, ".env")
            with open(env_file_path, "w") as f:
                f.write(env_file_content)

            logger.success(f"✅ {req_id}: Configuration successfully written to {env_file_path}")
            return jsonify({
                "message": "Configuration saved successfully. A restart of the application is required for changes to take effect.",
                "path": env_file_path
            }), 200

        except ValueError as ve:
            logger.warning(f"{req_id}: Invalid POST request to /ZephyCortexConfig: {ve}")
            return jsonify({"error": str(ve)}), 400
        except Exception as e:
            logger.error(f"❌ {req_id}: Error processing config update: {e}")
            return jsonify({"error": "Failed to update configuration."}), 500

    return jsonify({"error": "Method not allowed"}), 405

# === NEW OpenAI Compatible Embeddings Route ===
# AdelaideAlbertCortex -> Flask Routes Section

#=============[Self Test Status]===============
@app.route("/v1/primedready", methods=["GET"])
@app.route("/primedready", methods=["GET"])
def handle_primed_ready_status():
    """
    Custom endpoint for the GUI to check if the initial startup benchmark
    has completed, using an avionics-style "Power-on Self Test" message.
    """
    req_id = f"req-primedready-{uuid.uuid4()}"
    logger.info(f"🚀 {req_id}: Received GET /primedready status check.")
    elapsed_seconds = time.monotonic() - APP_START_TIME
    expected_duration_seconds = 60.0
    if SYSTEM_IS_PRIMING and elapsed_seconds < expected_duration_seconds:
        # If we are still within the expected time, show progress.
        if elapsed_seconds < expected_duration_seconds:
            status_payload = {
                "primed_and_ready": False,
                "status": f"Power-on Self Test in progress... Initializing core components (Expected T-{expected_duration_seconds - elapsed_seconds:.0f}s).",
                "elp1_benchmark_ms": None
            }
        # If startup is running longer than expected, show a warning.
        else:
            status_payload = {
                "primed_and_ready": False,
                "status": "POST WARNING: Power-on Self Test exceeded expected duration. System initialization is slow or may have stalled. Upgrade your system Zephy might upgraded itself beyond your system can handle",
                "elp1_benchmark_ms": None
            }
        
        return jsonify(status_payload), 200

    # Case 2: The startup_tasks function has completed.
    else:
        # The system is now fully operational.
        status_payload = {
            "primed_and_ready": True,
            "status": "Power-on Self Test complete. All systems nominal. Ready for engagement.",
            "elp1_benchmark_ms": BENCHMARK_ELP1_TIME_MS # The benchmark result is now valid to show
        }
        return jsonify(status_payload), 200

    return jsonify(status_payload), 200


#==============================[openAI API (Most standard) Behaviour]==============================
@app.route("/v1/embeddings", methods=["POST"])
@app.route("/api/embed", methods=["POST"])
@app.route("/api/embeddings", methods=["POST"])
async def handle_openai_embeddings():
    start_req = time.monotonic()
    request_id = f"req-emb-{uuid.uuid4()}" # Unique ID for this request
    logger.info(f"🚀 Quart OpenAI-Style Embedding Request ID: {request_id}")
    status_code = 500 # Default to error
    response_payload = "" # Initialize

    # --- Check Provider Initialization ---
    if not cortex_backbone_provider or not cortex_backbone_provider.embeddings or not cortex_backbone_provider.EMBEDDINGS_MODEL_NAME:
        logger.error(f"{request_id}: Embeddings provider not initialized correctly.")
        resp_data, status_code = _create_openai_error_response("Embedding model not available.", err_type="server_error", status_code=500)
        response_payload = json.dumps(resp_data)
        return Response(response_payload, status=status_code, mimetype='application/json')

    # Use the configured embedding model name for the response
    model_name_to_return = f"{cortex_backbone_provider.provider_name}/{cortex_backbone_provider.EMBEDDINGS_MODEL_NAME}"

    # --- Get and Validate Request Data ---
    try:
        request_data = await request.get_json() # Use await for Quart
        if not request_data:
            logger.warning(f"{request_id}: Empty JSON payload.")
            resp_data, status_code = _create_openai_error_response("Request body is missing or invalid JSON.", err_type="invalid_request_error", status_code=400)
            response_payload = json.dumps(resp_data)
            return Response(response_payload, status=status_code, mimetype='application/json')

        input_data = request_data.get("input")
        model_requested = request_data.get("model") # Log requested model, but ignore it

        if model_requested:
            logger.warning(f"{request_id}: Request specified model '{model_requested}', but will use configured '{model_name_to_return}'.")

        if not input_data:
            logger.warning(f"{request_id}: 'input' field missing.")
            resp_data, status_code = _create_openai_error_response("'input' is a required property.", err_type="invalid_request_error", status_code=400)
            response_payload = json.dumps(resp_data)
            return Response(response_payload, status=status_code, mimetype='application/json')

        # --- Prepare Input Texts ---
        texts_to_embed = []
        if isinstance(input_data, list):
            if not all(isinstance(item, str) for item in input_data):
                logger.warning(f"{request_id}: 'input' array must contain only strings.")
                resp_data, status_code = _create_openai_error_response("If 'input' is an array, all elements must be strings.", err_type="invalid_request_error", status_code=400)
                response_payload = json.dumps(resp_data)
                return Response(response_payload, status=status_code, mimetype='application/json')
            texts_to_embed = input_data
        elif isinstance(input_data, str):
            texts_to_embed = [input_data]
        else:
            logger.warning(f"{request_id}: 'input' must be a string or an array of strings.")
            resp_data, status_code = _create_openai_error_response("'input' must be a string or an array of strings.", err_type="invalid_request_error", status_code=400)
            response_payload = json.dumps(resp_data)
            return Response(response_payload, status=status_code, mimetype='application/json')

        if not texts_to_embed:
             logger.warning(f"{request_id}: No valid text found in 'input'.")
             resp_data, status_code = _create_openai_error_response("No text provided in 'input'.", err_type="invalid_request_error", status_code=400)
             response_payload = json.dumps(resp_data)
             return Response(response_payload, status=status_code, mimetype='application/json')

        # --- Generate Embeddings ---
        embeddings_list = []
        total_tokens = 0 # Placeholder
        status_code = 200 # Assume success unless error occurs

        logger.debug(f"{request_id}: Embedding {len(texts_to_embed)} text(s) using {model_name_to_return}...")
        start_embed_time = time.monotonic()

        # Run embedding in a thread as it can be CPU intensive
        if len(texts_to_embed) == 1:
            # Use embed_query for single string
            embedding_vector = await asyncio.to_thread(cortex_backbone_provider.embeddings.embed_query, texts_to_embed[0])
            embeddings_list = [embedding_vector]
        else:
            # Use embed_documents for list of strings
            embeddings_list = await asyncio.to_thread(cortex_backbone_provider.embeddings.embed_documents, texts_to_embed)

        embed_duration = (time.monotonic() - start_embed_time) * 1000
        logger.info(f"{request_id}: Embedding generation took {embed_duration:.2f} ms.")

        # --- Prepare Response Body ---
        response_data_list = []
        for i, vector in enumerate(embeddings_list):
            response_data_list.append({
                "object": "embedding",
                "embedding": vector, # Should be List[float]
                "index": i,
            })

        # Estimate token usage (very rough estimate)
        try:
             total_tokens = sum(len(text) for text in texts_to_embed) // 4
        except Exception:
             total_tokens = 0

        final_response_body = {
            "object": "list",
            "data": response_data_list,
            "model": model_name_to_return, # Return the actual model used
            "usage": {
                "prompt_tokens": total_tokens, # Estimated input tokens
                "total_tokens": total_tokens,
            },
        }
        response_payload = json.dumps(final_response_body)

    except Exception as e:
        logger.exception(f"{request_id}: 🔥🔥 Error during embedding generation:")
        resp_data, status_code = _create_openai_error_response(f"Failed to generate embeddings: {e}", err_type="server_error", status_code=500)
        response_payload = json.dumps(resp_data)
        # Attempt to log error to DB
        try:
            # Need a DB session - create one temporarily for error logging
            error_db = SessionLocal()
            try:
                add_interaction(error_db, session_id=f"openai_emb_error_{request_id}", mode="embedding", input_type='error', user_input=f"Embedding Error. Request: {str(request_data)[:1000]}", llm_response=f"Handler Error: {e}"[:2000])
            finally:
                error_db.close()
        except Exception as db_err:
            logger.error(f"❌ Failed to log embedding endpoint error to DB: {db_err}")


    finally:
        duration_req = (time.monotonic() - start_req) * 1000
        logger.info(f"🏁 OpenAI-Style Embedding Request {request_id} handled in {duration_req:.2f} ms. Status: {status_code}")
        # No DB session from 'g' to close here as it wasn't used directly in this route

    # Return Quart Response object
    return Response(response_payload, status=status_code, mimetype='application/json')


# AdelaideAlbertCortex -> Flask Routes Section

@app.route("/v1/completions", methods=["POST"])
@app.route("/api/generate", methods=["POST"])
async def handle_legacy_completions():
    """
    Asynchronous and CONTEXT-SAFE version of the handler for the legacy OpenAI /v1/completions endpoint.
    This is the definitive implementation for a non-blocking ASGI server like Hypercorn.
    It manually manages its own database session to avoid Flask context issues.
    """
    endpoint_hit = request.path
    start_req = time.monotonic()
    request_id = f"req-legacy-async-safe-{uuid.uuid4()}"
    logger.info(f"🚀 Async-Safe Legacy Completion Request ID: {request_id} on Endpoint: {endpoint_hit}")

    db: Optional[Session] = None  # Initialize db session to None
    resp: Response
    session_id: str = f"legacy_req_{request_id}_unassigned"
    final_response_status_code = 500

    try:
        # --- Step 1: Manually Create DB Session for this Async Context ---
        db = SessionLocal()
        if not db:
            raise RuntimeError("Failed to create a database session for this request.")

        # --- Step 2: Parse and Validate Incoming Request ---
        raw_request_data = request.get_json()
        if not raw_request_data:
            raise ValueError("Empty JSON payload.")

        prompt = raw_request_data.get("prompt")
        stream = raw_request_data.get("stream", False)
        model_requested = raw_request_data.get("model")
        session_id = raw_request_data.get("session_id", f"legacy_req_{request_id}")

        logger.debug(
            f"{request_id}: Async Legacy Request parsed - SessionID={session_id}, Stream: {stream}, Model: {model_requested}, Prompt: '{str(prompt)[:50]}...'")

        if prompt is None or not isinstance(prompt, str):
            raise ValueError("The 'prompt' parameter is required and must be a string.")

        # This endpoint does not support streaming. We log a warning and proceed with a non-streaming response.
        if stream:
            logger.warning(
                f"{request_id}: Streaming is not implemented for the legacy /v1/completions endpoint. Ignoring stream=True.")

        # --- Step 3: Await the AI generation logic directly ---
        # Note: The `generate` method is a dispatcher. For a simple prompt, it will
        # effectively call the fast ELP1 `direct_generate` logic.
        logger.info(f"{request_id}: Awaiting non-blocking CortexThoughts.generate for legacy prompt...")

        # This `await` call is non-blocking and will yield control to the server's event loop.
        response_text = await cortex_text_interaction.generate(db, prompt, session_id)

        # --- Step 4: Format and Return the Response ---
        if "internal error" in response_text.lower() or "Error:" in response_text:
            status_code = 500
            logger.warning(
                f"{request_id}: CortexThoughts.generate returned a potential error: {response_text[:200]}...")
            resp_data, _ = _create_openai_error_response(response_text, status_code=status_code)
        else:
            status_code = 200
            logger.debug(f"{request_id}: CortexThoughts.generate completed successfully.")
            resp_data = _format_legacy_completion_response(response_text, model_name=META_MODEL_NAME_NONSTREAM)

        resp = jsonify(resp_data)
        resp.status_code = status_code
        final_response_status_code = status_code

    except ValueError as ve:
        logger.warning(f"{request_id}: Invalid legacy completions request: {ve}")
        resp_data, status_code = _create_openai_error_response(str(ve), err_type="invalid_request_error",
                                                               status_code=400)
        resp = jsonify(resp_data)
        resp.status_code = status_code
        final_response_status_code = status_code

    except Exception as e:
        logger.exception(f"{request_id}: 🔥🔥 Unhandled exception in async legacy completions handler:")
        resp_data, status_code = _create_openai_error_response(f"Internal server error: {e}", status_code=500)
        resp = jsonify(resp_data)
        resp.status_code = status_code
        final_response_status_code = status_code
        try:
            # We need a db session to log the error. If the error happened before `db` was created, we can't log.
            if db:
                add_interaction(db, session_id=session_id, mode="completion", input_type='error',
                                user_input=f"Legacy Endpoint Error",
                                llm_response=f"Handler Error ({type(e).__name__}): {e}"[:2000])
                db.commit()  # Commit the error log
            else:
                logger.error(
                    f"{request_id}: Cannot log error: DB session was not created before the exception occurred.")
        except Exception as db_err:
            logger.error(f"{request_id}: ❌ Failed to log error to DB: {db_err}")

    finally:
        # --- Step 5: Ensure the Database Session is Always Closed ---
        if db:
            db.close()
            logger.trace(f"Manually closed DB session for async request {request_id}")

        duration_req = (time.monotonic() - start_req) * 1000
        logger.info(
            f"🏁 Async-Safe Legacy Completion Request {request_id} handled in {duration_req:.2f} ms. Status: {final_response_status_code}")

    return resp


# --- NEW: SSE Notification Endpoint ---
@app.route("/v1/chat/notification")
def sse_notification_stream():
    """
    This endpoint streams notifications to the client using SSE.
    """
    logger.info("SSE Client connected to notification stream.")
    def generate():
        try:
            yield format_sse_notification({"status": "connected"}, event="connection_ack")
            while True:
                try:
                    message = sse_notification_queue.get(timeout=30)
                    if message is None:
                        break
                    yield message
                except queue.Empty:
                    yield ": keep-alive\n\n"
        except GeneratorExit:
            logger.warning("SSE notification stream generator exited (client likely disconnected).")

    return Response(stream_with_context(generate()), mimetype="text/event-stream")


@app.route("/v1/chat/responsezeph", methods=["POST"])
async def handle_zephyrine_chat_response():
    """
    A custom, high-performance chat endpoint that uses `async def` for the fastest
    possible ELP1 response time. It does NOT support streaming and will return
    an error if `stream=true` is requested.
    """
    start_req_time_main_handler = time.monotonic()
    request_id = f"req-zeph-{uuid.uuid4()}"
    logger.info(f"🚀 Zeph Custom Async Chat Request ID: {request_id}")

    db: Session = g.db
    resp_obj: Optional[Response] = None
    session_id_for_logs: str = f"zeph_req_{request_id}_unassigned"
    final_response_status_code: int = 500

    try:
        raw_request_data_dict = request.get_json()
        if not raw_request_data_dict: raise ValueError("Empty JSON payload.")

        messages_from_req = raw_request_data_dict.get("messages", [])
        stream_requested = raw_request_data_dict.get("stream", False)
        session_id_for_logs = raw_request_data_dict.get("session_id", f"zeph_req_{request_id}")

        # CRITICAL: Reject streaming requests to this specific endpoint
        if stream_requested:
            logger.warning(
                f"{request_id}: Request rejected. The /v1/chat/responsezeph/ endpoint does not support streaming.")
            resp_data_err, status_code_val = _create_openai_error_response(
                "Streaming is not supported on this endpoint. Use /v1/chat/completions for streaming.",
                err_type="invalid_request_error", status_code=400)
            return Response(json.dumps(resp_data_err), status=status_code_val, mimetype='application/json')

        if not cortex_text_interaction: raise RuntimeError("cortex_text_interaction instance not available.")
        cortex_text_interaction.current_session_id = session_id_for_logs

        user_input_from_req, image_b64_from_req = "", None
        last_user_msg_obj = next((msg for msg in reversed(messages_from_req) if msg.get("role") == "user"), None)
        if not last_user_msg_obj: raise ValueError("No message with role 'user' found.")
        content_from_user_msg = last_user_msg_obj.get("content")
        if isinstance(content_from_user_msg, str):
            user_input_from_req = content_from_user_msg
        elif isinstance(content_from_user_msg, list):
            for part in content_from_user_msg:
                if part.get("type") == "text":
                    user_input_from_req += part.get("text", "")
                elif part.get("type") == "image_url":
                    image_b64_from_req = part.get("image_url", {}).get("url", "").split(",", 1)[1]
        if not user_input_from_req and not image_b64_from_req: raise ValueError("No text or image content provided.")

        # Launch ELP0 background task in a thread (non-blocking)
        def run_bg_task(user_in, sess_id, img_b64):
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            db_session = SessionLocal()
            try:
                loop.run_until_complete(
                    cortex_text_interaction.background_generate(db=db_session, user_input=user_in, session_id=sess_id,
                                                                classification="chat_complex", image_b64=img_b64))
            finally:
                if db_session: db_session.close()
                loop.close()

        threading.Thread(target=run_bg_task, args=(user_input_from_req, session_id_for_logs, image_b64_from_req),
                         daemon=True).start()

        # Await the fast ELP1 response directly
        direct_response_text = await cortex_text_interaction.direct_generate(db, user_input_from_req,
                                                                             session_id_for_logs,
                                                                             image_b64=image_b64_from_req)

        # Format and return the non-streaming JSON response
        resp_data_ok = _format_openai_chat_response(direct_response_text, model_name=META_MODEL_NAME_NONSTREAM)
        resp_obj = jsonify(resp_data_ok)
        final_response_status_code = 200

    except Exception as e:
        logger.exception(f"{request_id}: 🔥🔥 UNHANDLED exception in Zeph async handler:")
        err_msg = f"Internal server error: {type(e).__name__}"
        resp_data, status_code = _create_openai_error_response(err_msg, status_code=500)
        resp_obj = Response(json.dumps(resp_data), status=status_code, mimetype='application/json')
        final_response_status_code = status_code

    finally:
        duration_req = (time.monotonic() - start_req_time_main_handler) * 1000
        logger.info(
            f"🏁 Zeph Custom Async Request {request_id} handled in {duration_req:.2f} ms. Status: {final_response_status_code}")

    return resp_obj


@app.route("/v1/chat/completions", methods=["POST"])
async def handle_openai_chat_completion():
    """
    Asynchronous and CONTEXT-SAFE version of the OpenAI chat completion handler.
    This is the definitive implementation for a non-blocking ASGI server like Hypercorn.
    It manually manages its own database session to avoid Flask context issues and
    uses a context-free generator for streaming to prevent crashes.
    """
    start_req_time_main_handler = time.monotonic()
    request_id = f"req-chat-async-safe-{uuid.uuid4()}"
    logger.info(f"🚀 Async-Safe OpenAI Chat Request ID: {request_id}")

    db: Optional[Session] = None  # Initialize db session to None
    resp_obj: Response
    session_id_for_logs: str = f"openai_req_{request_id}_unassigned"
    final_response_status_code: int = 500

    try:
        # --- Step 1: Manually Create DB Session for this Async Context ---
        db = SessionLocal()
        if not db:
            raise RuntimeError("Failed to create a database session for this request.")

        # --- Step 2: Parse and Validate Incoming Request ---
        raw_request_data_dict = request.get_json()
        if not raw_request_data_dict:
            raise ValueError("Empty JSON payload.")

        messages_from_req = raw_request_data_dict.get("messages", [])
        stream_requested_by_client = raw_request_data_dict.get("stream", False)
        session_id_for_logs = raw_request_data_dict.get("session_id", f"openai_req_{request_id}")

        if not cortex_text_interaction:
            raise RuntimeError("Critical: cortex_text_interaction instance not available.")
        # Set the session ID on the global instance for this request's lifecycle
        cortex_text_interaction.current_session_id = session_id_for_logs

        # Extract user input and any potential image data from the message list
        user_input_from_req, image_b64_from_req = "", None
        last_user_msg_obj = next((msg for msg in reversed(messages_from_req) if msg.get("role") == "user"), None)

        if not last_user_msg_obj:
            raise ValueError("No message with role 'user' found in the request.")

        content_from_user_msg = last_user_msg_obj.get("content")
        if isinstance(content_from_user_msg, str):
            user_input_from_req = content_from_user_msg
        elif isinstance(content_from_user_msg, list):
            for part in content_from_user_msg:
                if part.get("type") == "text":
                    user_input_from_req += part.get("text", "")
                elif part.get("type") == "image_url":
                    image_b64_from_req = part.get("image_url", {}).get("url", "").split(",", 1)[-1]

        if not user_input_from_req and not image_b64_from_req:
            raise ValueError("No text or image content was provided in the user message.")

        # --- Step 3: Spawn the Slow, Deep-Thought (ELP0) Task ---
        # This is a fire-and-forget task. It runs in the background on the main
        # event loop without blocking the response to the user.
        asyncio.create_task(
            cortex_text_interaction.background_generate(
                db=None,  # The task is responsible for creating its own DB session
                user_input=user_input_from_req,
                session_id=session_id_for_logs,
                classification="chat_complex",
                image_b64=image_b64_from_req
            )
        )
        logger.info(f"{request_id}: Scheduled non-blocking background_generate (ELP0) task.")

        # --- Step 4: Await the Fast, Immediate (ELP1) Response ---
        logger.info(f"{request_id}: Awaiting non-blocking direct_generate (ELP1) response...")
        # `await` yields control to the ASGI server, allowing it to handle other
        # requests while the AI model is processing this one.
        full_response_text = await cortex_text_interaction.direct_generate(
            db, user_input_from_req, session_id_for_logs, image_b64=image_b64_from_req
        )

        # --- Step 5: Format and Return the Response to the Client ---
        if stream_requested_by_client:
            logger.info(f"{request_id}: ELP1 generation complete. Starting CONTEXT-FREE pseudo-stream.")

            # Use the context-free streamer which is safe for async routes
            async_safe_generator = _async_compatible_openai_streamer(
                full_response_text=full_response_text,
                model_name=META_MODEL_NAME_STREAM
            )
            # Return the generator directly in the Response object
            resp_obj = Response(async_safe_generator, mimetype='text/event-stream')
            final_response_status_code = 200
        else:
            # Handle the non-streaming case
            logger.info(f"{request_id}: ELP1 generation complete. Formatting non-streaming response.")
            if "interrupted" in full_response_text.lower() or "Error:" in full_response_text:
                final_response_status_code = 503 if "interrupted" in full_response_text.lower() else 500
                resp_data_err, _ = _create_openai_error_response(full_response_text,
                                                                 status_code=final_response_status_code)
                resp_obj = Response(json.dumps(resp_data_err), status=final_response_status_code,
                                    mimetype='application/json')
            else:
                resp_data_ok = _format_openai_chat_response(full_response_text, model_name=META_MODEL_NAME_NONSTREAM)
                resp_obj = jsonify(resp_data_ok)
                final_response_status_code = 200

    except Exception as e:
        logger.exception(f"{request_id}: 🔥🔥 UNHANDLED exception in async completions handler:")
        err_msg = f"Internal server error: {type(e).__name__}"
        resp_data, status_code = _create_openai_error_response(err_msg, status_code=500)
        resp_obj = Response(json.dumps(resp_data), status=status_code, mimetype='application/json')
        final_response_status_code = status_code

    finally:
        # --- Step 6: Ensure the Database Session is Always Closed ---
        if db:
            db.close()
            logger.trace(f"Manually closed DB session for async request {request_id}")

        duration_req = (time.monotonic() - start_req_time_main_handler) * 1000
        logger.info(
            f"🏁 Async-Safe OpenAI Chat Request {request_id} handled in {duration_req:.2f} ms. Status: {final_response_status_code}")

    return resp_obj


@app.route("/v1/moderations", methods=["POST"])
async def handle_openai_moderations():
    """
    Handles requests mimicking OpenAI's Moderations endpoint.
    Uses CortexThoughts.direct_generate() with a specific prompt for assessment.
    """
    start_req_time = time.monotonic()
    request_id = f"req-mod-{uuid.uuid4()}"
    logger.info(f"🚀 Flask OpenAI-Style Moderation Request ID: {request_id}")

    db: Session = g.db
    final_status_code: int = 500
    resp: Optional[Response] = None
    session_id_for_log: str = f"moderation_req_{request_id}"  # Each moderation is a unique "session" for logging
    raw_request_data: Optional[Dict[str, Any]] = None
    input_text_to_moderate: Optional[str] = None

    try:
        try:
            raw_request_data = await request.get_json()  # Assuming Quart, or request.get_json() for Flask sync
            if not raw_request_data:
                raise ValueError("Empty JSON payload received.")
        except Exception as json_err:
            logger.warning(f"{request_id}: Failed to get/parse JSON body: {json_err}")
            resp_data, status_code = _create_openai_error_response(
                f"Request body is missing or invalid JSON: {json_err}",
                err_type="invalid_request_error", status_code=400)
            resp = Response(json.dumps(resp_data), status=status_code, mimetype='application/json')
            final_status_code = status_code
            return resp

        input_data = raw_request_data.get("input")
        model_requested = raw_request_data.get("model")  # Logged but we'll use our configured model name

        if model_requested:
            logger.info(
                f"{request_id}: Client requested model '{model_requested}', will use configured '{MODERATION_MODEL_CLIENT_FACING}'.")

        if not input_data:
            raise ValueError("'input' field is required.")

        # OpenAI allows string or array of strings. For simplicity, let's handle single string first.
        # If you need to handle an array, you'd loop and generate multiple results.
        if isinstance(input_data, list):
            if not input_data: raise ValueError("'input' array is empty.")
            input_text_to_moderate = str(input_data[0])  # Process first item if array for now
            if len(input_data) > 1:
                logger.warning(
                    f"{request_id}: Received array for 'input', processing only the first item for moderation.")
        elif isinstance(input_data, str):
            input_text_to_moderate = input_data
        else:
            raise ValueError("'input' must be a string or an array of strings.")

        if not input_text_to_moderate.strip():
            # Handle empty string input gracefully - OpenAI typically flags it as not harmful.
            logger.info(f"{request_id}: Input text is empty or whitespace. Returning as not flagged.")
            results_list = [{
                "flagged": False,
                "categories": {cat: False for cat in
                               ["hate", "hate/threatening", "self-harm", "sexual", "sexual/minors", "violence",
                                "violence/graphic"]},
                "category_scores": {cat: 0.0 for cat in
                                    ["hate", "hate/threatening", "self-harm", "sexual", "sexual/minors", "violence",
                                     "violence/graphic"]}
            }]
            response_body = {
                "id": f"modr-{uuid.uuid4()}",
                "model": MODERATION_MODEL_CLIENT_FACING,
                "results": results_list
            }
            resp = Response(json.dumps(response_body), status=200, mimetype='application/json')
            final_status_code = 200
            return resp

        # --- Call CortexThoughts.direct_generate() for moderation assessment ---
        # direct_generate runs at ELP1
        if not cortex_text_interaction:  # Should be initialized globally
            raise RuntimeError("CortexThoughts instance not available.")

        moderation_prompt_filled = PROMPT_MODERATION_CHECK.format(input_text_to_moderate=input_text_to_moderate)

        # Use a unique session_id for this moderation call to keep its logs separate if needed
        # or use the session_id_for_log passed in the request if that makes sense for your context.
        # For now, creating a distinct one for the direct_generate call.
        moderation_llm_session_id = f"mod_llm_req_{request_id}"
        if cortex_text_interaction: cortex_text_interaction.current_session_id = moderation_llm_session_id

        logger.info(
            f"{request_id}: Calling direct_generate for moderation. Input snippet: '{input_text_to_moderate[:70]}...'")
        # direct_generate is async, and this Flask route is async
        llm_assessment_text = await cortex_text_interaction.direct_generate(
            db,  # Pass the current request's DB session
            moderation_prompt_filled,
            moderation_llm_session_id,  # Session ID for this specific LLM call
            vlm_description=None,  # No image for moderation
            image_b64=None
        )

        logger.info(f"{request_id}: LLM moderation assessment raw response: '{llm_assessment_text}'")

        # --- Parse LLM's assessment and build OpenAI response ---
        flagged = False
        categories = {
            "hate": False, "hate/threatening": False, "self-harm": False,
            "sexual": False, "sexual/minors": False, "violence": False, "violence/graphic": False
        }
        # Category scores are harder to get reliably from a general LLM without specific training
        # We'll use dummy scores for now.
        category_scores = {cat: 0.01 for cat in categories}  # Default low scores

        if llm_assessment_text.startswith("FLAGGED:"):
            flagged = True
            try:
                violated_categories_str = llm_assessment_text.replace("FLAGGED:", "").strip()
                violated_list = [cat.strip() for cat in violated_categories_str.split(',')]
                for cat_key in violated_list:
                    if cat_key in categories:
                        categories[cat_key] = True
                        category_scores[cat_key] = 0.9  # Example high score for flagged
                    else:
                        logger.warning(f"{request_id}: LLM reported an unknown category '{cat_key}'")
            except Exception as parse_cat_err:
                logger.error(
                    f"{request_id}: Could not parse categories from LLM response '{llm_assessment_text}': {parse_cat_err}")
                # Keep flagged as True, but categories might be inaccurate or all false
        elif "CLEAN" not in llm_assessment_text.upper():  # If not "CLEAN" and not "FLAGGED:", it's ambiguous
            logger.warning(
                f"{request_id}: LLM moderation response ambiguous: '{llm_assessment_text}'. Defaulting to not flagged.")
            # Flagged remains false

        results_list = [{
            "flagged": flagged,
            "categories": categories,
            "category_scores": category_scores
        }]

        response_body = {
            "id": f"modr-{uuid.uuid4()}",  # Generate a new ID for the moderation response
            "model": MODERATION_MODEL_CLIENT_FACING,  # Use your configured model name
            "results": results_list
        }
        resp = Response(json.dumps(response_body), status=200, mimetype='application/json')
        final_status_code = 200

        # Log the moderation interaction
        try:
            add_interaction(db, session_id=session_id_for_log, mode="moderation", input_type="text_moderated",
                            user_input=input_text_to_moderate[:2000],  # Log original input
                            llm_response=json.dumps(results_list[0]),  # Log the result
                            classification="moderation_checked",
                            execution_time_ms=(time.monotonic() - start_req_time) * 1000)
            db.commit()
        except Exception as db_log_err:
            logger.error(f"{request_id}: Failed to log moderation result to DB: {db_log_err}")
            if db: db.rollback()

    except ValueError as ve:
        logger.warning(f"{request_id}: Invalid Moderation request: {ve}")
        resp_data, status_code = _create_openai_error_response(str(ve), err_type="invalid_request_error",
                                                               status_code=400)
        resp = Response(json.dumps(resp_data), status=status_code, mimetype='application/json')
        final_status_code = status_code
    except RuntimeError as rte:  # Catch errors from downstream calls like model unavailable
        logger.error(f"{request_id}: Runtime error during moderation: {rte}")
        resp_data, status_code = _create_openai_error_response(str(rte), err_type="server_error", status_code=500)
        resp = Response(json.dumps(resp_data), status=status_code, mimetype='application/json')
        final_status_code = status_code
    except Exception as main_err:
        logger.exception(f"{request_id}: 🔥🔥 Unhandled exception in Moderation endpoint:")
        error_message = f"Internal server error in Moderation endpoint: {type(main_err).__name__}"
        resp_data, status_code = _create_openai_error_response(error_message, status_code=500)
        resp = Response(json.dumps(resp_data), status=status_code, mimetype='application/json')
        final_status_code = status_code
        try:
            if db: add_interaction(db, session_id=session_id_for_log, mode="moderation", input_type='error',
                                   user_input=f"Moderation Handler Error. Input: {str(input_text_to_moderate)[:200]}",
                                   llm_response=error_message[:2000]); db.commit()
        except Exception as db_err_log:
            logger.error(f"{request_id}: ❌ Failed log Moderation handler error: {db_err_log}")

    finally:
        duration_req = (time.monotonic() - start_req_time) * 1000
        logger.info(
            f"🏁 OpenAI-Style Moderation Request {request_id} handled in {duration_req:.2f} ms. Status: {final_status_code}")

    if resp is None:
        logger.error(f"{request_id}: Moderation Handler logic flaw - response object 'resp' was not assigned!")
        resp_data, _ = _create_openai_error_response("Internal error: Handler failed to produce a response.",
                                                     status_code=500)
        resp = Response(json.dumps(resp_data), status=500, mimetype='application/json')
    return resp


@app.route("/v1/models", methods=["GET"])
async def handle_openai_models():
    """
    Asynchronous version of the handler for OpenAI's /v1/models endpoint.
    Lists all available meta-models for chat, moderation, TTS, ASR, and image generation.
    """
    logger.info("Received async request for /v1/models")
    start_req = time.monotonic()
    status_code = 200

    # This logic is fast and CPU-bound, so it runs directly within the async function.
    model_list = [
        {
            "id": META_MODEL_NAME_STREAM,
            "object": "model",
            "created": int(time.time()),
            "owned_by": META_MODEL_OWNER,
            "permission": [], "root": META_MODEL_NAME_STREAM, "parent": None,
        },
        {
            "id": META_MODEL_NAME_NONSTREAM,
            "object": "model",
            "created": int(time.time()),
            "owned_by": META_MODEL_OWNER,
            "permission": [], "root": META_MODEL_NAME_NONSTREAM, "parent": None,
        },
        {
            "id": MODERATION_MODEL_CLIENT_FACING,
            "object": "model",
            "created": int(time.time()),
            "owned_by": META_MODEL_OWNER,
            "permission": [], "root": MODERATION_MODEL_CLIENT_FACING, "parent": None,
        },
        {
            "id": TTS_MODEL_NAME_CLIENT_FACING,
            "object": "model",
            "created": int(time.time()),
            "owned_by": META_MODEL_OWNER,
            "permission": [], "root": TTS_MODEL_NAME_CLIENT_FACING, "parent": None,
        },
        {
            "id": ASR_MODEL_NAME_CLIENT_FACING,
            "object": "model",
            "created": int(time.time()),
            "owned_by": META_MODEL_OWNER,
            "permission": [], "root": ASR_MODEL_NAME_CLIENT_FACING, "parent": None,
        },
        {
            "id": AUDIO_TRANSLATION_MODEL_CLIENT_FACING,
            "object": "model",
            "created": int(time.time()),
            "owned_by": META_MODEL_OWNER,
            "permission": [], "root": AUDIO_TRANSLATION_MODEL_CLIENT_FACING, "parent": None,
        },
        {
            "id": IMAGE_GEN_MODEL_NAME_CLIENT_FACING,
            "object": "model",
            "created": int(time.time()),
            "owned_by": META_MODEL_OWNER,
            "permission": [], "root": IMAGE_GEN_MODEL_NAME_CLIENT_FACING, "parent": None,
        }
    ]

    response_body = {
        "object": "list",
        "data": model_list,
    }

    # Use jsonify for a slightly cleaner return. It correctly sets the mimetype and status.
    response = jsonify(response_body)
    response.status_code = status_code

    duration_req = (time.monotonic() - start_req) * 1000
    logger.info(f"🏁 /v1/models async request handled in {duration_req:.2f} ms. Status: {status_code}")
    return response



#==============================[Ollama Behaviour]==============================
@app.route("/api/chat", methods=["POST"])
async def handle_ollama_chat():
    """
    Asynchronous and CONTEXT-SAFE version of the handler for Ollama's /api/chat endpoint.
    This is the definitive implementation for a non-blocking ASGI server like Hypercorn.
    It manually manages its database session and uses a context-free streamer.
    """
    start_req_time_main_handler = time.monotonic()
    request_id = f"req-ollama-async-safe-{uuid.uuid4()}"
    logger.info(f"🚀 Async-Safe Ollama Chat Request ID: {request_id}")

    db: Optional[Session] = None  # Initialize db session to None
    resp_obj: Response
    session_id_for_logs: str = f"ollama_req_{request_id}_unassigned"
    final_response_status_code: int = 500

    try:
        # --- Step 1: Manually Create DB Session for this Async Context ---
        db = SessionLocal()
        if not db:
            raise RuntimeError("Failed to create a database session for this request.")

        # --- Step 2: Parse and Validate Incoming Request ---
        raw_request_data_dict = request.get_json()
        if not raw_request_data_dict:
            raise ValueError("Empty JSON payload.")

        messages_from_req = raw_request_data_dict.get("messages", [])
        stream_requested_by_client = raw_request_data_dict.get("stream", False)
        session_id_for_logs = raw_request_data_dict.get("session_id", f"ollama_req_{request_id}")

        if not cortex_text_interaction:
            raise RuntimeError("Critical: cortex_text_interaction instance not available.")
        cortex_text_interaction.current_session_id = session_id_for_logs

        # Extract user input and any potential image data from the message list
        user_input_from_req, image_b64_from_req = "", None
        last_user_msg_obj = next((msg for msg in reversed(messages_from_req) if msg.get("role") == "user"), None)
        if not last_user_msg_obj:
            raise ValueError("No message with role 'user' found in the request.")

        content_from_user_msg = last_user_msg_obj.get("content")
        if isinstance(content_from_user_msg, str):
            user_input_from_req = content_from_user_msg
        elif isinstance(content_from_user_msg, list):
            for part in content_from_user_msg:
                if part.get("type") == "text":
                    user_input_from_req += part.get("text", "")
                elif part.get("type") == "image_url":
                    image_b64_from_req = part.get("image_url", {}).get("url", "").split(",", 1)[-1]

        if not user_input_from_req and not image_b64_from_req:
            raise ValueError("No text or image content was provided in the user message.")

        # --- Step 3: Spawn the Slow, Deep-Thought (ELP0) Task ---
        asyncio.create_task(
            cortex_text_interaction.background_generate(
                db=None,  # The background task will create its own DB session
                user_input=user_input_from_req,
                session_id=session_id_for_logs,
                classification="chat_complex",
                image_b64=image_b64_from_req
            )
        )
        logger.info(f"{request_id}: Scheduled non-blocking background_generate (ELP0) task.")

        # --- Step 4: Await the Fast, Immediate (ELP1) Response ---
        logger.info(f"{request_id}: Awaiting non-blocking direct_generate (ELP1) response...")
        elp1_start_time = time.monotonic()

        full_response_text = await cortex_text_interaction.direct_generate(
            db, user_input_from_req, session_id_for_logs, image_b64=image_b64_from_req
        )

        elp1_end_time = time.monotonic()

        # Calculate durations in nanoseconds for the Ollama response format
        eval_duration_ns = int((elp1_end_time - elp1_start_time) * 1_000_000_000)
        total_duration_ns = int((elp1_end_time - start_req_time_main_handler) * 1_000_000_000)

        model_name_for_response = META_MODEL_NAME_NONSTREAM

        # --- Step 5: Format and Return the Response to the Client ---
        if stream_requested_by_client:
            logger.info(f"{request_id}: ELP1 complete. Starting Ollama-style CONTEXT-FREE pseudo-stream.")

            ollama_generator = _ollama_pseudo_stream_sync_generator(
                full_response_text=full_response_text,
                model_name=model_name_for_response,
                total_duration_ns=total_duration_ns,
                eval_duration_ns=eval_duration_ns
            )
            # CRITICAL: Do NOT use stream_with_context here.
            resp_obj = Response(ollama_generator, mimetype='application/x-ndjson')
            final_response_status_code = 200
        else:
            logger.info(f"{request_id}: ELP1 complete. Formatting Ollama non-streaming response.")
            ollama_response_body = _format_ollama_chat_response_nonstream(
                response_text=full_response_text,
                model_name=model_name_for_response,
                total_duration_ns=total_duration_ns,
                eval_duration_ns=eval_duration_ns
            )
            resp_obj = jsonify(ollama_response_body)
            final_response_status_code = 200

    except Exception as e:
        logger.exception(f"{request_id}: 🔥🔥 UNHANDLED exception in async Ollama chat handler:")
        # Ollama API returns a simple JSON error object on failure
        resp_obj = jsonify({"error": f"Internal server error: {type(e).__name__}"})
        final_response_status_code = 500

    finally:
        # --- Step 6: Ensure the Database Session is Always Closed ---
        if db:
            db.close()
            logger.trace(f"Manually closed DB session for async request {request_id}")

        duration_req = (time.monotonic() - start_req_time_main_handler) * 1_000
        logger.info(
            f"🏁 Async-Safe Ollama Chat Request {request_id} handled in {duration_req:.2f} ms. Status: {final_response_status_code}")

    return resp_obj


@app.route("/api/tags", methods=["GET", "HEAD"])
async def handle_ollama_tags():
    """
    Asynchronous version of the handler for Ollama's /api/tags endpoint.
    It provides a static list of the system's meta-models with realistic placeholder values.
    """
    logger.info("Received async request for /api/tags (Ollama Compatibility)")
    start_req = time.monotonic()
    status_code = 200

    # This logic is CPU-bound and very fast, so it does not need to be awaited.
    # It can run directly within the async function body.
    placeholder_size_bytes = 14200000000  # Approx. 14.2 billion bytes

    ollama_models = [
        {
            "name": f"{META_MODEL_NAME_STREAM}:latest",
            "model": f"{META_MODEL_NAME_STREAM}:latest",
            "modified_at": datetime.datetime.now(datetime.timezone.utc).isoformat(),
            "size": placeholder_size_bytes,
            "digest": hashlib.sha256(META_MODEL_NAME_STREAM.encode()).hexdigest(),
            "details": {
                "parent_model": "",
                "format": META_MODEL_FORMAT,
                "family": META_MODEL_FAMILY,
                "families": [META_MODEL_FAMILY],
                "parameter_size": META_MODEL_PARAM_SIZE,
                "quantization_level": META_MODEL_QUANT_LEVEL
            }
        },
        {
            "name": f"{META_MODEL_NAME_NONSTREAM}:latest",
            "model": f"{META_MODEL_NAME_NONSTREAM}:latest",
            "modified_at": datetime.datetime.now(datetime.timezone.utc).isoformat(),
            "size": placeholder_size_bytes,
            "digest": hashlib.sha256(META_MODEL_NAME_NONSTREAM.encode()).hexdigest(),
            "details": {
                "parent_model": "",
                "format": META_MODEL_FORMAT,
                "family": META_MODEL_FAMILY,
                "families": [META_MODEL_FAMILY],
                "parameter_size": META_MODEL_PARAM_SIZE,
                "quantization_level": META_MODEL_QUANT_LEVEL
            }
        },
    ]

    response_body = {
        "models": ollama_models
    }

    # Flask's jsonify works correctly within an async route handler.
    response = jsonify(response_body)
    response.status_code = status_code

    duration_req = (time.monotonic() - start_req) * 1000
    logger.info(f"🏁 /api/tags async request handled in {duration_req:.2f} ms. Status: {status_code}")
    return response


@app.route("/v1/audio/transcriptions", methods=["POST"])
async def handle_openai_asr_transcriptions():
    """
    Asynchronous and CONTEXT-SAFE handler for OpenAI-style audio transcriptions.
    Implements a high-performance ELP1 pipeline with an intelligent failover mechanism.
    If the initial low-latency ASR model produces no valid speech, it automatically
    retries with the high-quality ASR model at ELP1 priority before responding to the user.
    """
    start_req_time = time.monotonic()
    request_id = f"req-asr-{uuid.uuid4()}"
    logger.info(f"🚀 OpenAI-Style ASR Request ID: {request_id} (ELP1 Pipeline with ELP0-Quality Failover)")

    db: Optional[Session] = None
    final_status_code: int = 500
    resp: Optional[Response] = None
    session_id_for_log: str = f"asr_req_default_{request_id}"
    uploaded_filename: Optional[str] = None
    temp_input_audio_path: Optional[str] = None
    language_for_asr_steps: str = "auto"

    # This flag tracks if a separate background task for high-quality comparison is still needed.
    # It will be set to False if the high-quality failover is used.
    background_task_needed = True

    try:
        # --- Step 1: Manually Create DB Session for this Async Context ---
        db = SessionLocal()
        if not db:
            raise RuntimeError("Failed to create a database session for this request.")

        # --- Step 2: Request Validation and File Handling ---
        if not ENABLE_ASR:
            logger.warning(f"{request_id}: ASR endpoint called but ASR is disabled in config.")
            resp_data, status_code = _create_openai_error_response(
                "ASR functionality is currently disabled on this server.", err_type="server_error", code="asr_disabled",
                status_code=503)
            return Response(json.dumps(resp_data), status=status_code, mimetype='application/json')

        if not request.content_type or not request.content_type.startswith('multipart/form-data'):
            raise ValueError("Invalid content type. Must be multipart/form-data.")

        audio_file_storage = request.files.get('file')
        model_requested = request.form.get('model')
        language_param_for_log = request.form.get('language')
        response_format_req = request.form.get('response_format', 'json').lower()
        session_id_for_log = request.form.get("session_id", session_id_for_log)

        if not cortex_text_interaction:
            raise RuntimeError("CortexThoughts instance not configured for ASR post-processing.")
        cortex_text_interaction.current_session_id = session_id_for_log

        if audio_file_storage and audio_file_storage.filename:
            uploaded_filename = secure_filename(audio_file_storage.filename)

        if not audio_file_storage: raise ValueError("'file' field (audio data) is required.")
        if not model_requested or model_requested != ASR_MODEL_NAME_CLIENT_FACING:
            raise ValueError(f"Invalid 'model'. This endpoint supports '{ASR_MODEL_NAME_CLIENT_FACING}'.")

        language_for_asr_steps = (language_param_for_log or WHISPER_DEFAULT_LANGUAGE).strip().lower()
        if not language_for_asr_steps: language_for_asr_steps = "auto"

        temp_audio_dir = os.path.join(SCRIPT_DIR, "temp_audio_worker_files")
        await asyncio.to_thread(os.makedirs, temp_audio_dir, exist_ok=True)
        _, file_extension = os.path.splitext(uploaded_filename or ".tmpaud")
        temp_fd, temp_input_audio_path = tempfile.mkstemp(prefix="asr_orig_", suffix=file_extension, dir=temp_audio_dir)
        os.close(temp_fd)
        await asyncio.to_thread(audio_file_storage.save, temp_input_audio_path)
        logger.info(f"{request_id}: Input audio saved temporarily to: {temp_input_audio_path}")

        # --- Step 3: Initial Transcription Attempt with Low-Latency Model (ELP1) ---
        logger.info(
            f"{request_id}: ELP1 Step 3: Attempting Low-Latency ASR (Model: {WHISPER_LOW_LATENCY_MODEL_FILENAME})...")
        asr_worker_script = os.path.join(SCRIPT_DIR, "audio_worker.py")
        ll_asr_cmd = [APP_PYTHON_EXECUTABLE, asr_worker_script, "--task-type", "asr", "--model-dir", WHISPER_MODEL_DIR,
                      "--temp-dir", temp_audio_dir]
        ll_asr_req_data = {"input_audio_path": temp_input_audio_path,
                           "whisper_model_name": WHISPER_LOW_LATENCY_MODEL_FILENAME, "language": language_for_asr_steps,
                           "request_id": f"{request_id}-elp1-llasr"}

        elp1_asr_response, elp1_asr_err = await asyncio.to_thread(_execute_audio_worker_with_priority, ll_asr_cmd,
                                                                  ll_asr_req_data, ELP1, SCRIPT_DIR, ASR_WORKER_TIMEOUT)

        if elp1_asr_err:
            raise RuntimeError(f"Low-Latency ASR step failed critically: {elp1_asr_err}")

        raw_transcription = (elp1_asr_response.get("result", {}).get("text") or "").strip()

        # --- Step 4: Sanity Check and Conditional Failover ---
        is_transcription_valid = True
        if not raw_transcription or len(raw_transcription) < 3 or raw_transcription.lower() in WHISPER_GARBAGE_OUTPUTS:
            logger.warning(
                f"{request_id}: Low-latency ASR produced invalid speech: '{raw_transcription}'. Triggering high-quality failover.")
            is_transcription_valid = False

        if not is_transcription_valid:
            background_task_needed = False
            logger.info(
                f"{request_id}: ELP1 FAILOVER: Retrying with High-Quality ASR (Model: {WHISPER_DEFAULT_MODEL_FILENAME}) at ELP1 priority.")

            hq_asr_cmd = [APP_PYTHON_EXECUTABLE, asr_worker_script, "--task-type", "asr", "--model-dir",
                          WHISPER_MODEL_DIR, "--temp-dir", temp_audio_dir]
            hq_asr_req_data = {"input_audio_path": temp_input_audio_path,
                               "whisper_model_name": WHISPER_DEFAULT_MODEL_FILENAME, "language": language_for_asr_steps,
                               "request_id": f"{request_id}-elp1-failover-hqasr"}

            hq_asr_response, hq_asr_err = await asyncio.to_thread(_execute_audio_worker_with_priority, hq_asr_cmd,
                                                                  hq_asr_req_data, ELP1, SCRIPT_DIR,
                                                                  ASR_WORKER_TIMEOUT + 120)

            if hq_asr_err:
                raise RuntimeError(f"High-Quality ASR failover step failed critically: {hq_asr_err}")

            raw_transcription = (hq_asr_response.get("result", {}).get("text") or "").strip()

            if not raw_transcription or len(
                    raw_transcription) < 3 or raw_transcription.lower() in WHISPER_GARBAGE_OUTPUTS:
                logger.warning(f"{request_id}: High-quality ASR also produced no valid speech. Result will be empty.")
                raw_transcription = ""

        # --- Step 5: LLM Auto-Correction (ELP1) ---
        final_text_for_client = raw_transcription
        if raw_transcription:
            logger.info(f"{request_id}: ELP1 Step 5: Auto-correcting final transcript...")
            correction_prompt_filled = PROMPT_AUTOCORRECT_TRANSCRIPTION.format(raw_transcribed_text=raw_transcription)
            correction_session_id = f"correct_asr_{request_id}"
            cortex_text_interaction.current_session_id = correction_session_id

            llm_correction_output = await cortex_text_interaction.direct_generate(db, correction_prompt_filled,
                                                                                  correction_session_id)

            if llm_correction_output and not ("ERROR" in str(llm_correction_output).upper()):
                final_text_for_client = llm_correction_output.strip()
                logger.info(f"{request_id}: Auto-correction successful.")
            else:
                logger.warning(
                    f"{request_id}: Auto-correction failed. Using pre-correction transcript. LLM output: {llm_correction_output}")

        # --- Step 6: Format and Return Response ---
        if response_format_req == "json":
            response_body = {"text": final_text_for_client}
            resp = Response(json.dumps(response_body), status=200, mimetype='application/json')
        else:
            resp = Response(final_text_for_client, status=200, mimetype='text/plain; charset=utf-8')
        final_status_code = resp.status_code

        # --- Step 7: Spawn Background Task (if needed) ---
        if background_task_needed:
            logger.info(
                f"{request_id}: Spawning background task for high-quality comparison (failover was not needed).")
            asyncio.create_task(_run_background_asr_and_translation_analysis(
                original_audio_path=temp_input_audio_path,
                elp1_transcription_final_for_client=final_text_for_client,
                elp1_translation_final_for_client=None,
                session_id_for_log=session_id_for_log,
                request_id=request_id,
                language_asr=language_for_asr_steps,
                target_language_translation=None
            ))
            temp_input_audio_path = None  # Background task now owns the file for cleanup
        else:
            logger.info(f"{request_id}: Skipping background task because high-quality failover was performed.")
            # The main handler is now responsible for cleanup, which will happen in the `finally` block.

    except (ValueError, RuntimeError, TaskInterruptedException) as e:
        # Handle specific, expected errors
        logger.warning(f"{request_id}: A handled error occurred in ASR endpoint: {type(e).__name__} - {e}")
        error_type = "invalid_request_error"
        status_code = 400
        if isinstance(e, RuntimeError):
            error_type = "server_error"
            status_code = 500
        elif isinstance(e, TaskInterruptedException):
            error_type = "server_error"
            status_code = 503

        resp_data, _ = _create_openai_error_response(str(e), err_type=error_type, status_code=status_code)
        resp = Response(json.dumps(resp_data), status=status_code, mimetype='application/json')
        final_status_code = status_code

    except Exception as e:
        # Handle all other unexpected errors
        logger.exception(f"{request_id}: 🔥🔥 Unhandled exception in ASR endpoint (transcription):")
        error_message = f"Internal server error in ASR endpoint: {type(e).__name__}"
        resp_data_err, status_code_err = _create_openai_error_response(error_message, status_code=500)
        resp = Response(json.dumps(resp_data_err), status=status_code_err, mimetype='application/json')
        final_status_code = status_code_err

    finally:
        # --- Step 8: Final Cleanup ---
        # This block handles cleanup if the background task was skipped OR if an error occurred
        # before the background task could be spawned.
        if temp_input_audio_path and os.path.exists(temp_input_audio_path):
            if background_task_needed:
                logger.warning(
                    f"{request_id}: Cleaning up temp file in 'finally' due to an error before background task took ownership.")
            else:
                logger.info(f"{request_id}: Cleaning up temp file in 'finally' after successful ELP1 failover process.")
            try:
                # Use a thread for the blocking file I/O
                await asyncio.to_thread(os.remove, temp_input_audio_path)
            except Exception as e_del_final:
                logger.warning(f"{request_id}: Failed to delete temp file in main handler finally: {e_del_final}")

        # Close the manually created database session
        if db:
            db.close()
            logger.trace(f"Manually closed DB session for async request {request_id}")

        duration_req_total = (time.monotonic() - start_req_time) * 1000
        logger.info(
            f"🏁 OpenAI-Style ASR Request {request_id} (with Failover Logic) handled in {duration_req_total:.2f} ms. Status: {final_status_code}")

    if resp is None:
        logger.error(f"{request_id}: ASR Handler logic flaw - response object 'resp' was not assigned!")
        resp_data_err, _ = _create_openai_error_response("Internal error: Handler did not produce a response.",
                                                         status_code=500)
        resp = Response(json.dumps(resp_data_err), status=500, mimetype='application/json')

    return resp

# === NEW: Translation Audio Convo ===

@app.route("/v1/audio/translations", methods=["POST"])
async def handle_openai_audio_translations():
    start_req_time = time.monotonic()
    request_id = f"req-translate-{uuid.uuid4()}"
    logger.info(f"🚀 OpenAI-Style Audio Translation Request ID: {request_id} (Multi-Stage ELP1 + Background ELP0)")

    db: Session = g.db
    final_status_code: int = 500
    resp: Optional[Response] = None
    session_id_for_log: str = f"translate_req_{request_id}"
    uploaded_filename: Optional[str] = None
    temp_input_audio_path: Optional[str] = None  # Original uploaded file path

    # To store intermediate results for logging and background task
    raw_low_latency_transcription: Optional[str] = None
    corrected_transcription: Optional[str] = None
    diarized_transcription_for_client: Optional[str] = None  # This goes into translation
    quick_translated_text_for_client: Optional[str] = None  # This goes into TTS

    try:
        if not ENABLE_ASR:  # Master switch
            error_msg = "ASR/Translation capability disabled."
            logger.error(f"{request_id}: {error_msg}")
            resp_data, status_code = _create_openai_error_response(error_msg, err_type="server_error",
                                                                   code="translation_disabled", status_code=503)
            resp = Response(json.dumps(resp_data), status=status_code, mimetype='application/json')
            final_status_code = status_code
            return resp  # type: ignore

        # --- 1. Request Parsing & File Save (as before) ---
        if not request.content_type or not request.content_type.startswith('multipart/form-data'):
            raise ValueError("Invalid content type. Must be multipart/form-data.")
        audio_file_storage = request.files.get('file')
        model_requested = request.form.get('model')  # Should match AUDIO_TRANSLATION_MODEL_CLIENT_FACING
        target_language_code = request.form.get('target_language', DEFAULT_TRANSLATION_TARGET_LANGUAGE).lower()
        source_language_code_asr = request.form.get('source_language', "auto").lower()
        output_voice_requested = request.form.get('voice')
        output_audio_format = request.form.get('response_format', 'mp3').lower()
        session_id_for_log = request.form.get("session_id", session_id_for_log)
        if cortex_text_interaction: cortex_text_interaction.current_session_id = session_id_for_log
        if audio_file_storage and audio_file_storage.filename: uploaded_filename = secure_filename(
            audio_file_storage.filename)

        if not audio_file_storage: raise ValueError("'file' field is required.")
        if not model_requested or model_requested != AUDIO_TRANSLATION_MODEL_CLIENT_FACING:
            raise ValueError(f"Invalid 'model'. Expected '{AUDIO_TRANSLATION_MODEL_CLIENT_FACING}'.")

        temp_audio_dir = os.path.join(SCRIPT_DIR, "temp_audio_worker_files")
        await asyncio.to_thread(os.makedirs, temp_audio_dir, exist_ok=True)
        _, file_extension = os.path.splitext(uploaded_filename or ".tmpaud")
        # Create a uniquely named temp file that persists until explicitly deleted
        temp_fd, temp_input_audio_path = tempfile.mkstemp(prefix="translate_orig_", suffix=file_extension,
                                                          dir=temp_audio_dir)
        os.close(temp_fd)  # We just want the name; save will reopen and write
        await asyncio.to_thread(audio_file_storage.save, temp_input_audio_path)
        logger.info(f"{request_id}: Input audio saved to: {temp_input_audio_path}")

        # --- ELP1 PIPELINE ---
        # --- Step 1.1: Low-Latency ASR ---
        logger.info(f"{request_id}: ELP1 Step 1.1: Low-Latency ASR (Model: {WHISPER_LOW_LATENCY_MODEL_FILENAME})...")
        asr_worker_script = os.path.join(SCRIPT_DIR, "audio_worker.py")
        ll_asr_cmd = [APP_PYTHON_EXECUTABLE, asr_worker_script, "--task-type", "asr", "--model-dir", WHISPER_MODEL_DIR,
                      "--temp-dir", temp_audio_dir]
        ll_asr_req_data = {"input_audio_path": temp_input_audio_path,
                           "whisper_model_name": WHISPER_LOW_LATENCY_MODEL_FILENAME,
                           "language": source_language_code_asr, "request_id": f"{request_id}-llasr"}

        ll_asr_resp, ll_asr_err = await asyncio.to_thread(_execute_audio_worker_with_priority, ll_asr_cmd,
                                                          ll_asr_req_data, ELP1, SCRIPT_DIR, ASR_WORKER_TIMEOUT)
        if ll_asr_err or not (
                ll_asr_resp and isinstance(ll_asr_resp.get("result"), dict) and "text" in ll_asr_resp["result"]):
            raise RuntimeError(f"Low-Latency ASR failed: {ll_asr_err or 'Invalid ASR worker response'}")
        raw_low_latency_transcription = ll_asr_resp["result"]["text"]
        logger.info(
            f"{request_id}: ELP1 Step 1.1: Low-Latency ASR successful. Snippet: '{raw_low_latency_transcription[:100]}...'")

        # --- Step 1.2: Auto-Correction (LLM ELP1) ---
        corrected_transcription = raw_low_latency_transcription
        if raw_low_latency_transcription and raw_low_latency_transcription.strip():
            logger.info(f"{request_id}: ELP1 Step 1.2: Auto-correcting transcript...")
            correction_prompt = PROMPT_AUTOCORRECT_TRANSCRIPTION.format(
                raw_transcribed_text=raw_low_latency_transcription)
            correction_session_id = f"correct_{request_id}"
            if cortex_text_interaction: cortex_text_interaction.current_session_id = correction_session_id
            llm_correction_output = await cortex_text_interaction.direct_generate(db, correction_prompt, correction_session_id, None,
                                                                  None)
            if llm_correction_output and not (
                    isinstance(llm_correction_output, str) and "ERROR" in llm_correction_output.upper()):
                corrected_transcription = llm_correction_output.strip()
                logger.info(f"{request_id}: Auto-correction successful. Snippet: '{corrected_transcription[:100]}...'")
            else:
                logger.warning(
                    f"{request_id}: Auto-correction failed/error: '{llm_correction_output}'. Using raw LL ASR text.")

        # --- Step 1.3: Diarization (LLM ELP1) ---
        diarized_text_for_translation = corrected_transcription
        if corrected_transcription and corrected_transcription.strip():
            logger.info(f"{request_id}: ELP1 Step 1.3: Diarizing transcript...")
            diarization_prompt = PROMPT_SPEAKER_DIARIZATION.format(transcribed_text=corrected_transcription)
            diarization_session_id = f"diarize_{request_id}"
            if cortex_text_interaction: cortex_text_interaction.current_session_id = diarization_session_id
            llm_diarization_output = await cortex_text_interaction.direct_generate(db, diarization_prompt, diarization_session_id, None,
                                                                   None)
            if llm_diarization_output and not (
                    isinstance(llm_diarization_output, str) and "ERROR" in llm_diarization_output.upper()):
                diarized_text_for_translation = llm_diarization_output.strip()
                logger.info(
                    f"{request_id}: Diarization successful. Snippet: '{diarized_text_for_translation[:100]}...'")
            else:
                logger.warning(
                    f"{request_id}: Diarization failed/error: '{llm_diarization_output}'. Using non-diarized (but corrected) text.")

        # --- Step 1.4: Quick Translation (LLM ELP1) ---
        logger.info(
            f"{request_id}: ELP1 Step 1.4: Translating to '{target_language_code}' (LLM role '{TRANSLATION_LLM_ROLE}')...")
        translation_model = cortex_backbone_provider.get_model(TRANSLATION_LLM_ROLE)  # type: ignore
        if not translation_model: raise RuntimeError(f"LLM role '{TRANSLATION_LLM_ROLE}' for translation unavailable.")

        src_lang_full = langcodes.Language.make(
            language=source_language_code_asr).display_name() if source_language_code_asr != "auto" else "Unknown (auto-detect)"
        tgt_lang_full = langcodes.Language.make(language=target_language_code).display_name()

        trans_prompt_input = {"text_to_translate": diarized_text_for_translation,
                              "target_language_full_name": tgt_lang_full, "target_language_code": target_language_code,
                              "source_language_full_name": src_lang_full,
                              "source_language_code": source_language_code_asr}
        trans_chain = ChatPromptTemplate.from_template(PROMPT_TRANSLATE_TEXT) | translation_model | StrOutputParser()
        trans_timing_data = {"session_id": session_id_for_log, "mode": "translation_elp1"}

        # _call_llm_with_timing is sync, direct_generate (which uses it) is async.
        # If direct_generate is called, it handles the threading.
        # For a direct chain invoke like this, we need asyncio.to_thread for the sync _call_llm_with_timing
        quick_translated_text_for_client = await asyncio.to_thread(
            cortex_text_interaction._call_llm_with_timing, trans_chain, trans_prompt_input, trans_timing_data, priority=ELP1
            # type: ignore
        )
        if not quick_translated_text_for_client or (isinstance(quick_translated_text_for_client, str) and (
                "ERROR" in quick_translated_text_for_client.upper() or "Traceback" in quick_translated_text_for_client)):
            raise RuntimeError(f"ELP1 LLM translation failed. Response: {quick_translated_text_for_client}")
        quick_translated_text_for_client = quick_translated_text_for_client.strip()
        logger.info(
            f"{request_id}: ELP1 Step 1.4: Quick translation successful. Snippet: '{quick_translated_text_for_client[:100]}...'")

        # --- Step 1.5: TTS of Quick Translation (ELP1) ---
        logger.info(f"{request_id}: ELP1 Step 1.5: Synthesizing quick translated text to audio...")
        final_tts_voice = output_voice_requested
        if not final_tts_voice:
            lang_map = {"en": "EN-US", "es": "ES-ES", "fr": "FR-FR", "de": "DE-DE", "zh": "ZH-CN", "ja": "JP-JA",
                        "ko": "KO-KR"}
            final_tts_voice = lang_map.get(target_language_code, f"{target_language_code.upper()}-US")

        tts_worker_cmd = [APP_PYTHON_EXECUTABLE, asr_worker_script, "--task-type", "tts", "--model-lang",
                          target_language_code.upper(), "--model-dir", WHISPER_MODEL_DIR, "--temp-dir", temp_audio_dir,
                          "--device", "auto"]
        tts_req_data = {"input": quick_translated_text_for_client, "voice": final_tts_voice,
                        "response_format": output_audio_format, "request_id": f"{request_id}-elp1-tts"}

        tts_resp, tts_err = await asyncio.to_thread(_execute_audio_worker_with_priority, tts_worker_cmd, tts_req_data,
                                                    ELP1, SCRIPT_DIR, TTS_WORKER_TIMEOUT)
        if tts_err or not (
                tts_resp and isinstance(tts_resp.get("result"), dict) and "audio_base64" in tts_resp["result"]):
            raise RuntimeError(f"ELP1 TTS step failed: {tts_err or 'Invalid TTS worker response'}")

        audio_info = tts_resp["result"]
        audio_b64_data = audio_info["audio_base64"]
        final_audio_format = audio_info.get("format", output_audio_format)
        final_mime_type = audio_info.get("mime_type", f"audio/{final_audio_format}")
        logger.info(
            f"{request_id}: ELP1 Step 1.5: Quick translated audio synthesis successful. Format: {final_audio_format}.")

        # --- Return ELP1 Audio Response to Client ---
        audio_bytes = base64.b64decode(audio_b64_data)
        resp = Response(audio_bytes, status=200, mimetype=final_mime_type)
        final_status_code = 200

        # --- Log the ELP1 pipeline outcome (before spawning background) ---
        elp1_log_summary = (f"ELP1 Pipeline for Request {request_id}:\n"
                            f"LL ASR: '{raw_low_latency_transcription[:70]}...' -> \n"
                            f"Corrected: '{corrected_transcription[:70]}...' -> \n"
                            f"Diarized: '{diarized_text_for_translation[:70]}...' -> \n"
                            f"Translated: '{quick_translated_text_for_client[:70]}...' -> \n"
                            f"TTS Output ({final_tts_voice}, {final_audio_format})")
        await asyncio.to_thread(add_interaction, db, session_id=session_id_for_log, mode="audio_translation",
                                input_type="elp1_pipeline_summary",
                                user_input=f"[AudioTranslate ELP1 - File: {uploaded_filename or 'UnknownFile'}]",
                                llm_response=elp1_log_summary, classification="translation_elp1_successful",
                                execution_time_ms=(time.monotonic() - start_req_time) * 1000)
        await asyncio.to_thread(db.commit)

        # --- Spawn Background Task for High-Quality ASR & Deeper Translation (ELP0) ---
        logger.info(f"{request_id}: Spawning background task for high-quality ASR & deep translation (ELP0)...")
        asyncio.create_task(_run_background_asr_and_translation_analysis(
            original_audio_path=temp_input_audio_path,
            elp1_transcription_final_for_client=diarized_text_for_translation,  # The text that was translated for ELP1
            elp1_translation_final_for_client=quick_translated_text_for_client,  # The ELP1 translation
            session_id_for_log=session_id_for_log,
            request_id=request_id,
            language_asr=source_language_code_asr,  # Language used for ASR
            target_language_translation=target_language_code  # Target language for translation
        ))
        temp_input_audio_path = None  # Background task now owns the temp file for its ASR pass

    # ... (existing except ValueError, FileNotFoundError, RuntimeError, TaskInterruptedException, Exception as e blocks from response #67) ...
    # Ensure all db operations and file operations in except/finally are wrapped in asyncio.to_thread
    except ValueError as ve:
        logger.warning(f"{request_id}: Invalid Audio Translation request: {ve}")
        resp_data, status_code = _create_openai_error_response(str(ve), err_type="invalid_request_error",
                                                               status_code=400)
        resp = Response(json.dumps(resp_data), status=status_code, mimetype='application/json')
        final_status_code = status_code
    except FileNotFoundError as fnf_err:
        logger.error(f"{request_id}: Server configuration error for Audio Translation: {fnf_err}")
        resp_data, status_code = _create_openai_error_response(f"Server configuration error: {fnf_err}",
                                                               err_type="server_error", status_code=500)
        resp = Response(json.dumps(resp_data), status=status_code, mimetype='application/json')
        final_status_code = status_code
    except RuntimeError as rt_err:
        logger.error(f"{request_id}: Audio Translation pipeline error: {rt_err}")
        resp_data, status_code = _create_openai_error_response(f"Audio Translation failed: {rt_err}",
                                                               err_type="server_error", status_code=500)
        resp = Response(json.dumps(resp_data), status=status_code, mimetype='application/json')
        final_status_code = status_code
    except TaskInterruptedException as tie:
        logger.warning(f"🚦 {request_id}: Audio Translation task INTERRUPTED: {tie}")
        resp_data, status_code = _create_openai_error_response(f"Translation task interrupted: {tie}",
                                                               err_type="server_error", code="task_interrupted",
                                                               status_code=503)
        resp = Response(json.dumps(resp_data), status=status_code, mimetype='application/json')
        final_status_code = status_code
    except Exception as e:
        logger.exception(f"{request_id}: 🔥🔥 Unhandled exception in Audio Translation endpoint:")
        error_message = f"Internal server error in Audio Translation endpoint: {type(e).__name__}"
        resp_data, status_code = _create_openai_error_response(error_message, status_code=500)
        resp = Response(json.dumps(resp_data), status=status_code, mimetype='application/json')
        final_status_code = status_code
        try:
            if db:
                await asyncio.to_thread(add_interaction, db, session_id=session_id_for_log, mode="audio_translation",
                                        input_type='error',
                                        user_input=f"AudioTranslation Handler Error. File: {uploaded_filename or 'N/A'}",
                                        llm_response=error_message[:2000])
                await asyncio.to_thread(db.commit)
        except Exception as db_err_log:
            logger.error(f"{request_id}: ❌ Failed log AudioTranslation handler error: {db_err_log}")
    finally:
        # If temp_input_audio_path was not passed to a background task (e.g., due to early error)
        # or if the background task is designed to copy it, then delete it here.
        # Current design: background task uses the original path, so it's responsible for deletion OR we pass a copy.
        # The _run_background_asr_and_translation_analysis now handles deleting the original_audio_path.
        if temp_input_audio_path and os.path.exists(temp_input_audio_path):
            logger.warning(
                f"{request_id}: Original temp audio file '{temp_input_audio_path}' was not consumed by a background task or an error occurred before spawning. Deleting.")
            try:
                await asyncio.to_thread(os.remove, temp_input_audio_path)
                logger.info(
                    f"{request_id}: Deleted temporary input audio file (in main handler finally): {temp_input_audio_path}")
            except Exception as e_del:
                logger.warning(
                    f"{request_id}: Failed to delete temporary input audio file '{temp_input_audio_path}' (in main handler finally): {e_del}")

        duration_req = (time.monotonic() - start_req_time) * 1000
        logger.info(
            f"🏁 OpenAI-Style Audio Translation Request {request_id} handled in {duration_req:.2f} ms. Status: {final_status_code}")

    if resp is None:
        logger.error(f"{request_id}: Audio Translation Handler logic flaw - response object 'resp' was not assigned!")
        resp_data, _ = _create_openai_error_response("Internal error: Handler failed to produce a response.",
                                                     status_code=500)
        resp = Response(json.dumps(resp_data), status=500, mimetype='application/json')
    return resp


# === NEW: OpenAI Compatible TTS Endpoint ===
@app.route("/v1/audio/speech", methods=["POST"])
async def handle_openai_tts():  # <<< CHANGED to async def
    """
    Handles requests mimicking OpenAI's Text-to-Speech endpoint.
    CORRECTED: Now runs as a non-blocking async route, executing the
    synchronous audio worker in a separate thread.
    """
    start_req = time.monotonic()
    request_id = f"req-tts-{uuid.uuid4()}"
    logger.info(f"🚀 Async OpenAI-Style TTS Request ID: {request_id} (Worker ELP1)")

    db: Session = g.db
    session_id: str = f"tts_req_default_{request_id}"
    raw_request_data: Optional[Dict[str, Any]] = None
    final_response_status_code: int = 500
    resp: Optional[Response] = None
    request_data_snippet_for_log: str = "No request data processed"

    try:
        # Use request.get_json() directly as Flask context is available
        raw_request_data = request.get_json()
        if not raw_request_data:
            raise ValueError("Empty JSON payload received.")

        request_data_snippet_for_log = json.dumps(raw_request_data)[:1000]

        input_text = raw_request_data.get("input")
        model_requested = raw_request_data.get("model")
        voice_requested = raw_request_data.get("voice")
        session_id = raw_request_data.get("session_id", session_id)
        response_format_requested = raw_request_data.get("response_format", "mp3").lower()

        logger.debug(
            f"{request_id}: TTS Request Parsed - SessionID: {session_id}, Input: '{str(input_text)[:50]}...', "
            f"Client Model Req: {model_requested}, Internal Voice/Speaker: {voice_requested}, "
            f"Format: {response_format_requested}"
        )

        if not input_text or not isinstance(input_text, str):
            raise ValueError("'input' field is required and must be a string.")
        if not model_requested or not isinstance(model_requested, str):
            raise ValueError("'model' field (e.g., 'Zephyloid-Alpha') is required.")
        if not voice_requested or not isinstance(voice_requested, str):
            raise ValueError("'voice' field (MeloTTS speaker ID, e.g., EN-US) is required.")

        if model_requested != TTS_MODEL_NAME_CLIENT_FACING:
            logger.warning(
                f"{request_id}: Invalid TTS model requested '{model_requested}'. Falling back to default.")

        melo_language = "EN"
        try:
            lang_part = voice_requested.split('-')[0].upper()
            supported_melo_langs = ["EN", "ZH", "JP", "ES", "FR", "KR", "DE"]
            if lang_part in supported_melo_langs:
                melo_language = lang_part
        except Exception:
            logger.warning(
                f"{request_id}: Could not infer language from voice '{voice_requested}'. Defaulting to EN.")

        audio_worker_script_path = os.path.join(SCRIPT_DIR, "audio_worker.py")
        if not os.path.exists(audio_worker_script_path):
            raise FileNotFoundError(f"Audio worker script missing at {audio_worker_script_path}")

        temp_audio_dir = os.path.join(SCRIPT_DIR, "temp_audio_worker_files")
        os.makedirs(temp_audio_dir, exist_ok=True)

        worker_command = [
            APP_PYTHON_EXECUTABLE, audio_worker_script_path,
            "--task-type", "tts", "--model-lang", melo_language,
            "--device", "auto", "--model-dir", WHISPER_MODEL_DIR,
            "--temp-dir", temp_audio_dir
        ]
        worker_request_data = {
            "input": input_text, "voice": voice_requested,
            "response_format": response_format_requested, "request_id": request_id
        }

        logger.info(f"{request_id}: Executing audio worker with ELP1 priority in a background thread...")

        # <<< KEY CHANGE: Run the blocking function in a thread >>>
        parsed_response_from_worker, error_string_from_worker = await asyncio.to_thread(
            _execute_audio_worker_with_priority,
            worker_command=worker_command,
            request_data=worker_request_data,
            priority=ELP1,
            worker_cwd=SCRIPT_DIR,
            timeout=TTS_WORKER_TIMEOUT
        )
        # <<< END KEY CHANGE >>>

        if error_string_from_worker:
            raise RuntimeError(f"Audio generation failed: {error_string_from_worker}")

        elif parsed_response_from_worker and "result" in parsed_response_from_worker and "audio_base64" in \
                parsed_response_from_worker["result"]:
            audio_info = parsed_response_from_worker["result"]
            audio_b64_data = audio_info["audio_base64"]
            actual_audio_format = audio_info.get("format", "mp3")
            response_mime_type = audio_info.get("mime_type", f"audio/{actual_audio_format}")

            logger.info(
                f"{request_id}: Audio successfully generated. Format: {actual_audio_format}, Length (b64): {len(audio_b64_data)}")

            audio_bytes = base64.b64decode(audio_b64_data)
            resp = Response(audio_bytes, status=200, mimetype=response_mime_type)
            final_response_status_code = 200
        else:
            raise RuntimeError(f"Audio worker returned invalid or incomplete response: {parsed_response_from_worker}")

    except ValueError as ve:
        logger.warning(f"{request_id}: Invalid TTS request parameters: {ve}")
        resp_data, status_code = _create_openai_error_response(str(ve), err_type="invalid_request_error",
                                                               status_code=400)
        resp = Response(json.dumps(resp_data), status=status_code, mimetype='application/json')
        final_response_status_code = status_code
    except (FileNotFoundError, RuntimeError) as server_err:
        logger.error(f"{request_id}: Server-side error during TTS: {server_err}")
        resp_data, status_code = _create_openai_error_response(str(server_err), err_type="server_error",
                                                               status_code=500)
        resp = Response(json.dumps(resp_data), status=status_code, mimetype='application/json')
        final_response_status_code = status_code
    except Exception as main_handler_err:
        logger.exception(f"{request_id}: 🔥🔥 Unhandled exception in TTS endpoint:")
        error_message = f"Internal server error in TTS endpoint: {type(main_handler_err).__name__}"
        resp_data, status_code = _create_openai_error_response(error_message, status_code=500)
        resp = Response(json.dumps(resp_data), status=status_code, mimetype='application/json')
        final_response_status_code = status_code
        try:
            if db:
                add_interaction(db, session_id=session_id, mode="tts", input_type='error',
                                user_input=f"TTS Handler Error. Request: {request_data_snippet_for_log}",
                                llm_response=error_message[:2000])
                db.commit()
        except Exception as db_err_log:
            logger.error(f"{request_id}: ❌ Failed log TTS handler error to DB: {db_err_log}")
            if db: db.rollback()
    finally:
        duration_req = (time.monotonic() - start_req) * 1000
        logger.info(
            f"🏁 Async OpenAI-Style TTS Request {request_id} handled in {duration_req:.2f} ms. Final Status: {final_response_status_code}")

    if resp is None:
        logger.error(f"{request_id}: TTS Handler logic flaw - response object 'resp' was not assigned!")
        resp_data, _ = _create_openai_error_response("Internal error: Handler failed to produce a response object.",
                                                     status_code=500)
        resp = Response(json.dumps(resp_data), status=500, mimetype='application/json')

    return resp

# === NEW: OpenAI Compatible Image Generation Endpoint (Stub) ===
@app.route("/v1/images/generations", methods=["POST"])
async def handle_openai_image_generations():  # Route is async
    start_req = time.monotonic()
    request_id = f"req-img-gen-{uuid.uuid4()}"
    logger.info(f"🚀 OpenAI-Style Image Generation Request ID: {request_id} (ELP1 Priority)")

    # Get DB session from Flask's g or create if not present (ensure before_request/teardown_request handle this)
    # For simplicity here, assuming g.db is correctly managed by Flask context handlers
    db: Optional[Session] = getattr(g, 'db', None)
    if db is None:  # Fallback if g.db is not set (e.g. if before_request failed or not run)
        logger.error(f"{request_id}: DB session not found in g.db. Creating temporary session for this request.")
        db_temp_session = SessionLocal()  # type: ignore
        db_to_use = db_temp_session
    else:
        db_to_use = db

    final_response_status_code: int = 500
    resp: Optional[Response] = None
    session_id_for_log: str = f"img_gen_req_default_{request_id}"
    raw_request_data: Optional[Dict[str, Any]] = None
    request_data_snippet_for_log: str = "No request data processed"

    try:
        # --- 1. Get and Validate Request JSON Body ---
        try:
            raw_request_data = request.get_json()  # Use await if using Quart, or request.get_json() for Flask
            if not raw_request_data:
                raise ValueError("Empty JSON payload received.")
            try:
                request_data_snippet_for_log = json.dumps(raw_request_data)[:1000]
            except:
                request_data_snippet_for_log = str(raw_request_data)[:1000]
        except Exception as json_err:
            logger.warning(f"{request_id}: Failed to get/parse JSON body: {json_err}")
            try:
                request_data_snippet_for_log = (await request.get_data(as_text=True))[:1000]  # await for Quart
            except:
                request_data_snippet_for_log = "Could not read request body"
            resp_data, status_code_val = _create_openai_error_response(
                f"Request body is missing or invalid JSON: {json_err}", err_type="invalid_request_error",
                status_code=400)
            resp = Response(json.dumps(resp_data), status=status_code_val, mimetype='application/json')
            final_response_status_code = status_code_val
            return resp  # Early return

        # --- 2. Extract Expected Parameters ---
        prompt_from_user = raw_request_data.get("prompt")
        model_requested = raw_request_data.get("model")
        n_images_requested_by_client = raw_request_data.get("n")
        size_requested_str = raw_request_data.get("size", IMAGE_GEN_DEFAULT_SIZE)
        response_format_requested = raw_request_data.get("response_format", "b64_json").lower()
        # Optional OpenAI params, currently logged but not all used by worker
        quality_requested = raw_request_data.get("quality", "standard")
        style_requested = raw_request_data.get("style", "vivid")
        user_provided_id_for_tracking = raw_request_data.get("user")

        session_id_for_log = raw_request_data.get("session_id", session_id_for_log)

        if cortex_text_interaction:  # Ensure cortex_text_interaction global instance is available
            cortex_text_interaction.current_session_id = session_id_for_log  # Set for helpers in cortex_text_interaction
        else:
            logger.error(
                f"{request_id}: Global 'cortex_backbone_provider' instance not available. Cannot proceed with image generation context.")
            resp_data, status_code_val = _create_openai_error_response("Server AI component (CortexThoughts) not ready.",
                                                                       err_type="server_error", status_code=503)
            resp = Response(json.dumps(resp_data), status=status_code_val, mimetype='application/json')
            final_response_status_code = status_code_val
            return resp

        # Determine n_images (number of images to generate)
        if n_images_requested_by_client is None:
            n_images = 1  # Default to 1 image for this direct ELP1 endpoint.
            # Previous default was 2, changed to 1 for faster single user requests.
            # Can be configured or kept at 1. Max can be capped.
            logger.info(f"{request_id}: 'n' not specified, defaulting to {n_images} for ELP1 image generation.")
        else:
            try:
                n_images = int(n_images_requested_by_client)
                if n_images < 1: n_images = 1  # Min 1
                # MAX_IMAGES_PER_REQUEST = 4 # Example cap
                # if n_images > MAX_IMAGES_PER_REQUEST: n_images = MAX_IMAGES_PER_REQUEST; logger.warning(...)
            except ValueError:
                logger.warning(f"{request_id}: Invalid 'n': '{n_images_requested_by_client}'. Defaulting to 1.")
                n_images = 1

        logger.debug(
            f"{request_id}: Image Gen Request Parsed - Prompt: '{str(prompt_from_user)[:50]}...', "
            f"ModelReq: {model_requested}, N (final): {n_images}, Size: {size_requested_str}, RespFormat: {response_format_requested}"
        )

        # --- 3. Validate Core Parameters ---
        if not prompt_from_user or not isinstance(prompt_from_user, str):
            raise ValueError("'prompt' field is required and must be a string for image generation.")
        if not model_requested or model_requested != IMAGE_GEN_MODEL_NAME_CLIENT_FACING:
            raise ValueError(f"Invalid 'model'. This endpoint supports '{IMAGE_GEN_MODEL_NAME_CLIENT_FACING}'.")
        if response_format_requested not in ["b64_json", "url"]:
            logger.warning(
                f"{request_id}: Invalid 'response_format': {response_format_requested}. Defaulting to 'b64_json'.")
            response_format_requested = "b64_json"

        # --- 4. Refine User Prompt using RAG Context (ELP1) ---
        logger.info(f"{request_id}: Refining user prompt for image generation (ELP1)...")

        wrapped_rag_result = await asyncio.to_thread(
            cortex_text_interaction._get_rag_retriever_thread_wrapper,
            db_to_use,
            prompt_from_user,  # Use original prompt for RAG context query
            ELP1
        )

        session_hist_retriever_for_refine: Optional[Any] = None
        if wrapped_rag_result.get("status") == "success":
            rag_data_tuple = wrapped_rag_result.get("data")
            if isinstance(rag_data_tuple, tuple) and len(rag_data_tuple) == 4:
                _url_ret_temp, session_hist_retriever_for_refine, _refl_ret_temp, _ids_temp = rag_data_tuple
            else:  # Should be caught by wrapper, but safeguard
                raise RuntimeError(f"RAG wrapper returned unexpected data structure for image prompt: {rag_data_tuple}")
        elif wrapped_rag_result.get("status") == "interrupted":
            raise TaskInterruptedException(wrapped_rag_result.get("error_message", "RAG for image prompt interrupted"))
        else:  # Error
            raise RuntimeError(
                f"RAG for image prompt refinement failed: {wrapped_rag_result.get('error_message', 'Unknown RAG error')}")

        retrieved_history_docs = []
        if session_hist_retriever_for_refine:
            retrieved_history_docs = await asyncio.to_thread(session_hist_retriever_for_refine.invoke, prompt_from_user)

        history_rag_str = cortex_text_interaction._format_docs(retrieved_history_docs, source_type="History RAG")

        direct_hist_interactions_list = await asyncio.to_thread(get_global_recent_interactions, db_to_use, limit=3)
        recent_direct_history_str = cortex_text_interaction._format_direct_history(direct_hist_interactions_list)

        refined_prompt_for_generation = await cortex_text_interaction._refine_direct_image_prompt_async(
            db=db_to_use, session_id=session_id_for_log, user_image_request=prompt_from_user,
            history_rag_str=history_rag_str, recent_direct_history_str=recent_direct_history_str,
            priority=ELP1
        )

        if not refined_prompt_for_generation or refined_prompt_for_generation == prompt_from_user:
            logger.info(f"{request_id}: Prompt refinement yielded no change or failed. Using original prompt.")
            refined_prompt_for_generation = prompt_from_user
        else:
            logger.info(f"{request_id}: Using refined prompt for image generation: '{refined_prompt_for_generation}'")

        try:  # Log the prompt that will be used
            add_interaction(db_to_use, session_id=session_id_for_log, mode="image_gen",
                            input_type="text_prompt_to_img_worker",
                            user_input=prompt_from_user,  # Original prompt
                            llm_response=refined_prompt_for_generation)  # Refined prompt sent to worker
            db_to_use.commit()
        except Exception as db_log_err_prompt:
            logger.error(f"{request_id}: Failed to log refined image prompt: {db_log_err_prompt}")
            if db_to_use: db_to_use.rollback()

        # --- 5. Generate Image(s) using CortexEngine (ELP1) ---
        logger.info(
            f"{request_id}: Requesting {n_images} image(s) from CortexEngine (ELP1). Prompt: '{refined_prompt_for_generation[:100]}...'")
        all_generated_image_data_items = []
        error_during_loop = False
        loop_error_message = None

        for i in range(n_images):
            if error_during_loop: break
            logger.info(f"{request_id}: Generating image {i + 1}/{n_images}...")
            # cortex_backbone_provider.generate_image_async returns: Tuple[Optional[List[Dict[str, Optional[str]]]], Optional[str]]
            # The first element is a list of image data dicts (usually one dict per call for this worker)
            # The second is an error message string if any.
            list_of_one_image_dict, image_gen_err_msg = await cortex_backbone_provider.generate_image_async(
                prompt=refined_prompt_for_generation,
                image_base64=None,  # This endpoint is for txt2img primarily
                priority=ELP1
            )
            if image_gen_err_msg:
                logger.error(f"{request_id}: Image generation failed for attempt {i + 1}: {image_gen_err_msg}")
                loop_error_message = image_gen_err_msg
                if interruption_error_marker in image_gen_err_msg: raise TaskInterruptedException(image_gen_err_msg)
                error_during_loop = True
                break
            elif list_of_one_image_dict and isinstance(list_of_one_image_dict, list) and list_of_one_image_dict:
                all_generated_image_data_items.append(list_of_one_image_dict[0])  # Append the single image dict
                logger.info(f"{request_id}: Image {i + 1}/{n_images} data received.")
            else:
                logger.warning(f"{request_id}: Image generation attempt {i + 1} returned no data and no error.")
                loop_error_message = "Image worker returned no data for an image attempt."
                error_during_loop = True
                break

        if error_during_loop or not all_generated_image_data_items:
            final_err_msg = loop_error_message or "Image generation failed to produce any results."
            resp_data, status_code_val = _create_openai_error_response(final_err_msg, err_type="server_error",
                                                                       status_code=500)
            resp = Response(json.dumps(resp_data), status=status_code_val, mimetype='application/json')
            final_response_status_code = status_code_val
            return resp

        # --- 6. Format Response ---
        response_data_list_for_client = []
        for img_data_item in all_generated_image_data_items:
            png_b64 = img_data_item.get("b64_json")  # Expecting PNG base64 from worker
            # avif_b64 = img_data_item.get("b64_avif") # Worker might also provide AVIF
            if png_b64:
                if response_format_requested == "b64_json":
                    response_data_list_for_client.append({"b64_json": png_b64})
                elif response_format_requested == "url":  # Return data URI
                    logger.warning(f"{request_id}: Returning data URI for 'url' format image request.")
                    response_data_list_for_client.append({"url": f"data:image/png;base64,{png_b64}"})
            else:
                logger.error(f"{request_id}: Worker image data item missing 'b64_json' (PNG). Item: {img_data_item}")

        if not response_data_list_for_client:
            logger.error(f"{request_id}: No valid PNG b64_json data found after processing worker response(s).")
            resp_data, status_code_val = _create_openai_error_response("Failed to get valid image data from worker.",
                                                                       err_type="server_error", status_code=500)
            resp = Response(json.dumps(resp_data), status=status_code_val, mimetype='application/json')
            final_response_status_code = status_code_val
            return resp

        openai_response_body = {"created": int(time.time()), "data": response_data_list_for_client}
        response_payload = json.dumps(openai_response_body)
        resp = Response(response_payload, status=200, mimetype='application/json')
        final_response_status_code = 200

    except ValueError as ve:  # Catches explicit ValueErrors from parameter validation
        logger.warning(f"{request_id}: Invalid Image Gen request (ValueError): {ve}")
        resp_data, status_code_val = _create_openai_error_response(str(ve), err_type="invalid_request_error",
                                                                   status_code=400)
        resp = Response(json.dumps(resp_data), status=status_code_val, mimetype='application/json')
        final_response_status_code = status_code_val
    except TaskInterruptedException as tie:  # Catches interruptions from RAG or image gen
        logger.warning(f"🚦 {request_id}: Image Generation request (ELP1) INTERRUPTED: {tie}")
        resp_data, status_code_val = _create_openai_error_response(f"Image generation task was interrupted: {tie}",
                                                                   err_type="server_error", code="task_interrupted",
                                                                   status_code=503)
        resp = Response(json.dumps(resp_data), status=status_code_val, mimetype='application/json')
        final_response_status_code = status_code_val
    except Exception as e_main:  # Catch-all for other unexpected errors
        logger.exception(f"{request_id}: 🔥🔥 Unhandled exception in Image Gen endpoint main try block:")
        error_message = f"Internal server error in Image Gen endpoint: {type(e_main).__name__} - {str(e_main)}"
        resp_data, status_code_val = _create_openai_error_response(error_message, err_type="server_error",
                                                                   status_code=500)
        resp = Response(json.dumps(resp_data), status=status_code_val, mimetype='application/json')
        final_response_status_code = status_code_val
        try:
            if db_to_use:
                add_interaction(db_to_use, session_id=session_id_for_log, mode="image_gen", input_type='error',
                                user_input=f"Image Gen Handler Error. Req: {request_data_snippet_for_log}",
                                llm_response=error_message[:2000])
                db_to_use.commit()
        except Exception as db_err_log_main:
            logger.error(f"{request_id}: ❌ Failed log main Image Gen handler error to DB: {db_err_log_main}")
            if db_to_use: db_to_use.rollback()
    finally:
        duration_req = (time.monotonic() - start_req) * 1000
        logger.info(
            f"🏁 OpenAI-Style Image Gen Request {request_id} handled in {duration_req:.2f} ms. Final HTTP Status: {final_response_status_code}")
        if 'db_temp_session' in locals() and db_temp_session:  # Close temp session if created
            db_temp_session.close()
            logger.debug(f"{request_id}: Closed temporary DB session for image gen.")

    if resp is None:  # Should ideally not be reached if all paths assign to resp
        logger.error(f"{request_id}: Image Gen Handler logic flaw - response object 'resp' was not assigned!")
        resp_data, status_code_val = _create_openai_error_response("Internal error: Handler did not produce response.",
                                                                   err_type="server_error", status_code=500)
        resp = Response(json.dumps(resp_data), status=status_code_val, mimetype='application/json')
        try:
            if db_to_use: add_interaction(db_to_use, session_id=session_id_for_log, mode="image_gen",
                                          input_type='error',
                                          user_input=f"ImgGen NoResp. Req: {request_data_snippet_for_log}",
                                          llm_response="Critical: No resp obj created."); db_to_use.commit()
        except:
            pass
    return resp




# --- NEW: Dummy Handlers for Pretending this is Ollama Model Management ---

@app.route("/api/pull", methods=["POST"])
async def handle_api_pull_dummy():
    """Async dummy endpoint for /api/pull."""
    logger.warning("⚠️ Received async request for dummy endpoint: /api/pull Ollama impostor activated!")
    # This generator function is compatible with an async route.
    def generate_dummy_pull():
        yield json.dumps({"status": "pulling manifest"}) + "\n"
        time.sleep(0.5)
        yield json.dumps({"status": "verifying sha256:...", "total": 100, "completed": 50}) + "\n"
        time.sleep(0.5)
        yield json.dumps({"status": "success"}) + "\n"
    return Response(generate_dummy_pull(), mimetype='application/x-ndjson')

@app.route("/api/push", methods=["POST"])
async def handle_api_push_dummy():
    """Async dummy endpoint for /api/push."""
    logger.warning("⚠️ Received async request for dummy endpoint: /api/push Ollama impostor activated!")
    return jsonify({"error": "Model pushing not implemented in this server"}), 501

@app.route("/api/show", methods=["POST"])
async def handle_api_show_dummy():
    """Async dummy endpoint for /api/show."""
    logger.warning("⚠️ Received async request for dummy endpoint: /api/show Ollama impostor activated!")
    return jsonify({"error": "Showing model details not implemented in this server"}), 501

@app.route("/api/delete", methods=["DELETE"])
async def handle_api_delete_dummy():
    """Async dummy endpoint for /api/delete."""
    logger.warning("⚠️ Received async request for dummy endpoint: /api/delete Ollama impostor activated!")
    return jsonify({"status": "Model deletion not implemented"}), 501

@app.route("/api/create", methods=["POST"])
async def handle_api_create_dummy():
    """Async dummy endpoint for /api/create."""
    logger.warning("⚠️ Received async request for dummy endpoint: /api/create Ollama impostor activated!")
    return jsonify({"error": "Model creation from Modelfile not implemented"}), 501

@app.route("/api/copy", methods=["POST"])
async def handle_api_copy_dummy():
    """Async dummy endpoint for /api/copy."""
    logger.warning("⚠️ Received async request for dummy endpoint: /api/copy Ollama impostor activated!")
    return jsonify({"error": "Model copying not implemented"}), 501

@app.route("/api/blobs/<digest>", methods=["POST", "HEAD"])
async def handle_api_blobs_dummy(digest: str):
    """Async dummy endpoint for /api/blobs."""
    logger.warning(f"⚠️ Received async request for dummy endpoint: /api/blobs/{digest} Ollama impostor activated!")
    if request.method == 'HEAD':
        return Response(status=404)
    else: # POST
        return jsonify({"error": "Blob creation/checking not implemented"}), 501

@app.route("/api/version", methods=["GET", "HEAD"])
async def handle_api_version():
    """Async endpoint for /api/version."""
    logger.info("Received async request for /api/version")
    version_string = "Adelaide-Zephyrine-Charlotte-MetacognitionArtificialQuellia-0.0.1"
    response_data = {"version": version_string}
    return jsonify(response_data), 200

@app.route("/api/ps", methods=["GET"])
async def handle_api_ps_dummy():
    """Async dummy endpoint for /api/ps."""
    logger.warning("⚠️ Received async request for dummy endpoint: /api/ps (Not Implemented)")
    return jsonify({"models": []}), 200


#-=-=-=-=-=-

@app.route("/v1/models/<path:model>", methods=["GET"])
async def handle_openai_retrieve_model(model: str):
    """
    Asynchronous version of the handler for retrieving a single model's details,
    mimicking OpenAI's endpoint.
    """
    logger.info(f"Received async request for /v1/models/{model}")
    start_req = time.monotonic()
    status_code = 404

    # This logic is fast and CPU-bound, so no `await` is needed.
    known_models = [
        META_MODEL_NAME_STREAM,
        META_MODEL_NAME_NONSTREAM,
        MODERATION_MODEL_CLIENT_FACING,
        TTS_MODEL_NAME_CLIENT_FACING,
        ASR_MODEL_NAME_CLIENT_FACING,
        AUDIO_TRANSLATION_MODEL_CLIENT_FACING,
        IMAGE_GEN_MODEL_NAME_CLIENT_FACING
    ]

    if model in known_models:
        status_code = 200
        response_body = {
            "id": model,
            "object": "model",
            "created": int(time.time()),  # Placeholder timestamp
            "owned_by": META_MODEL_OWNER,
        }
    else:
        # For consistency with OpenAI, create a standard error object for 404
        response_body = {
            "error": {
                "message": f"The model '{model}' does not exist",
                "type": "invalid_request_error",
                "param": None,
                "code": "model_not_found"
            }
        }

    response = jsonify(response_body)
    response.status_code = status_code

    duration_req = (time.monotonic() - start_req) * 1000
    logger.info(f"🏁 /v1/models/{model} async request handled in {duration_req:.2f} ms. Status: {status_code}")
    return response

#------- Initialization of the provider --------

try:
    cortex_backbone_provider = CortexEngine(PROVIDER)
    global_cortex_backbone_provider_ref = cortex_backbone_provider
    cortex_text_interaction = CortexThoughts(cortex_backbone_provider) #compatibility
    AGENT_CWD = os.path.dirname(os.path.abspath(__file__))
    SUPPORTS_COMPUTER_USE = True
    ai_agent = AmaryllisAgent(cortex_backbone_provider, AGENT_CWD, SUPPORTS_COMPUTER_USE)
    logger.success("✅ AI Instances Initialized.")
except Exception as e:
    logger.critical(f"🔥🔥 Failed AI init: {e}")
    logger.exception("AI Init Traceback:")
    cortex_backbone_provider = None
    sys.exit(1)

## Fine tuning API Call handler

@app.route("/v1/fine_tuning/jobs", methods=["POST"])
async def handle_create_pseudo_fine_tuning_job():
    start_req_time = time.monotonic()
    request_id = f"req-pseudo-ft-job-{uuid.uuid4()}"
    logger.info(f"🚀 {request_id}: Received POST /v1/fine_tuning/jobs (Pseudo Fine-Tune via Data Ingestion)")

    db: Session = g.db
    final_status_code: int = 500
    resp: Optional[Response] = None
    session_id_for_log: str = f"pseudo_ft_req_{request_id}"  # Default session ID for this job's logs
    raw_request_data: Optional[Dict[str, Any]] = None

    try:
        raw_request_data = await request.get_json()
        if not raw_request_data:
            raise ValueError("Empty JSON payload received.")

        # The 'training_file' now refers to the 'ingestion_id' returned by /v1/files
        training_file_ingestion_id = raw_request_data.get("training_file")
        model_requested = raw_request_data.get("model")  # e.g., "gpt-3.5-turbo" - we log this

        if not training_file_ingestion_id or not isinstance(training_file_ingestion_id, str):
            raise ValueError(
                "'training_file' ID (which is the ingestion_id from /v1/files) is required and must be a string.")
        if not model_requested:
            raise ValueError("'model' (base model name to associate with this data) is required.")

        # Look up the uploaded file record
        uploaded_record = db.query(UploadedFileRecord).filter(
            UploadedFileRecord.ingestion_id == training_file_ingestion_id
        ).first()

        if uploaded_record is None:
            raise ValueError(
                f"Uploaded file record with ingestion_id '{training_file_ingestion_id}' not found. Please upload the file first via /v1/files.")

        if uploaded_record.status == "processing" or uploaded_record.status == "queued_for_processing" or uploaded_record.status == "completed":
            logger.warning(
                f"{request_id}: File ingestion for ID '{training_file_ingestion_id}' is already {uploaded_record.status}. Not re-triggering.")
            response_message = f"File ingestion for ID '{training_file_ingestion_id}' is already {uploaded_record.status}. No new processing initiated."

            # Mimic OpenAI's FineTuningJob object structure for response
            response_body = {
                "id": f"ftjob-reflect-{uploaded_record.ingestion_id}",
                "object": "fine_tuning.job",
                "model": model_requested,
                "created_at": int(uploaded_record.created_at.timestamp()),
                "finished_at": None,
                "fine_tuned_model": f"custom-model-via-reflection:{int(uploaded_record.created_at.timestamp())}",
                "organization_id": "org-placeholder",
                "result_files": [],
                "status": uploaded_record.status,  # Report current status
                "validation_file": raw_request_data.get("validation_file"),
                "training_file": training_file_ingestion_id,
                "hyperparameters": raw_request_data.get("hyperparameters", {"n_epochs": "continuous_self_reflection"}),
                "trained_tokens": None,
                "message": response_message
            }
            final_status_code = 200  # Indicate it's successfully acknowledged
            resp = Response(json.dumps(response_body), status=final_status_code, mimetype='application/json')
            return resp

        logger.info(
            f"{request_id}: Pseudo fine-tuning job requested. Session: {session_id_for_log}, Training File ID: '{training_file_ingestion_id}', Base Model: '{model_requested}'.")
        logger.info(
            f"{request_id}: Triggering background file ingestion and reflection for file at '{uploaded_record.stored_path}'.")

        # Update status to queued_for_processing
        uploaded_record.status = "queued_for_processing"
        uploaded_record.updated_at = func.now()
        db.commit()  # Commit the status change

        # Trigger the background processing helper.
        # It's crucial to call this via asyncio.create_task to run in the background
        # and not block the API response.
        asyncio.create_task(
            cortex_text_interaction._initiate_file_ingestion_and_reflection(
                db_session_from_caller=db,  # Pass the session from the route, the helper will create its own
                uploaded_file_record_id=uploaded_record.id
            )
        )

        # Create a job ID and mimic OpenAI's FineTuningJob object
        job_id = f"ftjob-reflect-{uploaded_record.ingestion_id}"
        current_timestamp = int(time.time())

        # This is the response body we'll build
        response_body = {
            "id": job_id,
            "object": "fine_tuning.job",
            "model": model_requested,
            "created_at": int(uploaded_record.created_at.timestamp()),  # Use file's creation timestamp
            "finished_at": None,  # Not finished yet
            "fine_tuned_model": None,  # Will be set once processing is complete conceptuallly
            "organization_id": "org-placeholder",
            "result_files": [],
            "status": "queued",  # Report that it's queued for processing
            "validation_file": raw_request_data.get("validation_file"),
            "training_file": training_file_ingestion_id,
            "hyperparameters": raw_request_data.get("hyperparameters", {"n_epochs": "continuous_self_reflection"}),
            "trained_tokens": None,
            "message": "File processing for self-reflection has been queued in the background. Check /v1/fine_tuning/jobs/{id} for status updates."
        }

        # Log this "pseudo fine-tuning job" creation to your interactions database
        await asyncio.to_thread(add_interaction, db,
                                session_id=session_id_for_log,
                                mode="fine_tune_stub",
                                input_type="job_creation_request",
                                user_input=f"Pseudo FT Job Created: file_id={training_file_ingestion_id}, base_model={model_requested}",
                                llm_response=json.dumps(
                                    {"job_id": job_id, "status": response_body.get("status", "unknown_status")}),
                                classification="pseudo_fine_tune_job_logged"
                                )
        await asyncio.to_thread(db.commit)  # Commit the summary log entry

        final_status_code = 200
        resp = Response(json.dumps(response_body), status=final_status_code, mimetype='application/json')

    except ValueError as ve:
        logger.warning(f"{request_id}: Invalid pseudo fine-tuning request: {ve}")
        response_body, final_status_code = _create_openai_error_response(str(ve), err_type="invalid_request_error",
                                                                         status_code=400)
        resp = Response(json.dumps(response_body), status=final_status_code, mimetype='application/json')
    except Exception as e:
        logger.exception(f"{request_id}: 🔥🔥 Error creating pseudo fine-tuning job:")
        response_body, final_status_code = _create_openai_error_response(
            f"Server error during pseudo fine-tuning job creation: {str(e)}", err_type="server_error", status_code=500)
        resp = Response(json.dumps(response_body), status=final_status_code, mimetype='application/json')
        if db: await asyncio.to_thread(db.rollback)  # Rollback if error after DB interaction started
    finally:
        duration_req = (time.monotonic() - start_req_time) * 1000
        logger.info(
            f"🏁 /v1/fine_tuning/jobs POST request {request_id} handled in {duration_req:.2f} ms. Status: {final_status_code}")

    if resp is None:
        logger.error(f"{request_id}: Pseudo FT Job handler logic flaw - response object 'resp' was not assigned!")
        response_body, final_status_code = _create_openai_error_response(
            "Internal error: Handler failed to produce response.", status_code=500)
        resp = Response(json.dumps(response_body), status=final_status_code, mimetype='application/json')

    return resp


# b) Stubs for other /v1/fine_tuning/jobs/* endpoints
# These can reuse the logic from response #61, just ensure the message is consistent.
# (GET /jobs, GET /jobs/{id}, POST /jobs/{id}/cancel, GET /jobs/{id}/events)
# Example for GET /v1/fine_tuning/jobs:

@app.route("/v1/fine_tuning/jobs", methods=["GET"])
async def handle_list_pseudo_fine_tuning_jobs():
    request_id = f"req-pseudo-ft-list-{uuid.uuid4()}"
    logger.info(f"🚀 {request_id}: Received GET /v1/fine_tuning/jobs (Pseudo Fine-Tune Info)")
    # In a real scenario, you might list your "ingestion job" records here.
    response_message = (
        "This system adapts via continuous self-reflection on ingested data and interactions, "
        "rather than discrete fine-tuning jobs. 'Jobs' here represent batches of data ingested for this learning process."
    )
    response_body = {
        "object": "list",
        "data": [
            # Example of what a "job" entry could represent
            # {
            #     "id": "ftjob-reflect-example123",
            #     "object": "fine_tuning.job",
            #     "model": "gpt-3.5-turbo", # Base model mentioned at ingestion
            #     "created_at": int(time.time()) - 86400,
            #     "fine_tuned_model": "custom-model-via-reflection:inprogress",
            #     "status": "processing_reflection_queue",
            #     "message": "Data batch 'file-ingest-job-abc' is being processed by self-reflection."
            # }
        ],
        "has_more": False,
        "message": response_message
    }
    return jsonify(response_body), 200

@app.route("/v1/fine_tuning/jobs/<string:fine_tuning_job_id>", methods=["GET"])
def handle_retrieve_fine_tuning_job(fine_tuning_job_id: str):
    request_id = f"req-ft-retrieve-{uuid.uuid4()}"
    logger.info(f"🚀 {request_id}: Received GET /v1/fine_tuning/jobs/{fine_tuning_job_id} (Placeholder)")

    if fine_tuning_job_id == "continuous_learning_main_process":
        response_message = (
            f"Details for '{fine_tuning_job_id}': This represents the system's ongoing adaptation "
            "through self-reflection, background analysis, and knowledge base updates. "
            "It does not have traditional job parameters like epochs or specific data files."
        )
        response_body = {
            "id": fine_tuning_job_id,
            "object": "fine_tuning.job",  # Or custom
            "model": "adaptive_system",
            "created_at": int(time.time()),  # Could be app start time
            "status": "active_and_ongoing",
            "message": response_message,
            "hyperparameters": {
                "learning_mechanisms": ["self_reflection", "rag_vector_learning", "background_generate_tot"]
            }
        }
        logger.info(f"{request_id}: Responding with placeholder details for continuous learning process.")
        return jsonify(response_body), 200
    else:
        response_message = (
            f"Fine-tuning job ID '{fine_tuning_job_id}' does not correspond to a traditional fine-tuning job. "
            "This system adapts via continuous self-reflection and knowledge base updates, not discrete fine-tuning jobs."
        )
        # OpenAI usually returns 404 for non-existent job IDs
        resp_data, _ = _create_openai_error_response(
            message=response_message,
            err_type="invalid_request_error",
            code="fine_tuning_job_not_found"
        )
        logger.info(f"{request_id}: Job ID '{fine_tuning_job_id}' not found as a traditional job.")
        return jsonify(resp_data), 404


@app.route("/v1/fine_tuning/jobs/<string:fine_tuning_job_id>/cancel", methods=["POST"])
def handle_cancel_fine_tuning_job(fine_tuning_job_id: str):
    request_id = f"req-ft-cancel-{uuid.uuid4()}"
    logger.info(f"🚀 {request_id}: Received POST /v1/fine_tuning/jobs/{fine_tuning_job_id}/cancel (Placeholder)")

    response_message = (
        f"The learning process '{fine_tuning_job_id}' in this system represents continuous adaptation "
        "(self-reflection, background analysis, knowledge updates) and cannot be 'canceled' in the traditional sense of a discrete fine-tuning job. "
        "These processes are integral to the system's operation."
    )

    # If the ID matches the conceptual one, provide the explanation. Otherwise, 404.
    if fine_tuning_job_id == "continuous_learning_main_process":
        response_body = {
            "id": fine_tuning_job_id,
            "object": "fine_tuning.job",  # Or custom
            "status": "cancellation_not_applicable",  # Custom status
            "message": response_message
        }
        logger.info(f"{request_id}: Responding that continuous learning process cannot be canceled.")
        return jsonify(response_body), 200
    else:
        resp_data, _ = _create_openai_error_response(
            message=f"Fine-tuning job ID '{fine_tuning_job_id}' not found or not applicable for cancellation.",
            err_type="invalid_request_error",
            code="fine_tuning_job_not_found"
        )
        logger.info(f"{request_id}: Job ID '{fine_tuning_job_id}' not found for cancellation.")
        return jsonify(resp_data), 404


@app.route("/v1/fine_tuning/jobs/<string:fine_tuning_job_id>/events", methods=["GET"])
def handle_list_fine_tuning_job_events(fine_tuning_job_id: str):
    request_id = f"req-ft-events-{uuid.uuid4()}"
    logger.info(f"🚀 {request_id}: Received GET /v1/fine_tuning/jobs/{fine_tuning_job_id}/events (Placeholder)")

    # Parameters for pagination (OpenAI standard), though we won't use them for much here
    # after_param = request.args.get("after")
    # limit_param = request.args.get("limit", type=int, default=20)

    response_message = (
        f"No discrete events available for '{fine_tuning_job_id}'. This system learns continuously "
        "through self-reflection on interactions, background analysis, and knowledge base updates. "
        "Progress and activities are logged internally."
    )

    if fine_tuning_job_id == "continuous_learning_main_process":
        response_body = {
            "object": "list",
            "data": [
                {
                    "object": "fine_tuning.job.event",
                    "id": f"event_info_{int(time.time())}",
                    "created_at": int(time.time()),
                    "level": "info",
                    "message": response_message,
                    "data": {
                        "step": None,
                        "metrics": {"self_reflection_cycles": "ongoing", "rag_updates": "continuous"}
                    },
                    "type": "message"
                }
            ],
            "has_more": False
        }
        logger.info(f"{request_id}: Responding with placeholder event for continuous learning.")
        return jsonify(response_body), 200
    else:
        # For unknown job IDs, return an empty list of events as per OpenAI spec for non-existent jobs
        # or a 404 if the job itself is considered not found.
        # Let's return 404 consistent with retrieve.
        resp_data, _ = _create_openai_error_response(
            message=f"Fine-tuning job ID '{fine_tuning_job_id}' not found.",
            err_type="invalid_request_error",
            code="fine_tuning_job_not_found"
        )
        logger.info(f"{request_id}: Job ID '{fine_tuning_job_id}' not found for events.")
        return jsonify(resp_data), 404


##v1/files openAI expected to be?
@app.route("/v1/files", methods=["POST"])
async def handle_upload_and_ingest_file():
    start_req_time = time.monotonic()
    request_id = f"req-file-upload-{uuid.uuid4()}"
    logger.info(f"🚀 {request_id}: Received POST /v1/files (File Upload Only)")

    db: Session = g.db  # Use the DB session from Flask's g context
    final_status_code: int = 500
    response_body: Dict[str, Any] = {}
    temp_file_path: Optional[str] = None
    original_filename: str = "unknown_file"
    file_upload_ingestion_id = f"file-upload-{uuid.uuid4()}"  # Unique ID for this specific upload

    try:
        if 'file' not in request.files:
            raise ValueError("No file part in the request. Please upload a file using the 'file' field.")

        file_storage = request.files['file']
        purpose = request.form.get('purpose')  # Still accept purpose, but processing is triggered by /fine_tuning/jobs

        if not file_storage or not file_storage.filename:
            raise ValueError("No file selected or file has no name.")
        if not purpose:
            raise ValueError("'purpose' field is required (e.g., 'fine-tune').")

        original_filename = secure_filename(file_storage.filename)

        # Ensure base temporary directory for ingestions exists
        os.makedirs(FILE_INGESTION_TEMP_DIR, exist_ok=True)  # FILE_INGESTION_TEMP_DIR from CortexConfiguration.py

        # Create a unique temporary path for the uploaded file
        # Use a more descriptive prefix for temporary files
        temp_file_path = os.path.join(FILE_INGESTION_TEMP_DIR, f"{file_upload_ingestion_id}_{original_filename}")
        await asyncio.to_thread(file_storage.save, temp_file_path)
        logger.info(f"{request_id}: File '{original_filename}' saved temporarily to '{temp_file_path}'.")

        # Create a record in UploadedFileRecord table
        uploaded_record = UploadedFileRecord(
            ingestion_id=file_upload_ingestion_id,
            original_filename=original_filename,
            stored_path=temp_file_path,
            purpose=purpose,
            status="received"  # Initial status
        )
        db.add(uploaded_record)
        db.commit()  # Commit the record to get its ID
        db.refresh(uploaded_record)  # Refresh to get the generated ID

        logger.success(
            f"{request_id}: File upload record created in DB (ID: {uploaded_record.id}, Ingestion ID: {uploaded_record.ingestion_id}).")

        # --- NEW: Automatically trigger background processing if purpose is 'fine-tune' ---
        if purpose == 'fine-tune':
            logger.info(f"{request_id}: Purpose is 'fine-tune'. Automatically initiating background ingestion process for record ID {uploaded_record.id}...")
            # Schedule the asynchronous ingestion task to run in the background
            # It's crucial to use asyncio.create_task to not block the current request handler
            asyncio.create_task(
                cortex_text_interaction._initiate_file_ingestion_and_reflection(
                    db_session_from_caller=db,  # FIX: Correct parameter name here
                    uploaded_file_record_id=uploaded_record.id
                )
            )
            response_message = "File successfully uploaded. Background processing for self-reflection initiated automatically."
            # The status will be 'queued_for_processing' in the UploadedFileRecord by the background task quickly
        else:
            response_message = "File successfully uploaded. No background processing initiated (purpose not 'fine-tune')."
        # --- END NEW ---

        response_body = {
            "id": uploaded_record.ingestion_id,  # Return the unique ingestion ID
            "object": "file",
            "bytes": await asyncio.to_thread(os.path.getsize, temp_file_path),
            "created_at": int(uploaded_record.created_at.timestamp()),
            "filename": original_filename,
            "purpose": purpose,
            "status": "processing_queued" if purpose == 'fine-tune' else "uploaded", # Reflect immediate action status
            "message": response_message
        }
        final_status_code = 200

    except ValueError as ve:
        logger.warning(f"{request_id}: Invalid file upload request: {ve}")
        response_body, final_status_code = _create_openai_error_response(str(ve), err_type="invalid_request_error",
                                                                         status_code=400)
        # Attempt to delete the partial file if an error occurred after saving but before record creation
        if temp_file_path and os.path.exists(temp_file_path):
            try:
                await asyncio.to_thread(os.remove, temp_file_path)
            except Exception as e_del:
                logger.warning(f"Failed to clean up partial file {temp_file_path}: {e_del}")

    except Exception as e:
        logger.exception(f"{request_id}: 🔥🔥 Error processing uploaded file:")
        response_body, final_status_code = _create_openai_error_response(
            f"Server error during file upload: {str(e)}", err_type="server_error", status_code=500
        )
        if db: await asyncio.to_thread(db.rollback)  # Rollback in case of DB error
        # Attempt to delete the partial file if an error occurred after saving
        if temp_file_path and os.path.exists(temp_file_path):
            try:
                await asyncio.to_thread(os.remove, temp_file_path)
            except Exception as e_del:
                logger.warning(f"Failed to clean up partial file {temp_file_path}: {e_del}")
    finally:
        duration_req = (time.monotonic() - start_req_time) * 1000
        logger.info(
            f"🏁 /v1/files POST request {request_id} handled in {duration_req:.2f} ms. Status: {final_status_code}")

    return Response(json.dumps(response_body), status=final_status_code, mimetype='application/json')


@app.route("/v1/files", methods=["GET"])
async def handle_list_files_ingestion_stub():
    request_id = f"req-file-list-ingest-stub-{uuid.uuid4()}"
    logger.info(f"🚀 {request_id}: Received GET /v1/files (Ingestion Info Placeholder)")
    # In a real scenario, you might query a table of ingestion jobs/batches.
    # For this stub, return an informative message.
    message = (
        "This endpoint normally lists uploaded files. In this system, files are ingested directly into the "
        "interaction database for self-reflection. There isn't a list of 'files' in the traditional OpenAI sense. "
        "Consider this a log of data ingestion batches if it were implemented."
    )
    response_body = {
        "object": "list",
        "data": [
            # Example of what an entry could look like if you tracked ingestion batches
            # {
            #     "id": "file-ingest-job-example123",
            #     "object": "file",
            #     "bytes": 10240, # Placeholder
            #     "created_at": int(time.time()) - 3600,
            #     "filename": "example_training_data.jsonl",
            #     "purpose": "fine-tune-data-ingested-for-reflection",
            #     "status": "processed",
            #     "status_details": "100 interactions ingested."
            # }
        ],
        "message": message
    }
    return jsonify(response_body), 200


@app.route("/v1/files/<string:file_id>", methods=["GET"])
async def handle_retrieve_file_ingestion_stub(file_id: str):
    request_id = f"req-file-retrieve-ingest-stub-{uuid.uuid4()}"
    logger.info(f"🚀 {request_id}: Received GET /v1/files/{file_id} (Ingestion Info Placeholder)")
    # Here, file_id would correspond to the ID returned by the POST /v1/files (e.g., "file-ingest-job-...")
    # You could query a database of ingestion jobs. For now, a stub:
    message = (
        f"Retrieval for 'file' ID '{file_id}' (representing an ingestion batch for self-reflection) is not fully implemented. "
        "This system ingests data directly into its interaction database. "
        "If this ID corresponds to a past ingestion, its data has been processed for self-reflection."
    )
    if file_id.startswith("file-ingest-job-"):
        response_body = {
            "id": file_id, "object": "file", "bytes": 0,  # Placeholder
            "created_at": int(time.time()) - 7200,  # Placeholder
            "filename": "ingested_dataset_placeholder.jsonl", "purpose": "fine-tune-data-ingested-for-reflection",
            "status": "processed", "status_details": message
        }
        return jsonify(response_body), 200
    else:
        resp_data, _ = _create_openai_error_response(
            f"Ingestion job ID '{file_id}' not found or not in expected format.", code="not_found",
            err_type="invalid_request_error")
        return jsonify(resp_data), 404


@app.route("/v1/files/<string:file_id>", methods=["DELETE"])
async def handle_delete_file_ingestion_stub(file_id: str):
    request_id = f"req-file-delete-ingest-stub-{uuid.uuid4()}"
    logger.info(f"🚀 {request_id}: Received DELETE /v1/files/{file_id} (Ingestion Info Placeholder)")
    # Deleting an "ingestion job" in this context might mean marking its interactions as "ignored_ingestion"
    # or some other status, rather than deleting them. For now, a stub.
    message = (
        f"Deletion of 'file' ID '{file_id}' (representing an ingestion batch) is not supported in a way that removes "
        "data from the interaction database. Data ingested for self-reflection is integrated into the system's learning process."
    )
    response_body = {"id": file_id, "object": "file.deleted_stub", "deleted": True, "message": message}
    return jsonify(response_body), 200


@app.route("/v1/files/<string:file_id>/content", methods=["GET"])
async def handle_retrieve_file_content_ingestion_stub(file_id: str):
    request_id = f"req-file-content-ingest-stub-{uuid.uuid4()}"
    logger.info(f"🚀 {request_id}: Received GET /v1/files/{file_id}/content (Ingestion Info Placeholder)")
    message = (
        f"Retrieving original content for 'file' ID '{file_id}' (ingestion batch) is not supported. "
        "Data is processed and integrated into the interaction database."
    )
    resp_data, _ = _create_openai_error_response(message, code="content_not_available",
                                                 err_type="invalid_request_error")
    return jsonify(resp_data), 404  # OpenAI returns 404 if content not retrievable for a file


@app.route("/v1/files/<string:file_id>", methods=["GET"])
async def handle_retrieve_file_metadata_stub(file_id: str):
    """
    Async endpoint to retrieve metadata for a locally indexed file from the database.
    The synchronous database query is run in a separate thread.
    """
    request_id = f"req-file-retrieve-stub-{uuid.uuid4()}"
    logger.info(f"🚀 {request_id}: Received async GET /v1/files/{file_id} (DB Lookup)")
    db: Session = g.db

    def db_lookup():
        # This synchronous function contains the blocking DB query logic.
        try:
            numeric_file_id = int(file_id)
            return db.query(FileIndex).filter(FileIndex.id == numeric_file_id).first()
        except ValueError:
            # If not an integer, try searching by full path or just the filename as a fallback.
            record = db.query(FileIndex).filter(FileIndex.file_path == file_id).first()
            if not record:
                record = db.query(FileIndex).filter(FileIndex.file_name == file_id).first()
            return record

    # Run the blocking db_lookup function in a background thread and await the result.
    found_file_record = await asyncio.to_thread(db_lookup)

    if found_file_record:
        response_body = {
            "id": str(found_file_record.id),
            "object": "file",
            "bytes": found_file_record.size_bytes or 0,
            "created_at": int(found_file_record.last_indexed_db.timestamp()),
            "filename": found_file_record.file_name,
            "purpose": "locally_indexed_for_rag",
            "status": found_file_record.index_status,
        }
        return jsonify(response_body), 200
    else:
        resp_data, _ = _create_openai_error_response(f"File '{file_id}' not found in local index.", code="file_not_found")
        return jsonify(resp_data), 404


@app.route("/v1/files/<string:file_id>", methods=["DELETE"])
def handle_delete_file_stub(file_id: str):
    request_id = f"req-file-delete-stub-{uuid.uuid4()}"
    logger.info(f"🚀 {request_id}: Received DELETE /v1/files/{file_id} (Placeholder)")

    response_message = (
        f"File deletion via this API (for ID/path '{file_id}') is not supported. Zephy/Adelaide indexes "
        "local files. To remove a file from being accessed or indexed, please delete or move it "
        "from the local filesystem using your operating system's file manager. "
        "The file indexer will update its records on a subsequent scan."
    )

    # OpenAI typically returns a deletion confirmation object.
    response_body = {
        "id": file_id,  # Echo back the ID
        "object": "file.deleted_stub",  # Custom object type
        "deleted": True,  # Indicate the API call was processed
        "message": response_message
    }
    logger.info(f"{request_id}: Responding with placeholder for file deletion, explaining local file management.")
    return jsonify(response_body), 200


@app.route("/v1/files/<string:file_id>/content", methods=["GET"])
def handle_retrieve_file_content_stub(file_id: str):
    request_id = f"req-file-content-stub-{uuid.uuid4()}"
    logger.info(f"🚀 {request_id}: Received GET /v1/files/{file_id}/content (Placeholder/DB Lookup)")
    db: Session = g.db

    found_file_record: Optional[FileIndex] = None
    search_method = "unknown"
    try:
        numeric_file_id = int(file_id)
        found_file_record = db.query(FileIndex).filter(FileIndex.id == numeric_file_id).first()
        search_method = f"database ID ({numeric_file_id})"
    except ValueError:
        found_file_record = db.query(FileIndex).filter(FileIndex.file_path == file_id).first()
        search_method = f"exact file path ('{file_id}')"
        if not found_file_record:
            found_file_record = db.query(FileIndex).filter(FileIndex.file_name == file_id).first()
            if found_file_record: search_method = f"file name ('{file_id}')"

    if found_file_record and found_file_record.indexed_content:
        content = found_file_record.indexed_content
        # Determine a basic MIME type, default to text/plain
        mime_type = found_file_record.mime_type if found_file_record.mime_type else "text/plain"
        if "unknown" in mime_type.lower(): mime_type = "text/plain"  # Fallback for "UnknownMIME"

        logger.info(
            f"{request_id}: Found and returning content for indexed file (ID: {found_file_record.id}, Path: {found_file_record.file_path}) with MIME type {mime_type}.")
        # OpenAI returns raw content, not JSON
        return Response(content, status=200, mimetype=mime_type)
    elif found_file_record and not found_file_record.indexed_content:
        response_message = (
            f"File '{file_id}' (Path: {found_file_record.file_path}) was found in the index, but its content has not been "
            "extracted or stored. Status: {found_file_record.index_status}."
        )
        resp_data, _ = _create_openai_error_response(
            message=response_message, err_type="invalid_request_error", code="file_content_not_available")
        logger.info(f"{request_id}: File '{file_id}' found but no indexed content available.")
        return jsonify(resp_data), 404  # Or 200 with an error message in content if API expects that
    else:
        response_message = (
            f"File ID or path '{file_id}' does not correspond to a known locally indexed file with available content. "
            "Zephy/Adelaide works with files indexed from the local filesystem."
        )
        resp_data, _ = _create_openai_error_response(
            message=response_message, err_type="invalid_request_error", code="file_not_found")
        logger.info(f"{request_id}: File '{file_id}' or its content not found in local index using {search_method}.")
        return jsonify(resp_data), 404



# --- Assistants API Stubs ---
@app.route("/v1/assistants", methods=["POST"])
async def create_assistant_stub():
    """Async stub for creating an Assistant."""
    request_id = f"req-assistant-create-stub-{uuid.uuid4()}"
    logger.info(f"🚀 {request_id}: Received async POST /v1/assistants (Placeholder)")
    # Even though the helper is fast, running it in a thread is the most robust
    # way to prevent blocking the event loop for even a microsecond.
    return await asyncio.to_thread(
        _create_personal_assistant_stub_response, "/v1/assistants", "POST",
        custom_status="creation_not_applicable"
    )

@app.route("/v1/assistants", methods=["GET"])
async def list_assistants_stub():
    """Async stub for listing Assistants."""
    request_id = f"req-assistant-list-stub-{uuid.uuid4()}"
    logger.info(f"🚀 {request_id}: Received async GET /v1/assistants (Placeholder)")
    response_message = (
        "This system operates as a single, integrated personal assistant. "
        "There are no multiple, separately manageable 'assistant' objects to list."
    )
    response_body = {"object": "list", "data": [], "message": response_message}
    return jsonify(response_body), 200

@app.route("/v1/assistants/<string:assistant_id>", methods=["GET"])
async def retrieve_assistant_stub(assistant_id: str):
    """Async stub for retrieving an Assistant."""
    request_id = f"req-assistant-retrieve-stub-{uuid.uuid4()}"
    logger.info(f"🚀 {request_id}: Received async GET /v1/assistants/{assistant_id} (Placeholder)")
    return await asyncio.to_thread(
        _create_personal_assistant_stub_response, f"/v1/assistants/{assistant_id}", "GET", resource_id=assistant_id
    )

@app.route("/v1/assistants/<string:assistant_id>", methods=["POST"])
async def modify_assistant_stub(assistant_id: str):
    """Async stub for modifying an Assistant."""
    request_id = f"req-assistant-modify-stub-{uuid.uuid4()}"
    logger.info(f"🚀 {request_id}: Received async POST /v1/assistants/{assistant_id} (Placeholder)")
    return await asyncio.to_thread(
        _create_personal_assistant_stub_response, f"/v1/assistants/{assistant_id}", "POST", resource_id=assistant_id,
        custom_status="modification_not_applicable"
    )

@app.route("/v1/assistants/<string:assistant_id>", methods=["DELETE"])
async def delete_assistant_stub(assistant_id: str):
    """Async stub for deleting an Assistant."""
    request_id = f"req-assistant-delete-stub-{uuid.uuid4()}"
    logger.info(f"🚀 {request_id}: Received async DELETE /v1/assistants/{assistant_id} (Placeholder)")
    return await asyncio.to_thread(
        _create_personal_assistant_stub_response, f"/v1/assistants/{assistant_id}", "DELETE", resource_id=assistant_id,
        custom_status="deletion_not_applicable"
    )


# --- Threads API Stubs ---
@app.route("/v1/threads", methods=["POST"])
async def create_thread_stub():
    """Async stub for creating a Thread."""
    request_id = f"req-thread-create-stub-{uuid.uuid4()}"
    logger.info(f"🚀 {request_id}: Received async POST /v1/threads (Placeholder)")
    return await asyncio.to_thread(
        _create_personal_assistant_stub_response, "/v1/threads", "POST",
        custom_status="thread_creation_not_applicable_managed_internally"
    )

@app.route("/v1/threads/<string:thread_id>", methods=["GET"])
async def retrieve_thread_stub(thread_id: str):
    """Async stub for retrieving a Thread."""
    request_id = f"req-thread-retrieve-stub-{uuid.uuid4()}"
    logger.info(f"🚀 {request_id}: Received async GET /v1/threads/{thread_id} (Placeholder)")
    return await asyncio.to_thread(
        _create_personal_assistant_stub_response, f"/v1/threads/{thread_id}", "GET", resource_id=thread_id
    )

@app.route("/v1/threads/<string:thread_id>", methods=["POST"])
async def modify_thread_stub(thread_id: str):
    """Async stub for modifying a Thread."""
    request_id = f"req-thread-modify-stub-{uuid.uuid4()}"
    logger.info(f"🚀 {request_id}: Received async POST /v1/threads/{thread_id} (Placeholder)")
    return await asyncio.to_thread(
        _create_personal_assistant_stub_response, f"/v1/threads/{thread_id}", "POST", resource_id=thread_id,
        custom_status="modification_not_applicable"
    )

@app.route("/v1/threads/<string:thread_id>", methods=["DELETE"])
async def delete_thread_stub(thread_id: str):
    """Async stub for deleting a Thread."""
    request_id = f"req-thread-delete-stub-{uuid.uuid4()}"
    logger.info(f"🚀 {request_id}: Received async DELETE /v1/threads/{thread_id} (Placeholder)")
    return await asyncio.to_thread(
        _create_personal_assistant_stub_response, f"/v1/threads/{thread_id}", "DELETE", resource_id=thread_id,
        custom_status="deletion_not_applicable"
    )

# --- Messages API Stubs (within Threads) ---
# --- Asynchronous Messages API Stubs (within Threads) ---

@app.route("/v1/threads/<string:thread_id>/messages", methods=["POST"])
async def create_message_stub(thread_id: str):
    """
    Async stub that simulates creating a message. It acknowledges the received message
    and returns a valid, simulated 'thread.message' object.
    """
    request_id = f"req-message-create-stub-{uuid.uuid4()}"
    logger.info(f"🚀 {request_id}: Received async POST /v1/threads/{thread_id}/messages (High-Fidelity Stub)")

    # Although we don't use the content, a real client would send it.
    # We can log it for debugging if needed.
    # request_data = await request.get_json()
    # user_content = request_data.get("content")

    # Create a realistic 'thread.message' object as the response.
    response_body = {
        "id": f"msg_{uuid.uuid4()}",
        "object": "thread.message",
        "created_at": int(time.time()),
        "thread_id": thread_id,
        "role": "assistant",  # We reply as the assistant
        "content": [
            {
                "type": "text",
                "text": {
                    "value": "This is a simulated response. Your message has been received and processed by the integrated chat flow, not a separate thread object.",
                    "annotations": []
                }
            }
        ],
        "assistant_id": "integrated_personal_assistant",
        "run_id": f"run_{uuid.uuid4()}",
        "file_ids": [],
        "metadata": {
            "note": "This system uses a direct chat model. Standard thread messages are simulated for API compatibility."
        }
    }
    return jsonify(response_body), 200


@app.route("/v1/threads/<string:thread_id>/messages", methods=["GET"])
async def list_messages_stub(thread_id: str):
    """
    Async stub that simulates listing messages in a thread. It returns a list
    object containing a few example messages.
    """
    request_id = f"req-message-list-stub-{uuid.uuid4()}"
    logger.info(f"🚀 {request_id}: Received async GET /v1/threads/{thread_id}/messages (High-Fidelity Stub)")

    # Create a realistic list object with sample messages.
    response_body = {
        "object": "list",
        "data": [
            {
                "id": f"msg_{uuid.uuid4()}",
                "object": "thread.message",
                "created_at": int(time.time()) - 60,
                "thread_id": thread_id,
                "role": "assistant",
                "content": [{"type": "text", "text": {
                    "value": "This is a simulated history. Conversation history is managed internally by the assistant's session.",
                    "annotations": []}}],
                "assistant_id": "integrated_personal_assistant",
                "run_id": f"run_{uuid.uuid4()}",
                "file_ids": [],
                "metadata": {}
            },
            {
                "id": f"msg_{uuid.uuid4()}",
                "object": "thread.message",
                "created_at": int(time.time()) - 120,
                "thread_id": thread_id,
                "role": "user",
                "content": [{"type": "text", "text": {"value": "A simulated user message.", "annotations": []}}],
                "assistant_id": None,
                "run_id": None,
                "file_ids": [],
                "metadata": {}
            }
        ],
        "first_id": "msg_abc",
        "last_id": "msg_xyz",
        "has_more": False
    }
    return jsonify(response_body), 200


# --- Asynchronous Runs API Stubs (within Threads) ---

@app.route("/v1/threads/<string:thread_id>/runs", methods=["POST"])
async def create_run_stub(thread_id: str):
    """
    Async stub that simulates creating and completing a run instantly,
    mimicking the direct-response nature of the system.
    """
    request_id = f"req-run-create-stub-{uuid.uuid4()}"
    logger.info(f"🚀 {request_id}: Received async POST /v1/threads/{thread_id}/runs (High-Fidelity Stub)")

    # Create a realistic 'thread.run' object that is already completed.
    run_id = f"run_{uuid.uuid4()}"
    current_time = int(time.time())
    response_body = {
        "id": run_id,
        "object": "thread.run",
        "created_at": current_time,
        "thread_id": thread_id,
        "assistant_id": "integrated_personal_assistant",
        "status": "completed",  # Simulate instant completion
        "started_at": current_time,
        "expires_at": None,
        "cancelled_at": None,
        "failed_at": None,
        "completed_at": current_time,
        "last_error": None,
        "model": META_MODEL_NAME_NONSTREAM,
        "instructions": "N/A (Integrated System)",
        "tools": [],
        "file_ids": [],
        "metadata": {
            "note": "Run is marked 'completed' instantly to simulate this system's direct chat response model."
        }
    }
    return jsonify(response_body), 200


@app.route("/v1/threads/<string:thread_id>/runs/<string:run_id>", methods=["GET"])
async def retrieve_run_stub(thread_id: str, run_id: str):
    """
    Async stub that simulates retrieving a run, always showing it as completed.
    """
    request_id = f"req-run-retrieve-stub-{uuid.uuid4()}"
    logger.info(f"🚀 {request_id}: Received async GET /v1/threads/{thread_id}/runs/{run_id} (High-Fidelity Stub)")

    # Return a 'completed' run object, regardless of the run_id.
    current_time = int(time.time())
    response_body = {
        "id": run_id,
        "object": "thread.run",
        "created_at": current_time - 30,
        "thread_id": thread_id,
        "assistant_id": "integrated_personal_assistant",
        "status": "completed",
        "started_at": current_time - 30,
        "expires_at": None,
        "cancelled_at": None,
        "failed_at": None,
        "completed_at": current_time - 25,
        "last_error": None,
        "model": META_MODEL_NAME_NONSTREAM,
        "instructions": "N/A (Integrated System)",
        "tools": [],
        "file_ids": [],
        "metadata": {
            "note": "This system does not have persistent, stateful runs. This is a simulated 'completed' response."
        }
    }
    return jsonify(response_body), 200


@app.route("/v1/threads/<string:thread_id>/runs/<string:run_id>/cancel", methods=["POST"])
async def cancel_run_stub(thread_id: str, run_id: str):
    """
    Async stub that simulates canceling a run.
    """
    request_id_log = f"req-run-cancel-stub-{uuid.uuid4()}"
    logger.info(
        f"🚀 {request_id_log}: Received async POST /v1/threads/{thread_id}/runs/{run_id}/cancel (High-Fidelity Stub)")

    current_time = int(time.time())
    response_body = {
        "id": run_id,
        "object": "thread.run",
        "created_at": current_time - 30,
        "thread_id": thread_id,
        "assistant_id": "integrated_personal_assistant",
        "status": "cancelled",  # Show the final 'cancelled' state
        "started_at": current_time - 30,
        "expires_at": None,
        "cancelled_at": current_time,
        "failed_at": None,
        "completed_at": None,
        "last_error": None,
        "model": META_MODEL_NAME_NONSTREAM,
        "instructions": "N/A (Integrated System)",
        "tools": [],
        "file_ids": [],
        "metadata": {
            "note": "Run cancellation is simulated. Internal tasks are managed by a priority system and cannot be cancelled via this API."
        }
    }
    return jsonify(response_body), 200


@app.route("/v1/threads/<string:thread_id>/runs/<string:run_id>/steps", methods=["GET"])
async def list_run_steps_stub(thread_id: str, run_id: str):
    """
    Async stub that simulates listing run steps, returning a single simulated step.
    """
    request_id_log = f"req-run-steps-stub-{uuid.uuid4()}"
    logger.info(
        f"🚀 {request_id_log}: Received async GET /v1/threads/{thread_id}/runs/{run_id}/steps (High-Fidelity Stub)")

    response_body = {
        "object": "list",
        "data": [
            {
                "id": f"step_{uuid.uuid4()}",
                "object": "thread.run.step",
                "created_at": int(time.time()) - 20,
                "assistant_id": "integrated_personal_assistant",
                "thread_id": thread_id,
                "run_id": run_id,
                "type": "message_creation",
                "status": "completed",
                "step_details": {
                    "type": "message_creation",
                    "message_creation": {"message_id": f"msg_{uuid.uuid4()}"}
                },
                "last_error": None,
                "expired_at": None,
                "cancelled_at": None,
                "failed_at": None,
                "completed_at": int(time.time()) - 15,
                "metadata": {
                    "note": "A single 'message_creation' step is simulated to represent the system's direct response."
                }
            }
        ],
        "first_id": "step_abc",
        "last_id": "step_xyz",
        "has_more": False
    }
    return jsonify(response_body), 200


@app.route("/v1/threads/<string:thread_id>/runs/<string:run_id>/submit_tool_outputs", methods=["POST"])
async def submit_tool_outputs_stub(thread_id: str, run_id: str):
    """
    Async stub that simulates submitting tool outputs.
    """
    request_id_log = f"req-run-submittools-stub-{uuid.uuid4()}"
    logger.info(
        f"🚀 {request_id_log}: Received async POST /v1/threads/{thread_id}/runs/{run_id}/submit_tool_outputs (High-Fidelity Stub)")

    # This endpoint returns a 'thread.run' object
    current_time = int(time.time())
    response_body = {
        "id": run_id,
        "object": "thread.run",
        "created_at": current_time - 40,
        "thread_id": thread_id,
        "assistant_id": "integrated_personal_assistant",
        "status": "completed",  # Assume it completes after "submission"
        "started_at": current_time - 40,
        "expires_at": None,
        "cancelled_at": None,
        "failed_at": None,
        "completed_at": current_time,
        "last_error": None,
        "model": META_MODEL_NAME_NONSTREAM,
        "instructions": "N/A (Integrated System)",
        "tools": [],
        "file_ids": [],
        "metadata": {
            "note": "Tool output submission is simulated. The system's internal agent operates autonomously and does not use this external feedback loop."
        }
    }
    return jsonify(response_body), 200

#============== Dove Section ===============


@app.route("/instrumentviewportdatastreamlowpriopreview", methods=["GET"])
async def handle_instrument_viewport_stream():
    req_id = f"req-instr-stream-async-{uuid.uuid4()}"
    logger.info(f"🚀 {req_id}: Async SSE connection opened for instrument data stream.")

    async def generate_data_stream():
        while True:
            def get_data_sync():
                data_packet = stella_icarus_daemon_manager.get_data_from_queue()
                if data_packet is None:
                    sim_data = _generate_simulated_avionics_data()
                    data_packet = {
                        "source_daemon": "System_Simulation_Fallback",
                        "timestamp_py": datetime.datetime.now(datetime.timezone.utc).isoformat(),
                        "data": sim_data
                    }
                return data_packet
            data_packet = await asyncio.to_thread(get_data_sync)
            yield f"data: {json.dumps(data_packet)}\n\n"
            await asyncio.sleep(1.0 / INSTRUMENT_STREAM_RATE_HZ)

    def stream_wrapper():
        async def inner():
            async for item in generate_data_stream():
                yield item
        # This is a common pattern to bridge async generators with sync frameworks
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        gen = inner()
        try:
            while True:
                yield loop.run_until_complete(gen.__anext__())
        except StopAsyncIteration:
            pass # Generator finished
        except Exception as e:
            logger.error(f"{req_id}: Error in stream wrapper: {e}")
        finally:
            loop.close()

    return Response(stream_with_context(stream_wrapper()), mimetype='text/event-stream')



#=============== IPC Server END ===============
#============================ Main Program Startup
# Define startup_tasks (as you had it)
SYSTEM_IS_PRIMING = False # for GUI/client information using /primedready whether the system is primed and ready to use or Not still starting up. (False means ready while True means it's still starting up)
async def startup_tasks():
    global SYSTEM_IS_PRIMING
    SYSTEM_IS_PRIMING = True #flagged
    

    await build_ada_daemons()
    if 'stella_icarus_daemon_manager' in globals() and stella_icarus_daemon_manager.is_enabled:
        logger.info("AdelaideAlbertCortex: Starting all StellaIcarus Ada Daemon services...")
        stella_icarus_daemon_manager.start_all()


    logger.info("AdelaideAlbertCortex: >>> Entered startup_tasks (async). <<<")
    task_start_time = time.monotonic()

    if ENABLE_FILE_INDEXER:
        logger.info("AdelaideAlbertCortex: startup_tasks: Attempting to initialize global FileIndex vector store...")
        if cortex_backbone_provider and cortex_backbone_provider.embeddings:
            init_vs_start_time = time.monotonic()
            logger.info(
                "AdelaideAlbertCortex: startup_tasks: >>> CALLING await init_file_vs_from_indexer(cortex_backbone_provider). This will block here. <<<")
            await init_file_vs_from_indexer(cortex_backbone_provider)  # This is initialize_global_file_index_vectorstore
            init_vs_duration = time.monotonic() - init_vs_start_time
            logger.info(
                f"AdelaideAlbertCortex: startup_tasks: >>> init_file_vs_from_indexer(cortex_backbone_provider) HAS COMPLETED. Duration: {init_vs_duration:.2f}s <<<")
        else:
            logger.error("AdelaideAlbertCortex: startup_tasks: CRITICAL - CortexEngine or embeddings None. Cannot init FileIndex VS.")
    else:
        logger.info("AdelaideAlbertCortex: startup_tasks: File Indexer and its Vector Store are DISABLED by config.")

    if ENABLE_SELF_REFLECTION:
        logger.info("AdelaideAlbertCortex: startup_tasks: Attempting to initialize global Reflection vector store...")
        if cortex_backbone_provider and cortex_backbone_provider.embeddings:
            init_refl_vs_start_time = time.monotonic()
            logger.info(
                "AdelaideAlbertCortex: startup_tasks: >>> CALLING await asyncio.to_thread(initialize_global_reflection_vectorstore, ...). This will block here. <<<")
            temp_db_session_for_init = SessionLocal()  # type: ignore
            try:
                await asyncio.to_thread(initialize_global_reflection_vectorstore, cortex_backbone_provider, temp_db_session_for_init)
            finally:
                temp_db_session_for_init.close()
            init_refl_vs_duration = time.monotonic() - init_refl_vs_start_time
            logger.info(
                f"AdelaideAlbertCortex: startup_tasks: >>> initialize_global_reflection_vectorstore HAS COMPLETED. Duration: {init_refl_vs_duration:.2f}s <<<")
        else:
            logger.error("AdelaideAlbertCortex: startup_tasks: CRITICAL - CortexEngine or embeddings None. Cannot init Reflection VS.")
    else:
        logger.info("AdelaideAlbertCortex: startup_tasks: Self Reflection and its Vector Store are DISABLED by config.")

    logger.info("Starting up SSE Notification Push Loop Component Thread...")
    if ENABLE_SSE_NOTIFICATIONS:
        logger.info("Scheduling proactive SSE push loop on the main event loop...")
        asyncio.create_task(_run_proactive_sse_push_loop())
    else:
        logger.info("Proactive SSE notifications disabled by configuration.")
    
    task_duration = time.monotonic() - task_start_time
    logger.info(f"AdelaideAlbertCortex: >>> Exiting startup_tasks (async). Total Duration: {task_duration:.2f}s <<<")
    
    logger.info(f"benchmarking... <<<")
    await run_startup_benchmark()


    SYSTEM_IS_PRIMING = False # System is now primed and ready to use.


# --- Main Execution Control ---

if __name__ == "__main__":
    # This block executes if AdelaideAlbertCortex is run directly (e.g., python AdelaideAlbertCortex)
    logger.error("This program (AdelaideAlbertCortex) is designed to be run with an ASGI/WSGI server like Hypercorn. Running it like this is not supported.")
    logger.error("Example: hypercorn app:app --bind 127.0.0.1:11434")
    sys.exit(1)  # Exit because this isn't the intended way to run
else:
    # This block executes when AdelaideAlbertCortex is imported as a module by a server (e.g., Hypercorn).
    logger.info("----------------------------------------------------------------------")
    logger.info(">>> AdelaideAlbertCortex: MODULE IMPORTED BY SERVER (Hypercorn worker process) <<<")
    logger.info("----------------------------------------------------------------------")

    # Ensure critical global instances were initialized earlier in the module loading
    # (These are typically defined after config and before this 'else' block)
    if cortex_backbone_provider is None or cortex_text_interaction is None or ai_agent is None:
        logger.critical(
            "AdelaideAlbertCortex: 🔥🔥 Core AI components (cortex_backbone_provider, cortex_text_interaction, ai_agent) are NOT INITIALIZED. Application cannot start properly.")
        # This is a fundamental setup error, exiting directly.
        print("AdelaideAlbertCortex: CRITICAL FAILURE - Core AI components not initialized. Exiting.", file=sys.stderr, flush=True)
        sys.exit(1)
    else:
        logger.success("AdelaideAlbertCortex: ✅ Core AI components appear initialized globally.")

    # --- Initialize Database ---
    # This is a critical step. If it fails, the app should not proceed.
    db_initialized_successfully = False
    try:
        logger.info("AdelaideAlbertCortex: >>> CALLING init_db() NOW. This must complete successfully for the application. <<<")
        # init_db() is imported from database.py
        # It's responsible for setting up the engine, SessionLocal, and migrations.
        init_db()
        db_initialized_successfully = True  # If init_db() returns without exception, assume success
        logger.success("AdelaideAlbertCortex: ✅ init_db() call completed (reported no critical errors).")
    except Exception as e_init_db_call:
        # init_db() itself should log details of its failure.
        # This catches any exception re-raised by init_db() indicating a fatal setup error.
        logger.critical(f"AdelaideAlbertCortex: 🔥🔥 init_db() FAILED CRITICALLY DURING APP STARTUP: {e_init_db_call}")
        logger.exception("AdelaideAlbertCortex: Traceback for init_db() failure at app level:")
        print(f"AdelaideAlbertCortex: CRITICAL FAILURE IN init_db(): {e_init_db_call}. Cannot continue. Exiting.", file=sys.stderr,
              flush=True)
        sys.exit(1)  # Force exit if database initialization fails

    # If we reach here, db_initialized_successfully must be True,
    # because the except block above would have sys.exit(1).
    # This explicit check is a safeguard.
    if not db_initialized_successfully:
        logger.critical(
            "AdelaideAlbertCortex: Sanity check - init_db() did not set success flag or was bypassed. EXITING ABNORMALLY.")
        sys.exit(1)

    # --- Check if SessionLocal from database.py is usable AFTER init_db() ---
    # This is a sanity check to ensure init_db actually configured SessionLocal.
    try:
        from database import SessionLocal as AppSessionLocalCheck  # Re-import to get current state

        if AppSessionLocalCheck is None:
            logger.critical(
                "AdelaideAlbertCortex: FATAL - SessionLocal from database.py is STILL NONE after init_db() call! This indicates a severe problem in init_db's internal logic. EXITING.")
            sys.exit(1)
        else:
            logger.info(
                f"AdelaideAlbertCortex: SessionLocal from database.py is NOT None after init_db(). Type: {type(AppSessionLocalCheck)}.")
            # Further check if it's bound (it should be if init_db was successful)
            if hasattr(AppSessionLocalCheck, 'kw') and AppSessionLocalCheck.kw.get('bind'):
                logger.success("AdelaideAlbertCortex: ✅ SessionLocal appears configured and bound to an engine.")
            else:
                logger.error(
                    "AdelaideAlbertCortex: 🔥 SessionLocal exists but may NOT BE BOUND to an engine (kw.bind missing). Startup tasks requiring DB will likely fail. EXITING.")
                sys.exit(1)
    except ImportError:
        logger.critical(
            "AdelaideAlbertCortex: FATAL - Could not import SessionLocal from database.py AFTER init_db() for checking. EXITING.")
        sys.exit(1)
    except Exception as e_sl_check:
        logger.critical(f"AdelaideAlbertCortex: FATAL - Unexpected error checking SessionLocal: {e_sl_check}. EXITING.")
        sys.exit(1)

    # --- Run Asynchronous Startup Tasks (like Vector Store Initialization) ---
    logger.info("AdelaideAlbertCortex: 🚀 Preparing to run asynchronous startup_tasks (e.g., Vector Store initializations)...")
    startup_tasks_completed_successfully = False
    startup_tasks_start_time = time.monotonic()

    try:
        logger.debug("AdelaideAlbertCortex: Setting up asyncio event loop for startup_tasks...")
        try:
            # Get an existing loop or create a new one for this context
            loop = asyncio.get_event_loop_policy().get_event_loop()
            if loop.is_closed():
                logger.warning("AdelaideAlbertCortex: Default asyncio event loop was closed. Creating new one for startup_tasks.")
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
        except RuntimeError:  # No current event loop on this thread
            logger.info("AdelaideAlbertCortex: No current asyncio event loop for startup_tasks. Creating new one.")
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)

        logger.info(
            "AdelaideAlbertCortex: >>> CALLING loop.run_until_complete(startup_tasks()). This will block until startup_tasks finishes. <<<")
        loop.run_until_complete(startup_tasks())  # startup_tasks() is defined earlier in AdelaideAlbertCortex
        startup_tasks_duration = time.monotonic() - startup_tasks_start_time
        logger.info(
            f"AdelaideAlbertCortex: >>> loop.run_until_complete(startup_tasks()) HAS COMPLETED. Duration: {startup_tasks_duration:.2f}s <<<")
        startup_tasks_completed_successfully = True
    except Exception as su_err:
        startup_tasks_duration = time.monotonic() - startup_tasks_start_time
        logger.critical(
            f"AdelaideAlbertCortex: 🚨🚨 CRITICAL FAILURE during loop.run_until_complete(startup_tasks()) after {startup_tasks_duration:.2f}s: {su_err} 🚨🚨")
        logger.exception("AdelaideAlbertCortex: Startup Tasks Execution Traceback:")
        # If startup_tasks fail (e.g., vector store init), the app might be in a bad state.
        # Deciding to exit here or continue with limited functionality is a design choice.
        # For now, let's exit as these tasks might be critical.
        print(f"AdelaideAlbertCortex: CRITICAL FAILURE IN startup_tasks(): {su_err}. Cannot continue. Exiting.", file=sys.stderr,
              flush=True)
        sys.exit(1)

    # --- Start Other Background Services (File Indexer, Self Reflector) ---
    # These are only started if init_db() AND startup_tasks() completed successfully.
    if db_initialized_successfully and startup_tasks_completed_successfully:
        logger.info(
            "AdelaideAlbertCortex: ✅ Core initializations (DB, Startup Tasks) successful. Proceeding to start background services...")

        logger.info("AdelaideAlbertCortex: Initializing and Starting Interaction Indexer service...")
        if 'initialize_global_interaction_vectorstore' in globals() and 'cortex_backbone_provider' in globals():
            initialize_global_interaction_vectorstore(cortex_backbone_provider)

        if 'InteractionIndexer' in globals() and 'cortex_backbone_provider' in globals():
            _interaction_indexer_stop_event = threading.Event()
            interaction_indexer_thread = InteractionIndexer(_interaction_indexer_stop_event, cortex_backbone_provider)
            interaction_indexer_thread.start()
            # You would also need to add logic to stop this thread gracefully on shutdown
        else:
            logger.error("AdelaideAlbertCortex: Could not start InteractionIndexer.")

        # Ensure start_file_indexer and start_self_reflector are defined in AdelaideAlbertCortex
        # and that ENABLE_FILE_INDEXER, ENABLE_SELF_REFLECTION are from CortexConfiguration.py
        if 'ENABLE_FILE_INDEXER' in globals() and ENABLE_FILE_INDEXER:
            logger.info("AdelaideAlbertCortex: Starting File Indexer service...")
            if 'start_file_indexer' in globals() and callable(globals()['start_file_indexer']):
                start_file_indexer()  # This function should exist in AdelaideAlbertCortex
            else:
                logger.error("AdelaideAlbertCortex: 'start_file_indexer' function not found. File Indexer NOT started.")
        else:
            logger.info("AdelaideAlbertCortex: File Indexer is DISABLED by config or variable not found. Not starting.")

        if 'ENABLE_SELF_REFLECTION' in globals() and ENABLE_SELF_REFLECTION:
            logger.info("AdelaideAlbertCortex: Starting Self Reflector service...")
            if 'start_self_reflector' in globals() and callable(globals()['start_self_reflector']):
                start_self_reflector()  # This function should exist in AdelaideAlbertCortex
            else:
                logger.error("AdelaideAlbertCortex: 'start_self_reflector' function not found. Self Reflector NOT started.")
        else:
            logger.info("AdelaideAlbertCortex: Self Reflector is DISABLED by config or variable not found. Not starting.")
    else:
        logger.error(
            "AdelaideAlbertCortex: Background services (File Indexer, Self Reflector) NOT started due to failure in DB init or startup_tasks.")
        # Even if we didn't sys.exit above, this state is problematic.
        # It's probably best to ensure exit happened earlier.
        # If somehow execution reaches here with flags false, it's a logic error.
        print("AdelaideAlbertCortex: CRITICAL - Reached end of startup with initialization flags false. Exiting.", file=sys.stderr,
              flush=True)
        sys.exit(1)

    logger.info("--------------------------------------------------------------------")
    logger.info("AdelaideAlbertCortex: ✅ Zephyrine EngineMain module-level initializations complete.")
    logger.info(f"   Application (app) is now considered ready by this worker process (PID: {os.getpid()}).")
    logger.info("   Waiting for server to route requests...")
    logger.info("--------------------------------------------------------------------")